{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown practice\n",
    "\n",
    "----\n",
    "## Evaluating accuracy\n",
    "----\n",
    "\n",
    "### Evaluation accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz-Evaluate the Accuacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After loading:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "\n",
      "   Parch     Ticket     Fare Cabin Embarked  \n",
      "0      0  A/5 21171   7.2500   NaN        S  \n",
      "1      0   PC 17599  71.2833   C85        C  \n",
      "\n",
      "After Limiting to numeric:\n",
      "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare\n",
      "0            1         0       3  22.0      1      0   7.2500\n",
      "1            2         1       1  38.0      1      0  71.2833\n",
      "\n",
      "Decision Tree has accuracy:  0.648044692737\n",
      "\n",
      "GaussianNB has accuracy:  0.698324022346\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# In this and the following exercises, you'll be adding train test splits to the data\n",
    "# to see how it changes the performance of each classifier\n",
    "#\n",
    "# The code provided will load the Titanic dataset like you did in project 0, then train\n",
    "# a decision tree (the method you used in your project) and a Bayesian classifier (as\n",
    "# discussed in the introduction videos). You don't need to worry about how these work for\n",
    "# now. \n",
    "#\n",
    "# What you do need to do is import a train/test split, train the classifiers on the\n",
    "# training data, and store the resulting accuracy scores in the dictionary provided.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "print \"After loading:\\n\",X.head(2)\n",
    "\n",
    "# Limit to numeric data\n",
    "X = X._get_numeric_data()\n",
    "print\"\\nAfter Limiting to numeric:\\n\", X.head(2)\n",
    "\n",
    "# Separate the labels\n",
    "y = X['Survived']\n",
    "# Remove labels from the inputs, and age due to missing data\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# The decision tree classifier\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train,y_train)\n",
    "print \"\\nDecision Tree has accuracy: \",accuracy_score(y_test, clf1.predict(X_test))\n",
    "\n",
    "# The naive Bayes classifier\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train,y_train)\n",
    "print \"\\nGaussianNB has accuracy: \",accuracy_score(y_test, clf2.predict(X_test))\n",
    "\n",
    "answer = { \n",
    " \"Naive Bayes Score\": 0, \n",
    " \"Decision Tree Score\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712 179 712 179\n"
     ]
    }
   ],
   "source": [
    "print len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit data: {'images': array([[[  0.,   0.,   5., ...,   1.,   0.,   0.],\n",
      "        [  0.,   0.,  13., ...,  15.,   5.,   0.],\n",
      "        [  0.,   3.,  15., ...,  11.,   8.,   0.],\n",
      "        ..., \n",
      "        [  0.,   4.,  11., ...,  12.,   7.,   0.],\n",
      "        [  0.,   2.,  14., ...,  12.,   0.,   0.],\n",
      "        [  0.,   0.,   6., ...,   0.,   0.,   0.]],\n",
      "\n",
      "       [[  0.,   0.,   0., ...,   5.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,   9.,   0.,   0.],\n",
      "        [  0.,   0.,   3., ...,   6.,   0.,   0.],\n",
      "        ..., \n",
      "        [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
      "        [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
      "        [  0.,   0.,   0., ...,  10.,   0.,   0.]],\n",
      "\n",
      "       [[  0.,   0.,   0., ...,  12.,   0.,   0.],\n",
      "        [  0.,   0.,   3., ...,  14.,   0.,   0.],\n",
      "        [  0.,   0.,   8., ...,  16.,   0.,   0.],\n",
      "        ..., \n",
      "        [  0.,   9.,  16., ...,   0.,   0.,   0.],\n",
      "        [  0.,   3.,  13., ...,  11.,   5.,   0.],\n",
      "        [  0.,   0.,   0., ...,  16.,   9.,   0.]],\n",
      "\n",
      "       ..., \n",
      "       [[  0.,   0.,   1., ...,   1.,   0.,   0.],\n",
      "        [  0.,   0.,  13., ...,   2.,   1.,   0.],\n",
      "        [  0.,   0.,  16., ...,  16.,   5.,   0.],\n",
      "        ..., \n",
      "        [  0.,   0.,  16., ...,  15.,   0.,   0.],\n",
      "        [  0.,   0.,  15., ...,  16.,   0.,   0.],\n",
      "        [  0.,   0.,   2., ...,   6.,   0.,   0.]],\n",
      "\n",
      "       [[  0.,   0.,   2., ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,  14., ...,  15.,   1.,   0.],\n",
      "        [  0.,   4.,  16., ...,  16.,   7.,   0.],\n",
      "        ..., \n",
      "        [  0.,   0.,   0., ...,  16.,   2.,   0.],\n",
      "        [  0.,   0.,   4., ...,  16.,   2.,   0.],\n",
      "        [  0.,   0.,   5., ...,  12.,   0.,   0.]],\n",
      "\n",
      "       [[  0.,   0.,  10., ...,   1.,   0.,   0.],\n",
      "        [  0.,   2.,  16., ...,   1.,   0.,   0.],\n",
      "        [  0.,   0.,  15., ...,  15.,   0.,   0.],\n",
      "        ..., \n",
      "        [  0.,   4.,  16., ...,  16.,   6.,   0.],\n",
      "        [  0.,   8.,  16., ...,  16.,   8.,   0.],\n",
      "        [  0.,   1.,   8., ...,  12.,   1.,   0.]]]), 'data': array([[  0.,   0.,   5., ...,   0.,   0.,   0.],\n",
      "       [  0.,   0.,   0., ...,  10.,   0.,   0.],\n",
      "       [  0.,   0.,   0., ...,  16.,   9.,   0.],\n",
      "       ..., \n",
      "       [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
      "       [  0.,   0.,   2., ...,  12.,   0.,   0.],\n",
      "       [  0.,   0.,  10., ...,  12.,   1.,   0.]]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'DESCR': \"Optical Recognition of Handwritten Digits Data Set\\n===================================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\nReferences\\n----------\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\", 'target': array([0, 1, 2, ..., 8, 9, 8])}\n",
      "digit data after scale [[ 0.         -0.33501649 -0.04308102 ..., -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...,  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...,  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ..., \n",
      " [ 0.         -0.33501649 -0.88456568 ..., -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...,  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...,  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n",
      "digit target [0 1 2 ..., 8 9 8]\n",
      "score: [ 0.93355482  0.94657763  0.93791946]\n",
      "mean: 0.939350636582\n"
     ]
    }
   ],
   "source": [
    "#Bagging method \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "digits=load_digits()\n",
    "data=scale(digits.data)\n",
    "X=data\n",
    "y=digits.target\n",
    "\n",
    "bagging=BaggingClassifier(KNeighborsClassifier(),max_samples=0.5,max_features=0.5)\n",
    "scores=cross_val_score(bagging,X,y)\n",
    "mean=scores.mean()\n",
    "print \"digit data:\", digits\n",
    "print \"digit data after scale\", X\n",
    "print \"digit target\", y\n",
    "print \"score:\",scores\n",
    "print \"mean:\",mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: [ 0.93355482  0.94657763  0.93791946]\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_samples,n_features=data.shape\n",
    "n_digits=len(np.unique(digits.target))\n",
    "labels=digits.target\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=10)\n",
    "clf=clf.fit(data,labels)\n",
    "\n",
    "score=clf.score(data,labels)\n",
    "print \"score:\",scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_decisiontree [ 0.73920266  0.8130217   0.76342282]\n",
      "score_randomforest [ 0.90697674  0.89983306  0.90771812]\n",
      "score_extratree [ 0.89202658  0.90984975  0.91610738]\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree vs. RandomForest vs. ExtraTree\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_decisiontree=DecisionTreeClassifier(max_depth=None,min_samples_split=1,random_state=0)\n",
    "scores_decisiontree=cross_val_score(clf_decisiontree,X,y)\n",
    "print \"score_decisiontree\",scores_decisiontree\n",
    "\n",
    "\n",
    "clf_randomforest=RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1,random_state=0)\n",
    "scores_randomforest=cross_val_score(clf_randomforest,X,y)\n",
    "print \"score_randomforest\",scores_randomforest\n",
    "\n",
    "\n",
    "clf_extratree=ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1,random_state=0)\n",
    "scores_extratree=cross_val_score(clf_extratree,X,y)\n",
    "print \"score_extratree\",scores_extratree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaBoost score: [ 0.75        0.85185185  0.76923077]\n",
      "adaBoost mean score 0.790360873694\n"
     ]
    }
   ],
   "source": [
    "#adaBoost=Adjust dataset at each iteration by \n",
    "#(1) selecting a decision stump, \n",
    "#(2) increasing the weight of cases that decision stump labled incorrectly\n",
    "#while reducing the weight of correctly labeled cases\n",
    "#adaBoost can be used for classification and regression. \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "n_estimators=400\n",
    "learning_rate=1\n",
    "heart=fetch_mldata(\"heart\")\n",
    "X=heart.data\n",
    "y=np.copy(heart.target)\n",
    "y[y==-1]=0\n",
    "\n",
    "X_test,y_test=X[189:],y[189:]\n",
    "X_train,y_train=X[:189],y[:189]\n",
    "\n",
    "dt_stump=DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)\n",
    "dt_stump.fit(X_train,y_train)\n",
    "dt_stump_err=1.0-dt_stump.score(X_test,y_test)\n",
    "\n",
    "dt=DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\n",
    "dt.fit(X_train,y_train)\n",
    "dt_err=1.0-dt.score(X_test,y_test)\n",
    "\n",
    "ada_discrete=AdaBoostClassifier(base_estimator=dt_stump, learning_rate=learning_rate,n_estimators=n_estimators,algorithm=\"SAMME\")\n",
    "ada_discrete.fit(X_train,y_train)\n",
    "\n",
    "scores=cross_val_score(ada_discrete,X_test,y_test)\n",
    "print \"adaBoost score:\", scores\n",
    "print \"adaBoost mean score\", scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,   1.,   4., ...,   2.,   3.,   3.],\n",
       "       [ 67.,   0.,   3., ...,   2.,   0.,   7.],\n",
       "       [ 57.,   1.,   2., ...,   1.,   0.,   7.],\n",
       "       ..., \n",
       "       [ 56.,   1.,   4., ...,   2.,   1.,   6.],\n",
       "       [ 52.,   1.,   4., ...,   1.,   3.,   7.],\n",
       "       [ 62.,   0.,   4., ...,   2.,   0.,   3.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
