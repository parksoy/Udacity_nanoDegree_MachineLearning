#https://github.com/veeresht/MLNanoDegree/blob/master/project4/smartcab/agent.py
#https://github.com/tcya/Q-Learning-Smartcab/blob/master/smartcab/agent.py
#https://discussions.udacity.com/t/this-is-a-very-bad-project/208281/6

import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
from collections import defaultdict
from itertools import izip
import operator
import pprint
import numpy as np


class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q =dict()           #Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor
        #self.gamma=1 #discount-Reviewer1 says not to use it.
        '''
        if you were to use gamma > 0, the maximum Q-value of interest here would be the one
        for the next state, not the one for the current state. The idea is to discount future
        rewards so that the agent will consider them when taking this one immediate action.
        But, as discussed later on in the notebook, there is no need for taking future rewards
        into account in this setting.
        '''


        ###########
        ## TO DO ##
        ###########
        # Set any additional class parameters as needed
        self.no_trials=0


    def reset(self, destination=None, testing=False): #POR False
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)


        ###########
        ## TO DO ##
        ###########
        # Update epsilon using a decay function of your choice
        self.no_trials=self.no_trials+1
        #print "total_trials =====",self.no_trials

        #print "self.alpha======",self.alpha
        #print "before self.epsilon update============",self.epsilon

        #linear
        #self.epsilon=self.epsilon-0.002


        #exponential
        #at=self.alpha*self.no_trials
        self.epsilon=np.exp(-0.005*self.no_trials)

        #self.alpha=self.alpha*np.exp(-0.002*self.no_trials)
        self.alpha=self.alpha-0.0005
        #print "after self.epsilon update============",self.epsilon


        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0
        if testing==True:
            self.epsilon=0
            self.alpha=0

        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the
            environment. The next waypoint, the intersection inputs, and the deadline
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ###########
        ## TO DO ##
        ###########
        # Set 'state' as a tuple of relevant data for the agent
        #state = (waypoint, inputs['light'],inputs['oncoming'],inputs['left'])
        state = (waypoint, inputs['light'], ('oncoming',inputs['oncoming']), ('left',inputs['left']))

        #state=(waypoint,isSafe,deadline_region)

        return state


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        ###########
        ## TO DO ##
        ###########
        # Calculate the maximum Q-value of all actions for a given state

        #maxQidx=max(self.Q[self.state].iteritems(), key=operator.itemgetter(1))[0]
        #maxQval=self.Q[self.state][maxQidx]

        #Reviwer1 suggests a simpler way
        maxQval = max(self.Q[state].values())

        #print "\nmaxQidx------:", maxQidx
        #print "maxQval------:", maxQval

        return maxQval


    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ###########
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0

        if self.learning:
            if state in self.Q:
                pass
            else:
                state=self.build_state()
                #print "Current state------",state
                self.Q[state]={}
                self.Q[state]['forward']=0.0
                self.Q[state]['right']=0.0
                self.Q[state]['left']=0.0
                self.Q[state][None]=0.0


                '''
                #reviewer1 suggests:
                #https://www.python.org/dev/peps/pep-0274/
                if (self.learning) and (state not in self.Q):
                    self.Q[state] = {action: 0 for action in self.valid_actions}
                '''

        #print "initiallized self.Q:---------"
        #pprint.pprint(self.Q)

        return



    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        #self.learn(self, state, currrent_action, currrent_reward)
        #print "When choosing action, it is based on this Q-----------\n"
        #pprint.pprint(self.Q[self.state])

        ###########
        ## TO DO ##
        ###########
        # When not learning, choose a random action
        if self.learning==False:
            action= random.choice(self.env.valid_actions)

        #elif self.learning==True and self.epsilon>0.95: #first submission

        # When learning, choose a random action with 'epsilon' probability
        #Reviewer1 pointed here that "self.epsilon should be compared against a random number
        #between 0 and 1 to check whether the agent will perform an action at random or not.
        #Instead, you are always checking self.epsilon against 0.95."




        elif self.learning==True and self.epsilon > random.random():
            action=random.choice(self.env.valid_actions)
        #   Otherwise, choose an action with the highest Q-value for the current state
        else:
            maxQval=self.get_maxQ(state)
            actionoptions=[]
            for key,val in self.Q[state].items():
                if val==maxQval:
                    actionoptions.append(key)
            action=random.choice(actionoptions) ##reviewer2 pointed out indentation mistake


        #print "and this action was chosen-",action

        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards
            when conducting learning. """

        ###########
        ## TO DO ##
        ###########
        # When learning, implement the value iteration update rule

        #reviewer2 pointed out to
        #make sure your Q-Learning algorithm is only updated when learning is True
        if self.learning==True :

        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')


        #Reviewer2 pointed gamma is 1:the future rewards don't matter
        #self.Q[state][action] = (1 - self.alpha)*self.Q[state][action] + self.alpha*(reward + self.get_maxQ(state))
        #therefore,
        #self.Q[self.state][action] = (1.0 - self.alpha) * self.Q[self.state][action] + self.alpha * (reward + 0 * 0 * maxNextQ)
            self.Q[self.state][action] = (1.0 - self.alpha) * self.Q[self.state][action] + self.alpha * reward
        #print "Updated Q---------------"
        #pprint.pprint(self.Q)

        return


    def update(self):
        """ The update function is called when a time step is completed in the
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        ''''''
        #POR flow
        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn



        return


def run():
    """ Driving function for running the simulation.
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment(verbose=False,num_dummies=100)

    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    #initial Q-learning without optimization alpha, epsilon
    agent = env.create_agent(LearningAgent,learning=True,epsilon=1,alpha=0.5)

    #optimized Q-learning with a AND e variation
    #alpha_init=[0.9,0.7,0.5,0.3] # learning rate,alpha,default 0.5

    '''
    for a in alpha_init:
        for e in epsilon_init:
            agent = env.create_agent(LearningAgent,learning=True,epsilon=e,alpha=a)

    '''
    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent,enforce_deadline=True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, update_delay=0.01, display=False,log_metrics=True,optimized=True)

    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test=10,tolerance=0.2)


if __name__ == '__main__':
    run()
