{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (1.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarr_train.shape-- (60000, 28, 28)\n",
      "labels_raw_train.shape-- (60000,)\n",
      "ndarr_test.shape-- (10000, 28, 28)\n",
      "labels_raw_test-- (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "random.seed()\n",
    "\n",
    "# read data and convert idx file to numpy array\n",
    "ndarr_train = idx2numpy.convert_from_file('train-images-idx3-ubyte.idx')\n",
    "labels_raw_train = idx2numpy.convert_from_file('train-labels-idx1-ubyte.idx')\n",
    "ndarr_test = idx2numpy.convert_from_file('t10k-images-idx3-ubyte.idx')\n",
    "labels_raw_test = idx2numpy.convert_from_file('t10k-labels-idx1-ubyte.idx')\n",
    "\n",
    "print('ndarr_train.shape--',ndarr_train.shape)\n",
    "print('labels_raw_train.shape--',labels_raw_train.shape)\n",
    "print('ndarr_test.shape--',ndarr_test.shape)\n",
    "print('labels_raw_test--',labels_raw_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def displaySequence(n, dataset_testtrain, data_labels_testtrain):\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    img=Image.fromarray(dataset_testtrain[n])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print ('Label : {}'.format(data_labels_testtrain[n]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random shuffle data\n",
    "\n",
    "#trainset\n",
    "shuffled_train_index=[]\n",
    "shuffled_train_label=[]\n",
    "\n",
    "shuffled_train_index=np.arange(len(labels_raw_train))\n",
    "np.random.shuffle(shuffled_train_index)\n",
    "shuffled_train_dataset=ndarr_train[shuffled_train_index,:,:]\n",
    "\n",
    "for ix,newidx in enumerate(shuffled_train_index):\n",
    "    shuffled_train_label.append(labels_raw_train[newidx])\n",
    "\n",
    "shuffled_train_label=np.array(shuffled_train_label)\n",
    "\n",
    "#testset\n",
    "shuffled_test_index=[]\n",
    "shuffled_test_label=[]\n",
    "\n",
    "shuffled_test_index=np.arange(len(labels_raw_test))\n",
    "np.random.shuffle(shuffled_test_index)\n",
    "shuffled_test_dataset=ndarr_test[shuffled_test_index,:,:]\n",
    "\n",
    "for ix,newidx in enumerate(shuffled_test_index):\n",
    "    shuffled_test_label.append(labels_raw_test[newidx])\n",
    "\n",
    "shuffled_test_label=np.array(shuffled_test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate 5 single digits\n",
    "\n",
    "image_height = 28\n",
    "image_width = 140 # 28*5 \n",
    "num_digit=5\n",
    "\n",
    "def createSequences(ndarr,labels_raw):\n",
    "    \n",
    "    dataset_size = ndarr.shape[0]/5\n",
    "    dataset=[]\n",
    "    data_labels = []\n",
    "    \n",
    "    dataset = np.ndarray(shape=(dataset_size, image_height, image_width),dtype=np.float32)\n",
    "    data_labels=np.ndarray(shape=(dataset_size,num_digit+1),dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    w = 0\n",
    "    while i < dataset_size: #dataset_size\n",
    "        temp = np.hstack(\n",
    "            [ndarr[w], ndarr[w + 1], ndarr[w + 2], ndarr[w + 3], ndarr[w + 4]])\n",
    "        dataset[i, :, :] = temp\n",
    "        temp_str = np.hstack([labels_raw[w], labels_raw[w + 1], labels_raw[w + 2], labels_raw[w + 3], labels_raw[w + 4], \n",
    "                    num_digit])\n",
    "        data_labels[i,:]=temp_str #.append(temp_str)\n",
    "        w += 5\n",
    "        i += 1\n",
    "\n",
    "    np.array(data_labels)\n",
    "\n",
    "    return dataset, data_labels\n",
    "\n",
    "concat_dataset_train, concat_data_labels_train = createSequences(shuffled_train_dataset,shuffled_train_label)\n",
    "concat_dataset_test, concat_data_labels_test = createSequences(shuffled_test_dataset,shuffled_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_dataset_train--- (12000, 28, 140)\n",
      "concat_data_labels_train--- (12000, 6)\n",
      "concat_dataset_test--- (2000, 28, 140)\n",
      "concat_data_labels_test--- (2000, 6)\n"
     ]
    }
   ],
   "source": [
    "print('concat_dataset_train---',concat_dataset_train.shape)\n",
    "print('concat_data_labels_train---',concat_data_labels_train.shape) \n",
    "print('concat_dataset_test---',concat_dataset_test.shape)\n",
    "print('concat_data_labels_test---',concat_data_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding and resize to 28X28\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "\n",
    "img=Image.fromarray(concat_dataset_train[0])\n",
    "img2=Image.fromarray(concat_dataset_train[1])\n",
    "\n",
    "\n",
    "longer_side = max(img.size) #(252, 143)\n",
    "horizontal_padding = (longer_side - img.size[0]) / 2\n",
    "vertical_padding = (longer_side - img.size[1]) / 2\n",
    "img2 = img.crop(\n",
    "    (\n",
    "        -horizontal_padding,\n",
    "        -vertical_padding,\n",
    "        img.size[0] + horizontal_padding,\n",
    "        img.size[1] + vertical_padding\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.size== (140, 28)\n",
      "img2.size== (140, 140)\n",
      "<class 'PIL.Image.Image'>\n",
      "img3_resized_from_140_140.size= (50, 50)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGtCAYAAABOYZA0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Cn5snRtmmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118a84d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "f, ax = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "ax[0].set_title('img', loc='center')\n",
    "print('img.size==',img.size)\n",
    "ax[0].imshow(img)\n",
    "\n",
    "ax[1].set_title('img2_padded_140x140', loc='center')\n",
    "ax[1].axis('off')\n",
    "print('img2.size==',img2.size)\n",
    "ax[1].imshow(img2)\n",
    "\n",
    "img3_resized_from_140_140=img2.resize((50,50),Image.ANTIALIAS)\n",
    "print(type(img3_resized_from_140_140))\n",
    "ax[2].set_title('img3_resized_from_140_140', loc='center')\n",
    "ax[2].axis('off')\n",
    "print('img3_resized_from_140_140.size=',img3_resized_from_140_140.size)\n",
    "ax[2].imshow(img3_resized_from_140_140)\n",
    "backtoarray=np.array(img3_resized_from_140_140)\n",
    "print(type(backtoarray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pad and resize all samples:\n",
    "\n",
    "def pad_resize_to_square(rectangular_array, finalsize): #28x140\n",
    "    img=Image.fromarray(rectangular_array)\n",
    "    longer_side = max(img.size) \n",
    "    horizontal_padding = (longer_side - img.size[0]) / 2\n",
    "    vertical_padding = (longer_side - img.size[1]) / 2\n",
    "    \n",
    "    padded_img = img.crop(\n",
    "                        (\n",
    "                            -horizontal_padding,\n",
    "                            -vertical_padding,\n",
    "                            img.size[0] + horizontal_padding,\n",
    "                            img.size[1] + vertical_padding\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    img_resized=padded_img.resize((finalsize,finalsize),Image.ANTIALIAS)\n",
    "    back_to_array=np.array(img_resized)\n",
    "    return back_to_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x182b1b3d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBFIeESQghRFP8/41rzKVRjiRgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10452ab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=pad_resize_to_square(concat_dataset_train[0],70)\n",
    "test_image=Image.fromarray(test)\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(45,241.941;279x82.0588)\n",
      "i= 0\n",
      "====\n",
      "AxesImage(45,143.471;279x82.0588)\n",
      "i= 1\n",
      "====\n",
      "AxesImage(45,45;279x82.0588)\n",
      "i= 2\n",
      "====\n",
      "(3, 70, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x183aa4090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGMCAYAAACCkUanAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182d01c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ0AAAGxCAYAAAB4P7QvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6JjojOiY6IzomOiM6JjojOiY6Izo/Atz+JpWoKQ1xQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182f2e9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqr_concat_dataset_train=[]\n",
    "counter=0\n",
    "finalsize=70\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "\n",
    "#for rectangular in concat_dataset_train:\n",
    "for i in range(0,len(concat_dataset_train)):\n",
    "    while counter<3:\n",
    "        counter+=1\n",
    "        sqr_tmp=pad_resize_to_square(concat_dataset_train[i],finalsize)\n",
    "        sqr_concat_dataset_train.append(sqr_tmp)\n",
    "        img=Image.fromarray(concat_dataset_train[i])\n",
    "        print(ax[i].imshow(img))\n",
    "        print('i=',i)\n",
    "        i+=1\n",
    "        print('====')\n",
    "        \n",
    "\n",
    "sqr_concat_dataset_train=np.array(sqr_concat_dataset_train)\n",
    "print(sqr_concat_dataset_train.shape)\n",
    "\n",
    "f2, bx = plt.subplots(nrows=3, ncols=1)\n",
    "img0=Image.fromarray(sqr_concat_dataset_train[0])\n",
    "img1=Image.fromarray(sqr_concat_dataset_train[1])\n",
    "img2=Image.fromarray(sqr_concat_dataset_train[2])\n",
    "bx[0].imshow(img0)\n",
    "bx[1].imshow(img1)\n",
    "bx[2].imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 70, 70)\n"
     ]
    }
   ],
   "source": [
    "#convert all 28x140 to 70x70:\n",
    "sqr_concat_dataset_train=[]\n",
    "\n",
    "for i in range(0,len(concat_dataset_train)):\n",
    "    sqr_tmp=pad_resize_to_square(concat_dataset_train[i],finalsize)\n",
    "    sqr_concat_dataset_train.append(sqr_tmp)\n",
    "    i+=1\n",
    "        \n",
    "sqr_concat_dataset_train=np.array(sqr_concat_dataset_train)\n",
    "print(sqr_concat_dataset_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 70, 70)\n"
     ]
    }
   ],
   "source": [
    "sqr_concat_dataset_test=[]\n",
    "for i in range(0,len(concat_dataset_test)):\n",
    "    sqr_tmp=pad_resize_to_square(concat_dataset_test[i],finalsize)\n",
    "    sqr_concat_dataset_test.append(sqr_tmp)\n",
    "    i+=1\n",
    "        \n",
    "sqr_concat_dataset_test=np.array(sqr_concat_dataset_test)\n",
    "print(sqr_concat_dataset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1829dfa90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ0AAAGxCAYAAAB4P7QvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2eGkc2THb2Z6ZoJT9LNaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118a9fa10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "f3, cx = plt.subplots(nrows=3, ncols=1)\n",
    "img0=Image.fromarray(sqr_concat_dataset_test[0])\n",
    "img1=Image.fromarray(sqr_concat_dataset_test[1])\n",
    "img2=Image.fromarray(sqr_concat_dataset_test[2])\n",
    "cx[0].imshow(img0)\n",
    "cx[1].imshow(img1)\n",
    "cx[2].imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+MrFl93/nPt3//vn1/zNwhayceL4lDRIQDNoSNsROP\nZRZLdrAUEe9GYZ2IReAgsdlIwWixzDKR18KywzoxEdJq5dheb8Q/WWJW6zGG7AZDAJmxsRxjJwTG\nMMzcO3du39u/f/fZP6q/z3zr9Knqru7qrjrd75f0qOpW1e16nqee5/k+55zvOcdSSgIAoBYjg14B\nAAB6QeACAFSFwAUAqAqBCwBQFQIXAKAqBC4AQFUIXACAqhC4AABVIXABAKpC4AIAVOXcApeZ/QMz\n+5qZbZrZ58zsu8/ruwAAV8e5BC4z+9uSfl7ST0v6K5K+JOkpM7t1Ht8HALg67DwG2TWzz0n6fErp\n3Yf/NknfkPSLKaUPZp+9KemNkp6RtNX3lQEA1GJK0rdJeiqldL/Th8b6/a1mNi7pNZJ+xl9LKSUz\n+21Jry/8lzdK+j/6vR4AgGr9HUm/3unN86gqvCVpVNLd7PW7kh4rfP6Zc1gHAEC9nun25jBkFVI9\nCACIusaF8whcL0ral3Q7e/22pDvn8H0AgCuk74ErpbQr6YuSnvDXDpMznpD02X5/HwDgaul7csah\nX5D0y2b2RUlfkPQPJc1I+uVz+j4AwBVxLoErpfTRwz5bH1CrivD3Jb0xpXTvPL4PAHB1nEs/rp5W\nwOzValUtAgAgSa9JKT3d6c1hyCoEAODECFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIX\nAKAqBC4AQFUIXACAqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgA\nAFUhcAEAqkLgAgBUhcAFAKgKgQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUA\nqAqBCwBQFQIXAKAqBC4AQFUIXACAqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBA\nVQhcAICqELgAAFUhcAEAqtJz4DKzN5jZvzGzb5rZgZn9SOEzHzCz58xsw8w+YWYv78/qAgCuutOU\nuGYl/b6kn5CU8jfN7D2S3iXp7ZJeK2ld0lNmNnGG9QQAQJI01ut/SCn9pqTflCQzs8JH3i3pyZTS\nxw8/81ZJdyW9WdJHT7+qAAD0uY3LzB6X9JikT/prKaUVSZ+X9Pp+fhcA4Grqd3LGY2pVH97NXr97\n+B4AAGdCViEAoCr9Dlx3JJmk29nrtw/fAwDgTPoauFJKX1MrQD3hr5nZgqTXSfpsP78LAHA19ZxV\naGazkl6uVslKkr7dzF4laSml9A1JH5L0PjP7iqRnJD0p6VlJH+vLGgMArrSeA5ek75L0b9VKwkiS\nfv7w9X8p6e+nlD5oZjOSPiJpUdKnJb0ppbTTh/UFAFxxltKRPsQXuwJmr5b0xYGuBABgmLwmpfR0\npzfJKgQAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAFAKgKgQsA\nUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAqBC4AQFUIXACA\nqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBU\nhcAFAKgKgQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAq\nBC4AQFV6Clxm9l4z+4KZrZjZXTP712b2Fwqf+4CZPWdmG2b2CTN7ef9WGQBwlfVa4nqDpH8m6XWS\nfkDSuKTfMrNp/4CZvUfSuyS9XdJrJa1LesrMJvqyxgCAK81SSqf/z2a3JL0g6XtTSr9z+Npzkn4u\npfRPD/+9IOmupP8upfTRwt94taQvnnolAACXzWtSSk93evOsbVyLkpKkJUkys8clPSbpk/6BlNKK\npM9Lev0ZvwsAgNMHLjMzSR+S9DsppT86fPkxtQLZ3ezjdw/fAwDgTMbO8H8/LOkvSfprfVoXAACO\ndaoSl5n9c0k/JOmvp5SeD2/dkWSSbmf/5fbhewAAnEnPgeswaP1NSX8jpfT1+F5K6WtqBagnwucX\n1MpC/OzZVhUAgB6rCs3sw5L+G0k/ImndzLxktZxS2jp8/iFJ7zOzr0h6RtKTkp6V9LG+rDEA4Err\ntY3rHWolX/y/2et/T9KvSFJK6YNmNiPpI2plHX5a0ptSSjtnW1UAAM7Yj6svK0A/LgBAu3PtxwUA\nwIUicAEAqkLgAgBUhcAFAKgKgQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUA\nqAqBCwBQFQIXAKAqBC4AQFUIXACAqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBA\nVQhcAICqELjQkZnJzAa9GgDQhsCFohiwCGAAhgmBC0d0ClIEMADDgMAFAKgKgQtHpJQ6vt7pPQC4\nKGODXgEMp5SSzIxABWDoUOJCRwQtAMOIwAUAqAqBCwBQFdq4IKmV6j4yMqLR0VGNjo42z6WXkjLy\n5eDgQAcHB81zqhYBXAQCFxpjY2MaHx9vW8ysCVAxSO3t7Wl/f795JOMQwEUhcEFSq8Q1Ojqq8fFx\nTU1NNYskHRwcaH9/X/v7+83z3d1d7e7uSmqVyPb39we5+gCuEAIXGqOjo5qYmNDU1JRmZ2c1MzMj\nM2tKVV7C2tvb08hIq3nUS2J7e3sDXnsAVwWBC5JeKnFNTExoenpas7OzmpubawKXL17S8j5eHswA\n4KIQuCCpFbjGxsaawDU3N6fFxUWZmXZ2drSzs6Pd3V3t7Oxoe3u7KWXt7u5qZGSk6/iGtH0B6CcC\nV3DSAWQv44XYA9fU1JTm5uZ07do13bx5U5K0tbXVLJ5p6EHLMxA7/U1/vIz7DMOh03nLMXd5EbgO\nxYtsN5d5KCQPXLOzs7p27Zpu3LghSVpfX9f4+LhGR0ebLMPd3V1tb283r+X7rfTvy7jPMFidztvL\nfJ6CwCWpfbqOk5S6LuNJYWZNRmEeuMbHxzU2NtZs897enra3tzU2NlYsceVzefl+umz7rBf9nA7m\nqu7DTvLz1/cP++nyInAFvcw3ddlOilJV4Y0bN5RSatqwPBV+Z2dHExMTbYGrtN+oKmzp9xxmV31/\nOj9f41LCvrp8rnzgOunB7y7rSRAD1+zsrBYWFnTz5k0dHBy0lbR2dna0tbXVdFDu1saF/get+Hcv\n67F4WnFfs28ut54Cl5m9Q9I7JX3b4Uv/QdIHUkq/GT7zAUlvk7Qo6TOS3plS+kpf1vYcjIyMaGxs\nrCk9+PPSv3d3d7W1taXt7e3mcXd399KcJJ7e7inuOzs7Ojg40Pb2dtuytbXVZBl6p2RcvHijVToG\nL8txKakp1cdHH54sDlMmvZQ4FB9xufRa4vqGpPdI+k+STNKPS/qYmX1nSunLZvYeSe+S9FZJz0j6\nJ5KeMrNXpJR2+rbWfeSjRUxOTmpiYkITExOanJxsGz1iampKk5OT2tjY0MrKSrN4KcTVfqGIwzl5\n6vv+/n4TrLa2trS5uanNzU1tb29rZ2enbcin0t+jZNB/eXtsvo8vWxDrFKi81O9DlY2NtS5nfqxu\nb283N2M1bP95lc7PYlj3W0+BK6X0f2cvvc/M3inpr0r6sqR3S3oypfRxSTKzt0q6K+nNkj569tXt\nXSnbKBoZGWmSEqanpzU9Pa2ZmRnNzc1pbm6u6Yg7Nzen5eVlvfjiixofH1dKSVtbW213vDVfpD1o\neWnLA5cnYuTBy0ube3t7RwbY9X3hz+PrV8F5VlnFYBWXmJBQCmQ1H5veOT4GqPHx8eYm0284JyYm\nlFLS+vp6W7eN7e3tjn930PukW9uwNPhzZpjWJTp1G5eZjUh6i6QZSZ81s8clPSbpk/6ZlNKKmX1e\n0us1gMDV6aCIP0Ac5sgD1sLCgq5du6bFxUUtLi42z+/fv6/x8XEdHBxoa2tLy8vLQ/vDnkYcCcPT\n3ff29tr6cW1ubjZ3sx7YTlJVWPu+OamLumv2gOVVZz7IcaeLcc3737fTA5YHq7xWZGpqSgcHB21B\na2trq+Pf9MdB7ZvjupB0eu0ixX0zDIHe9Ry4zOyVkv69pClJq5J+NKX0J2b2eklJrRJWdFetgDYw\n3doBYonLkxIWFxd169Yt3bp1Szdv3mwe5+bmdHBwoM3NTS0vL2tiYqJ4YA3Lj9urOO6gl7g8gMWg\ndVyJS3ppHwzTwX7eBhG08sDV6XeoWQxcXsqanJzUzMyMZmZmmlqSmZmZplrQg1bsxhH/Xv73L3o/\n5V1GSs+jQQfXYTufT1Pi+mNJr5J0TdLfkvQrZva9fV2rPij1y/KdHne+153Hk8JLXj56xCOPPKLb\nt2/r4OBAy8vLunfvnqanpzU+Pq6RkZG2Escw/KhnEefa6jTvVvz3VZrOJK+ei4kCfhz589J+7Md+\n8gt43t4TfxdfYqKNPx/mUfy7XRRjVaGXtqanp9uq+Kenp7W/v6+NjY2mSrFbH0PUq+fAlVLak/TV\nw3/+npm9Vq22rQ+qlbBxW+2lrtuSfu+M63nu8kkSSwEt1qf7iRFHk4ip4zXKq2R8Pi7fdl+8SmZv\nb694cYh/zx9r3SdRnmXqx0Zsa/F9FEfS9+d+g3OSfdGpGsm/0y/g3p8uDoLsz2NJ2ZdhDVzdjpVS\nVeHU1FSzH/wczM/h4/bzII/LvB24W63QIA1rCb4f/bhGJE2mlL5mZnckPSHpDyTJzBYkvU7SL/Xh\ne85NKYEgNgj7HV4pcI2NjbVdDIblhz0N3+a43SMjI8ULc6xOLHVA7rU6pob+c94eGoO4j6SfL75v\n4uIZqL0Ervyx1LYzOTnZtDn6IMg7OztaX19vMmBHRka0v7/fsc1nmMWswljiip3g/fiKJVyp874e\nhpJXp4B1XELZoAzLeki99+P6GUn/j6SvS5qX9HckfZ+kHzz8yIfUyjT8ilrp8E9KelbSx/q0vn0X\nD/B4wPvJErOXfIlpuPn09sNwQpxWrPLybfTAle+DOFZhHrhK1bP58+gk+2wY7kjjtC/epuJVyp7A\n48+9o7a3C3rfN3eSIF7KIJycnGzmSvPHqampI22Qnjw0OTmpkZER7e3taXNz83x30Ckdd5OTl7g8\ncMXqQP8beTV2L+swiGMrnhv5a8NgmNYl6rXE9aikfynpZZKW1SpZ/WBK6VOSlFL6oJnNSPqIWh2Q\nPy3pTWlI+3C5UvA6aYlrdHS0qVr0qsIaq8Ziu42XJL0aJq8qnJyc1M7OzrEjZ5ykqvAkDdPScJxA\n+USb8/PzTSKPJ/H4862tLW1sbGh9fV3r6+va2NhoK+3kJYLSfsjb1MxM09PTmp+fb1tmZmbavsef\nLy0tNSWtzc1NraysXODe6p+8xOXHYGzrO01Vof/tYTi2hmEdatJrP663neAz75f0/lOuz8CUSlx5\nG1cMXrGqMDbED2sbwknEqkK/sz04OCiWuDxjK7/jPct3H/d+t3Tvi+D7xecr8/EcH330UT322GNt\ny8bGhlZXV9uWra2tIwHruMDl+9YXHwA5lu7m5+ePfNfq6qqmp6ebktbKyoomJiYueI/1T6nEle+b\nk1YV1lwrgpYrP1ZhDFgxGytWOUjtJZK8577///39/a7j9g3L3Z109K4+JhzETp57e3vNNsVU+Zip\nVkqHz0tbZ93249osBiGvVo7HhCew+M3AxMREM5JD/v+jTgHMH6enp7WwsKC5ubkmFdzberr9TqXf\nKP+eYdq3uW5tqCcNRIMMWHltRlyko7U+8fl5Z+922y+la2JMMhpUVvGlDVzxYpmfmKVMmTyFOJ7s\npbavmHnn//e4mYD9cdAXiFJKd34y+bbFdY6D7PZrrMJBl6J6kd/gdOoaEIOVVyNPTExod3e37W91\nUgok/tw7ysfkBE+/39/fb8bT3NjY0MbGRs8dxYdV6ZwtZeUNkzy4+iDWnrrvz+Nx1e0Y6+fv161N\nOtrd3W0WvxnKb+y73RSdl0sbuE6qVNrK7y5Kd9bxAu//L06q2C1LaBj4tvgSg1VcYop/vDB26oB8\nmm096YVn0MEtL53nx0pcP7+TjgkdvaTD59/n3zkxMdEWuGIbo085s729rY2NjSZJI/5OuU6JNMMo\nv7sf1oAV+fVAah0TpTbKPFD5uRYXLznnf7sX+Tl63KOk5lz3rhVm1gQvM2seCVwDkF+ITlLiitVC\nHrRO2tYz6AtEDFox/b1U4vID06ufYrp1rIbq9D3+GPdfaduH+YIZxWMlPubHiR8PXury/9uL0h33\n2NhYW5/CODKE31hsb29rc3OzSQi5DCWuKJZsc8MSyEqJNV7imp+f1/Xr13Xjxg3duHGjeDzFoddi\nH7349/uxfqXn8dFL7hsbG811wPf93t4eVYXnqduOjUXdXktcfnHf29trGzWhBnl7XR6w/Pnu7m7b\nhbFTVWEtgecsSiWu0nEi6ch0G73OW+b7O7+h8i4a8Xc6SVVh6Qaj04W/pt9y2AOY1N6O7CUuT+p5\n9NFHizfNfpPo1XT+PN/e47azUztqp8zV/H0v1XvQ2t3dPdLGNYgboisRuE4iHyqn01A9pQt+vEAN\n0wnTSacqT09vjxfZeJH2E8gvht3GKuz0vfm+LJ0sJRfRSH0c3xd+B+z7wvtObWxsaG1tTaurq23Z\np3F0h1KSQan9Jr4Xj8uU0pEhniQdKW15avzm5mbzW9Vc4spvGnxb4vBaw3juldq5vOO6D+gdb1Ji\nEPPfzTuWx7n/Thq8Op0reckqH77Mj9XY5cX3vZlpZ2en7fjs9h3ncb4SuFRuBC9dIEvF//xuZVh0\nO2g8aMXpIGZmZtrGXtzZ2Wkuxn4BLI0M322uo1LiRdxfpYkAS/vRT+yYyTiIyQH9jtM7XksvpchL\nrYZsTz3PhwjzfkedjptSe1ZpQsTx8fEjo3RMTEw0/bc8cK6srGh1dbWpLvTScVS62A9jaSsv5fr+\n8ONGeinIn7Za9rzE9fDf1G92/DeLgcLbm1NKTW3OcRNinqatK1+8JJ9PpFsaeGFycrK5NnhJrNs6\nnUcp/soHrjxQneSOPh5kpfrhQTvuoPGgEcd987RqD1x+EY5LnKAvT7V2pWBVWqe8yitmMeaf90Dq\nS6fvOG+xVOPr5evq/aVWV1d1//79ps+fP3oQy+9s451s3khfqiqanp7W9evXmyl39vf3NTc3d6LA\nVUuJq1MbaB64SjUhw6K0DfFmxAPX2tpa01YZq+g9IJTaUo/T6TpUKsH7o5cEY3/NOB5mHtQ8+civ\nFaXvjzdl/T5fr3zgik5a2or/zoOXvzfM8gFLPT03lrg8cOWlLV/y9sDjSl1S+wGdj4LgS6kqzUd+\niNlz+clyEXy/xGQVX7yktbS0pNnZ2SbdOY5iHrMAYymzlPDhVUX5Mjc3p0cffVRbW1tNUtDo6Gjb\niBkevDxw+Y3GsHaOP0k1X94G7aUPP/fiMTgspS2p/ZqSV7d7le7BwYGmpqaa9koPZKWSUf53u+l0\nTYrHre/LsbGxpl9gTNfP21NjzUi8Thy3Hv0OXgQulTv9+fPccdWFwxi0StV1MXD4geolgpRSW1Vh\nXuryjrSxlBCdpLRV+v7p6eniPowdu/2CPqgSl/RSABsdHW0uQD4yRRx8Ny4zMzOanJw80i46NjbW\nsXHe05Djsri42AStkZGR5gKTt7GtrKxobW3t2BJXDBrDcME/rrQVS1yS2kqtp7m4n6f8+2Nmrpe4\npqammrYvb7/040gqp6iX/nZJ6YYxpdTWN8sD6cTExJEZ32dmZtpKgvmYkB60vLq223f2G4Er6HR3\nU/oxhjVgnWQ98sDhpYJOJS6/+MU+Hb3wi2MsoXpVYRxhfW5urniietptzJobBA/Ue3t7besZG7T9\nuQ/CG5c4MGxcYuktXph9f8eS7q1bt3Rw0JrlN46i0amq0H+vbiWuQV/gfR2OSzLIqwrj/vYxQ4eR\nr1fMFvSEnvX19aaaLgau6enpturkGKDj3zzuJtEf4+veRu2P/v3z8/NaWFholrm5uebmKp+01G9s\nV1dXm2rD0vfG16gqxLG63UX7HZ7XYXupIA5cGhu7875EnU6WPNkiliriBdsDVj7K+ezsbPHE29nZ\naS78nuAQU3PjOp73xat0wYhtXf5YutD6+I4xm9MzBPOOprE6xwN3HFDW1yEGvVgN1UsSzbA4yfqd\ntB16GPlNj5fS8z548TjpFLji34qPUbfg5UEnz1icmppq6+vn37ezs9PcKE1OTmpubk5bW1tN5mw+\nH5qv03kn/RC4Ck5Tehr2EykeWJ1KXPkwQidJOonvlear8r+fJyvkVWmxqjD/u9vb25qdnW0LXH4B\nyC/wF9mO02m/xIASqzVjJlxc/MKV9yH0vx8b7H00+JilGAOkX/z8onSSsQprNCw1HL0oZaX6ceJV\nh6urq5qdnW3ae/OSvHTycTs7VdvFY8SfT09PNzUrXkL3kqG3aXs7mLfheuDKA+pFVOMTuII8+aKb\nYbwInLSI7m1cccoWPxDzAVt7+e6RkZGmJNVpiSWsGMg8uOXrL7WGnfH2IUlN0NrY2GhKGJIuLGh1\navSO+9mrNeMddaxyiZ2SS6VaSU2JOM603C1wxbv22EH8sgWuTufmsAezWOKS2oPW+vq6VldXm/PA\nb1TyEfDzv9dNt+SMvHvJzMxM28ACfhzFpCGvno6Bq9u1Il8/qgrPQemgP+mJMKwXhE53PnlyRgxc\nMXuolOFX+nesKvTAde3aNS0sLGhxcbGt3tyXmZmZtpmVfSnt842NjSZo7e/vN0HLT2ypnJabr3O/\nf6dux4dflGLbXOyEnGej5h3ezVpTmPjv4QkeMXDFu/YYtGIVUCkgXjadbjiHLZDFflx+vG5tbbVl\n7nnqeTz/8sDV63Gc74c8Hf7g4EBzc3PFbi4xmcgfvYTm65oHLl+/80z6IXCp8wX5OMNc196t1FWa\nBt2TM0pVhSf5Hg+GHrgWFhZ08+ZN3bx5sxmX7fr1683zeFGO6bal71xfX5f0UkahV6n4NvlFID+B\nSr/rWX+vUoN36c7S25Q8oOQXn/wOOq/+iXN/+aMHfa9KiiWuPHjFkRZqbhfKnfZcHQZe4orHa6kE\nft5dbEpp9vPz88Wq5Zgi71X8+/v7R9rEO33PeSFwZWo6EUpOuv5+158PsFs6abr9/3ii+czACwsL\nun79um7duqVHH31UN2/e1I0bN9oeZ2dn207W2D8kv8BPT083GXMrKyt6+PCh5ufn2y4C29vbJ6re\n7GfwKokBqFR12a1kULrD9rm95ubmmskjPVXZS12xFJcnw5QuUqWuH8Ogl9+m2zE6rOdwbMscNn7+\nxD6GnjjkTQCjo6NNEoe3h5dKXBeBwKXykE+1Oi6t2D8T03LX19ePdALudBflYtWGH8CLi4vNwKG+\n3L59u0mznZ6ebi6mPgJGzApMKRXHgdzc3GyqV/wkmpuba0oVnq133IWsX79tqQG6W3ZXfMzvrEuL\nJ7l4ifXmzZu6deuWbt682cx47Iu3N3gJd319vdm3Dx8+LPbZKfUbG/Rxf9LSfb6U9l8vbdVoibUD\nsYN0zDY0s7abovw4joPvnjcCV8GgT+J+6HZRLXWEnJiYaEuF9QDTqYTg7S7e9uJDEcWA5Y9xFAn/\nu7E6Kz6W6vu9RBXTcmdnZ5uMp1Ij8XlduGLQ6hYQSxdZ328xFb40I67f2XqwioHL2wdjsov3wblx\n44Z2dnYktTIRl5eX2zqQ+/NSIBuWY75bu2zePpiX0EvLsOs14aIf31X6jjxweT+z2dnZpq3Wb2jz\nNq9SLQ1ZhRekVG0yLCfzSZUuqiV+xx0DV8xi8iy2TvIqrIWFBc3Pz+vmzZtHSlu3b98+cmH2wBUv\npt7ROU+hn5qaasbty0tceSfO46os4r45y2/b6W+Usjfzarw8KOelVn+cnp7WrVu3moDlz+fm5tr2\njWegLSwsNG1aflPx8OHDpiOyP46NjTUpz5LaGuk77a/z1kuAKSW3lEpbwx60TlLN2c99X6qazm9u\nvdo99gX0di8/5vMSVyx1XeT1k8CVOWkfiVqUtqdU4op3UV76yrODIp8Ub25uTteuXdONGzeaNi0P\nWL7kbSpeVeg971dWVpoLq09NH9PmzUzb29vN2Hxe4vIhc+KEilFe6urnb3qSVOT8wtppfMYYrP35\n7OxsE6xi8PKkljwDzafH8L42CwsLevjwoR48eKClpaWmVOrVsZ7U4TcwpfU/r313nFLA7FZNWCqN\nDXPw6pR0Udrmfuz3/Pt6KXF5X0k/bvKagXzUmIvKXCVwFVyGoNVtG/I2Lj8I40ga3RqQvcTlcwot\nLi42pa1HHnnkSPDKqwP9+cbGhlZWVpqL69LSkmZmZjQ3N9dkOe3u7mp8fLxY4trY2GjSck/a9+yi\nShH+XXnbVew759mcpWVubq4JWo888kjz3EdUiA3ovk0xaN28eVMPHjzQ3Nxc07DuJV3/fOwgna/3\nMMrbCrsFMf/csG6L1DlDMh6f/Txe437Jaw06tXHFSUi9qjCOgpOXuC6qupDAlRn2u7XjnCQ5Iy9x\n+YE3OTmpmZkZ7e/vNwepBzIvWW1sbEhS20XVg5W3wXgShqQmQMV5vTyd/cGDB02pwJ/Pz8833+9j\nqI2MjByZbThP6jjpSXKWk6mUtpzPcpzPL5YPfRXb++Lir/vjzMxMM7W7J2TEERXixVtqzVQbq3N8\n7DtPvin10ztpl4calFL+a7wBHVRzhZ9L+dBhXlXo51mnqtr8ukkb1wXId/p5NexflOPufGLgihPx\nzc7ONu0kPghuLFX5e7H9JS7Xrl1rRpTe399vOgnHqsD8uU+/4cve3l5bdaBXPXi1VhyKJnaYvIgE\nAw/ksV0qLnE+o1IjtlcT5sNixX/786mpqabf1vz8fNOfK++Y6uLdsP87Hw2lU6fWWo7zUpVztyX+\nn2GSt0UAyPiMAAAeDUlEQVRf9Dp2qiqMfQE9cJXm3et0DPkSmxjOa9sIXEGnH8Lfc8N4MuSOqyqM\nQxJ5NcDW1lZbQ2wcH8/nPxodHW0y2GK6tncq9jmn9vb2mtRsrwaMVYIrKyttSRleGpPUlPy8ejCu\nczyp4qjnF5HS7dV8pSlL8na52BYQq1bypIzSZJr+Xvx7XiXqNxr5hdkDl/RSB3NvM8unpaglUJV0\nClKxFF4qfQ2b44LWeaz7caW5Tp3Y4yDNebt3tyBGVeE56rbjLysvceXPvSHW25L8gjc/P6+UUlMS\n2tzcbEbCiI9eXRVLRwcHB7p3755eeOGFtsfl5eWmvcurJHyKBW+nibPclkpc8cQqlbhiybMfJ5Fv\nv7fDeVucl4rikFb5BHzxeSn9Pa9S9CCXT53uDeDxIuK/l9Q+Kop3FciH8arxGC9VA8aglQewGlxk\nNeBxv7Xvv3hz6F1RSjOdS4MttV/5wCUdHevsJEFsmO/mjuPByh+9YdUnHIxVhd6oH6vvdnd3m6nj\nFxcXm6nkR0ZGmmwk72O1ubmpe/fu6c6dO3ruuef03HPP6fnnn9fy8vKRTrAHBweanJzUwsJC0zB8\nXInrJNN29Ou3ioFrfn6+bftLQTyWqmJ1XacRQ/LHfNJJ/y28lJxnfsZqX0lNxmU+jFenmoQanLSa\nsIZS10U6rvkgryr0vpMnqYofxE0/gSvodefXeoJ4EMgzB2PVW2zo92SL2It+YWFB165da1u8ytFL\nWj5vz/LyspaWlvTiiy/qhRdeaAJXaR97MMoDUWw49oynWOrq1Bep0/afhpeCvETo2ZSxnc8zAL2k\nE/toecr+cVXRUf56nIU5TuXSKfh5lc1lurB3W//83M0v1rUF6n467reOgctT3/MSV0zQOGnTynm4\nEoHruOqiuNNLvfFPqsaLgEvppU7Ba2trevjwoV588UXNzMw0GXSx5ODTfMd0dJ+6wz/nSQbeOdlP\niKmpKa2vrxcvtLdv39bLXvYyvexlL9P169eb9iKpNR2D9/168OCBlpeXtba2VpyaPm8A99fOopRl\n6aPfx/EDY9Wc9NIFodQe48G+0zEotR+f3v/NS7Pe16Z0AVlZWWlKuC+88ELTtri+vt6UaOPkgfm+\nG3axs7xnZ+7t7TUjwXQaR6/fVciXVackjm43PxeVFHMlAlc38eJQuluNnzmJWk8ED1xeSnr48GEz\nBp6naHuWYUxK8Ooov0jngWt3d1cLCwtNqcADmU+ml6eMLy4uNmngnvDhv4fPXeSBdWVlpUnqiNWK\npW0rPe9VDFw+dUseuEqTccYSbpzs0Z/7MVcawzA/Ljc3N49kaK6trRWP1fX1dd27d08vvvii7t27\np6WlJa2urraVVjtVsdZwcc8D18zMTDMA7EnmiupWdXbVdDpHSgGqVNK96NL7lQ9cUv9KXDXodDEq\nlbjiCBqeXOCjv+cjPXg1WB649vb2ND8/3xa0vPSVJymMj4+3Zed5Rp0nj8TA9eDBA62vrzdtaV5d\nmG9Tt23uVanE5X2s8sDlF0wPXF7K8ra5uOTtWaUBTH1ZX1/XgwcPdP/+fS0tLen+/ft6+PBhcfs2\nNzf18OHDtmV1dbUtGcazSEuG/WJeClw+5Uan6XlqCMjDqlMAOy6onYdLG7hKd6Cx6qhT3bgHr8uQ\nOpwr7QsXZ2NdW1trLr5m1sy8Ozo6qpmZGS0uLh5J3/bqvBi4YvuLBy3PCkwpHUlgyHvk+6N3evb1\n8xLhxsbGkdlaS/qZnFGqKozVph64fJ/G6sE4RXscp9HMitmHpQ7OXk36wgsv6O7du7pz547u3btX\nvHBsb29rfX29mRLGn8cZcLsFrmHXKXDFUu9xsxxIBLCSTsGoFLTi/7kolzZw9eK4oHUZgtdx2xBL\nXKurq03atbdn7e/vNyUuzyDMM+NiyWxiYqJp0J2ammq+w3mGXt7xNl5QYxahdLTEtbW1daTvznmK\ngWt+fr4JXLEfl18049QhMZXfA9fa2ppWV1e1tramkZGRIx2bPaMzZhWOjY1pbW1NS0tLeuGFF/Ts\ns8/qG9/4hp5//vkmOMZ9EYfYiqWsPKDWeuHuFrhi94FcLInXuu3noVOVsT92KlUNIsnnygeuThlI\n3V67rDxIbG5uNoHIB9L1TrA+KkQ+7JEPyxSrwPyCWUoB7zTKtJeevApwa2ur6bzs7TkbGxvN379I\neQDykenzuc18RPsYfH0w27W1tSNLHArKH32oq3zx4bHisrS0dCQQebua3wTEdjVXy0Xbt8v3vQfk\nfNRyv2GIWZyxG4H/rfh3cTLdEoj82IyJRuftygeubo77Abqlgw6b4zLF/ELnw0D5napfBFJKzYX3\n/v37RyZ79JHHYxKCPy+135RGlRgbGzsymsbGxoYePHigb37zm03H5e3t7YFcdOLAwEtLS5qenm6b\nQyxW9ZUma8zHbfTFbw58hJJYnZ3f6S4tLTWJKTE7sJTplfeRG9aL9nGJEr4tHrC8I7X3KfQqYj8m\n4zHVbSZoHFWqFiwFrdI4nPH/kQ5/wU4SgIY5QHXTqX3P+cXBg5ZnvEnSzs5Okxhw586dI+0xnapl\n/G44XkS8GqxUavP2GK9G8/YsH3FjZWWlqSK8aJ6Ov7Ky0kyKubGxUSwZxXmu/NH7oMXS5Pb2dlP9\n6kHLg3t+A7C3t9cMRuylzzi3Vn5RztehVOoYFt2OzRi4/Bg7ODho2kzjyOUxQSgGsJggU3P16EXJ\nE5tiwEopFYNXXgV9nlWxBK4zGPZSVkm3DDK/uMbSV6zievDgQZPpl09+6I3hpZJVaXy+0oCvkpoO\nyysrK1peXm4Wz4rzwDWoEpcHLk9GWVlZafZfXPKR7D34lGZ9vnbtWlOF5x29fSqX2Da1s7PTjKZf\nKnH5euTrk4/jN6y6HZseuGP3Au8w7zcxntBSGlrLL6T+92jfOrm8tJVSKg5TlvdNPE8ELvU+A6s/\nXrYEDr8rj3e33m6TD1+Uj6PXbYkjp/vi7Q5528zy8vKRNpzl5eW2LLxhCFz+b8+ezDP14kkcl7zd\ny0fR94uBD6vlY0LGjsZbW1ttAbwUuFy37K/axJuqvJ3Ru0HELgWdApeXaGsZy3BY5IFL0pHSVix1\nXcR0OQSuPqo5eMU2keP43F2lOaVihp0vpVl+PZkjz7zz0ePv37/fLD7dSbzgD7KqMD4fGRkplozy\n+cLyjL/47729vbahpLwkUUpn99Ln6upq04/topNULpofm3nJK5a4vPoqlgBid404cokfe+gu3ux0\na+fym4N4HneriepHaZfApe7VK3kje35nHS+kV6XuPDb+x3YHfy8fxb00h1U+OaQ/9ypCH+EhzsA6\n6OquOF5izKryTLfSuG75UpqAz8d99AF7fdSQmKnpI4eUvuOyyzsM+2Npxt44K4G3d/mkmv53LqIq\nq0ZxrEI/PuOxHW9svYTrc8zNzMw0x3W8wcjFGqqzHLuXNnDFeuxOB37ptTx4xbuITsErz9w67ge5\nDBeb/CCP+6sUtPLkjHxECV9i6SJ2MB6GhvW4fb7+nsKf38iUgpZfDPK2wNgnzIe8un79ehO01tfX\nJanj8ZbLL8o1H2+lbfHAkx9reeCS1ASuWPK9TDM/90t+zYuBKz+2fd954PLalnhDG9uwXenfpz02\nL23g6kUetDo1rOfBKy9xdQpaF5EeepHiAe7/jvvEM+fyDMLYryame8d9F6dFyUtcg26viaVJf+4X\n0PwGJ65nLG154Iptfj6H17Vr19qmSPGg5bMaxxJXpyBeuqOtNRGhFLTicVMqccW2sFjiiuey33h0\n+s4a99VZlc5pSW01Cb74+esd8r3E5X/Df5OT7OPT7m8C16HSHXIpeOV31ye5A86/5zLwA9T3zejo\naHP3mzfcxkFi48gkpYt7rJqIJ038zKC3Oa8KicdKDMQuPvest8nJyWaw4ljiivN6eZuWjwOZVxXm\n+yNPFKo5i65T0lPchlji8sCVUmraGGPgiuduqTRQ+s6a9lc/xMSVGLjy65zvHz+W46gl3jab7+N+\nlrakKxK4TrKDSkHruOrCXtq4LlupK+4fqZxlWfq3P4/iPuuU1JB/bhC6pft2Wsc8aMUSlw8mXGrj\nun79upaXlzU9PV0scXVKULkMF9+THCcppeb8i1WFfvMTA5dPdxJvrI6rxrpq8nPOzJrSax684kSn\nsY0rVtsed3NwVmcKXGb2k5J+RtKHUkr/Y3j9A5LeJmlR0mckvTOl9JWzfNdplIJF6cISRzbwIY/8\nbvfBgwdNZpzPabS8vNxkdMUf9rg2rhovIt0MugQ0CGfd5jxtO/aHywcZjg3dfifr/b8GmV15UY5r\nn/Y2QB/7cXl5uRkLMp6fw1LVPOziPvYbzXiT7tdH/038Bsyn+IndFLpNKdMPpw5cZvbdkt4u6UvZ\n6++R9C5Jb5X0jKR/IukpM3tFSmno8nb9h9ne3m529OjoqF588cUmq8tHJF9aWtJzzz2nO3fuaGlp\nSWtra0emj7/MF5J+OO6Oq/aLyklLOnm/GC9BxBmefYnDQ/nEkXGm6qjGasFcKWCVxEGXPeFnYmKi\nrd9b/thptuz8Jrfm/Xda+fbHGqdYJeudkH2MzYWFhbb2Le/7edwknmdxqsBlZnOSfk2tUtVPZW+/\nW9KTKaWPH372rZLuSnqzpI+eflVP57gD0IvDcSgZSW1By0fkXllZaZuUzwNXnk12lZ31oKzx4tGt\nSqSUPFGqPvULhF8cYsfjGLw8YSWW8l0MWnnwqmVfupPUlsTA5aWD8fHxtgGeS4+dztPLEPT7qVPg\n8lFjfP68g4ODpo073kjk3WT6eW6ftsT1S5J+I6X0KTNrApeZPS7pMUmfDCu4Ymafl/R6DSBwHcfv\nEvx5rIaJJa35+flmgNU48+z29nZbcsZV6VuT6xaweg1mtWTDHddO0ulCGNv/4g1THBk/Djgcg9fm\n5mbzuW4lh5qDljsukHjg8qDlSQF5Flycs82fX5Xq/JPodn76cRmzN33QbS9x+Xx729vbWltbayby\n7Db7dOl5L3oOXGb2Y5K+U9J3Fd5+TFJSq4QV3T18b+h4sPIqw5GRkeaOd3V1tW3Uh3y6DR+1INaf\nX8Wqwl4auk9SVVhDKvdJMqa6rXuevBLTiP0CXKoq3NraOjaLdRj312l125bd3V1J7SUvH+A4JlWV\nEqyu4nlaUjofY1tXqW/m5OSkJDVtsd7Be2Njoy2hKA9c/Sx19RS4zOxbJH1I0g+klHZP/a1DxA/s\nXJ667fW63dKer6JuF/DjXu/02WGvsum0zfnzUvCNAatT4PJhpUpToHjmHIkGampGfHT8k9wUDUuG\n6jA4yf6KNQEeuKanp9vGgfRko5WVFc3NzXUtcfVrv/da4nqNpEckPW0vbfWopO81s3dJ+ouSTNJt\ntZe6bkv6vTOu64XzH87v4jjwjyo1pHdrmzhpcsYw7998m0vb36k6JKZve/8s76Ttqdybm5uanJxs\nm4PMR4W46sGqJG/b6/SZ0nO0y/dNbLfyDGsfXDofePu4xKF+6jVw/bakv5y99suSvizpZ1NKXzWz\nO5KekPQHkmRmC5Jep1a7WDVKB3p+p8sJ0HJcFUCv1QM1XGS6BexOwTell6aL8baZvM/M5uamVldX\nNTExoTt37hQDV+lvX1XdElJKn8VLTpLh54NJr66u6sGDB5qYmND29nYzWLY/Tk5Otg36HMc0zL+z\nH3oKXCmldUl/FF8zs3VJ91NKXz586UOS3mdmX1ErHf5JSc9K+tiZ1/aC+U72iwsXjc46lbbie6f9\nm8OqU8Aqfcafe3uBD0/k1V0xaD148EBjY2NaWlpqm9YlH9Vg2PfPRemlapl91hsvca2urjbtVltb\nW5qZmWlGfvHn3gYbS1zn1RG+HyNntK1RSumDZjYj6SNqdUD+tKQ3pSHsw9VNfjJ0ekS74/bLZezH\nVaoK7RTEfFDimP7u7VqeDDQ1NaXR0VGtra01Aw5TVdjdcaUH9llZt+Dix6sfm95ksrm5qbm5Oc3O\nzmp2drbJ1MwDl3R+bdZnDlwppe8vvPZ+Se8/698eNIJU/132fXlcVZUHH2/rGhkZ0cbGxpFZo81e\nmi7F+23RT7A79s3pddp3XhPgXQ58ZBLv0B37xW1sbBzpK3dc++9pXYmxCoFh4Uk+cbBSz1ot9e2K\nmaykcOM8dAskXivgAScfcDeO3eqzcnupq3S8DiqrEMAZlbLg4gkd27IGPXkmrraYEu8zbfsNVqw5\n2NzcbCaB9ZLXeVZtE7iAAcqTN2LfLrJXMWh57UC8kfI+dJubm5qamtL6+nqzxMDlf6efCFzAAHSr\n88+DGcELg1KaMNZLYNvb220zG/iIQ1tbW0cCl///fiFwAQPUKWjR/QLDICYUeclrd3f3yESxo6Oj\nxQl2z+vYJXABQ6BbqQsYFG9v9VKXG3TXAwIXAKAng76xInABAE5tEHPond/cygCASyufFDU+njcC\nFwCgJ6VZvC8yeBG4AAAnVgpUFx28CFwAgDO7qGpCicAFAOhBaaaMvKP8eSdpELgAAKdykcEqInAB\nAHqSD0N20bOW048LAHAqg+qITIkLAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAF\nAKgKgQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAqBC4A\nQFUIXACAqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFXpKXCZ\n2U+b2UG2/FH2mQ+Y2XNmtmFmnzCzl/d3lQEAV9lpSlx/KOm2pMcOl+/xN8zsPZLeJentkl4raV3S\nU2Y2cfZVBQBAGjvF/9lLKd3r8N67JT2ZUvq4JJnZWyXdlfRmSR893SoCAPCS05S4/ryZfdPM/rOZ\n/ZqZfaskmdnjapXAPukfTCmtSPq8pNf3ZW0BAFder4Hrc5J+XNIbJb1D0uOS/p2ZzaoVtJJaJazo\n7uF7AACcWU9VhSmlp8I//9DMviDpTyW9RdIf93PFAAAoOVM6fEppWdJ/lPRySXckmVqJG9Htw/cA\nADizMwUuM5tTK2g9l1L6mloB6onw/oKk10n67Fm+BwAA11NVoZn9nKTfUKt68L+Q9D9L2pX0rw4/\n8iFJ7zOzr0h6RtKTkp6V9LE+rS8A4IrrNR3+WyT9uqSbku5J+h1JfzWldF+SUkofNLMZSR+RtCjp\n05LelFLa6d8qAwCuMkspDXYFzF4t6YsDXQkAwDB5TUrp6U5vMlYhAKAqBC4AQFUIXACAqhC4AABV\nIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAFAKgK\ngQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAqBC4AQFUI\nXACAqhC4AABVIXABAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLg\nAgBUhcAFAKgKgQsAUBUCFwCgKgQuAEBVCFwAgKoQuAAAVek5cJnZnzGzXzWzF81sw8y+ZGavzj7z\nATN77vD9T5jZy/u3ygCAq6ynwGVmi5I+I2lb0hslvULSP5L0IHzmPZLeJentkl4raV3SU2Y20ad1\nBgBcYWM9fv4nJX09pfS28NqfZp95t6QnU0oflyQze6uku5LeLOmjp11RAACk3qsKf1jS75rZR83s\nrpk9bWZNEDOzxyU9JumT/lpKaUXS5yW9vh8rDAC42noNXN8u6Z2S/kTSD0r6F5J+0cz+7uH7j0lK\napWworuH7wEAcCa9VhWOSPpCSumnDv/9JTN7paR3SPrVvq4ZAAAFvZa4npf05ey1L0v6s4fP70gy\nSbezz9w+fA8AgDPpNXB9RtJ3ZK99hw4TNFJKX1MrQD3hb5rZgqTXSfrs6VcTAICWXqsK/6mkz5jZ\ne9XKEHydpLdJ+u/DZz4k6X1m9hVJz0h6UtKzkj525rUFAFx5PQWulNLvmtmPSvpZST8l6WuS3p1S\n+lfhMx80sxlJH5G0KOnTkt6UUtrp32oDAK4qSykNdgVao258caArAQAYJq9JKT3d6U3GKgQAVIXA\nBQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAFAKgKgQsAUBUCFwCgKgQu\nAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAqBC4AQFUIXACAqhC4AABVIXAB\nAKpC4AIAVIXABQCoCoELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAFAKgKgQsA\nUBUCFwCgKgQuAEBVCFwAgKoQuAAAVSFwAQCqQuACAFSFwAUAqAqBCwBQFQIXAKAqBC4AQFUIXACA\nqgxD4Joa9AoAAIZK17gwDIHr2wa9AgCAofJt3d60lNIFrUeHFTC7KemNkp6RtDXQlQEADNKUWkHr\nqZTS/U4fGnjgAgCgF8NQVQgAwIkRuAAAVSFwAQCqQuACAFRlqAKXmf0DM/uamW2a2efM7LsHvU69\nMrM3mNm/MbNvmtmBmf1I4TMfMLPnzGzDzD5hZi8fxLr2yszea2ZfMLMVM7trZv/azP5C4XPVbZ+Z\nvcPMvmRmy4fLZ83sv84+U912lZjZTx4em7+QvV7l9pnZTx9uT1z+KPtMldsmSWb2Z8zsV83sxcP1\n/5KZvTr7TLXbdxpDE7jM7G9L+nlJPy3pr0j6kqSnzOzWQFesd7OSfl/ST0g6krJpZu+R9C5Jb5f0\nWknram3nxEWu5Cm9QdI/k/Q6ST8gaVzSb5nZtH+g4u37hqT3SHq1pNdI+pSkj5nZK6Sqt6vN4c3g\n29U6v+LrtW/fH0q6Lemxw+V7/I2at83MFiV9RtK2Wt2GXiHpH0l6ED5T7fadWkppKBZJn5P0v4Z/\nm6RnJf3jQa/bGbbpQNKPZK89J+kfhn8vSNqU9JZBr+8ptu/W4TZ+zyXdvvuS/t5l2S5Jc5L+RNL3\nS/q3kn7hMvxuat3sPt3l/Zq37Wcl/X/HfKba7TvtMhQlLjMbV+su95P+Wmr9Ar8t6fWDWq9+M7PH\n1bobjNu5IunzqnM7F9UqVS5Jl2f7zGzEzH5M0oykz16W7ZL0S5J+I6X0qfjiJdm+P39YPf+fzezX\nzOxbpUuxbT8s6XfN7KOH1fNPm9nb/M1LsH2nMhSBS60791FJd7PX76r1o1wWj6l1oa9+O83MJH1I\n0u+klLw9oertM7NXmtmqWtUyH5b0oymlP1Hl2yVJh4H4OyW9t/B27dv3OUk/rlZV2jskPS7p35nZ\nrOrftm+X9E61Sso/KOlfSPpFM/u7h+/Xvn2nMjboFUC1PizpL0n6a4NekT76Y0mvknRN0t+S9Ctm\n9r2DXaWzM7NvUesm4wdSSruDXp9+Syk9Ff75h2b2BUl/Kuktav2mNRuR9IWU0k8d/vtLZvZKtQL0\nrw5utQZrWEpcL0raV6txNbot6c7Fr865uaNW213V22lm/1zSD0n66yml58NbVW9fSmkvpfTVlNLv\npZT+J7USGN6tyrdLrWr4RyQ9bWa7ZrYr6fskvdvMdtS6O695+9qklJYl/UdJL1f9v93zkr6cvfZl\nSX/28Hnt23cqQxG4Du8CvyjpCX/tsCrqCUmfHdR69VtK6WtqHUxxOxfUytKrYjsPg9bflPQ3Ukpf\nj+9dhu3LjEiavATb9duS/rJaVYWvOlx+V9KvSXpVSumrqnv72pjZnFpB67lL8Nt9RtJ3ZK99h1ol\nyst4zp3MoLNDQibMWyRtSHqrpL8o6SNqZXU9Muh163E7ZtW6MHynWhl3/8Phv7/18P1/fLhdP6zW\nxeT/kvSfJE0Met1PsG0fVisN9w1q3dH5MhU+U+X2SfqZw+36c5JeKel/kbQn6ftr3q4u25tnFVa7\nfZJ+TtL3Hv52/5WkT6hVirx5Cbbtu9Rqc32vpP9S0n8raVXSj12G3+7U+2XQK5D9SD+h1vQmm5L+\nvaTvGvQ6nWIbvu8wYO1ny/8ePvN+tVJYNyQ9Jenlg17vE25babv2Jb01+1x12yfpf5P01cNj746k\n3/KgVfN2ddneT8XAVfP2Sfo/1eo6synp65J+XdLjl2HbDtf9hyT9weG6/wdJf7/wmWq37zQL05oA\nAKoyFG1cAACcFIELAFAVAhcAoCoELgBAVQhcAICqELgAAFUhcAEAqkLgAgBUhcAFAKgKgQsAUBUC\nFwCgKgQuAEBV/n/b5SZZ9i2X0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10452af90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : [1 6 7 1 7 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10452afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : [6 1 5 7 8 5]\n"
     ]
    }
   ],
   "source": [
    "#sqr_concat_dataset_train,concat_data_labels_train\n",
    "#sqr_concat_dataset_test,concat_data_labels_test\n",
    "n=random.randint(0,1000)\n",
    "displaySequence(n, sqr_concat_dataset_train, concat_data_labels_train)\n",
    "displaySequence(n, sqr_concat_dataset_test, concat_data_labels_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (10000, 70, 70) (10000, 6)\n",
      "valid: (2000, 70, 70) (2000, 6)\n",
      "test: (2000, 70, 70) (2000, 6)\n",
      "test_labels[0]--- [3 7 6 3 5 5]\n"
     ]
    }
   ],
   "source": [
    "#sqr_concat_dataset_train,concat_data_labels_train\n",
    "#sqr_concat_dataset_test,concat_data_labels_test\n",
    "\n",
    "train_dataset = sqr_concat_dataset_train[2000:,:,:] #--- original (12000, 28, 140)\n",
    "train_labels = concat_data_labels_train[2000:] #--- 12000\n",
    "\n",
    "valid_dataset = sqr_concat_dataset_train[:2000,:,:]\n",
    "valid_labels = concat_data_labels_train[:2000] \n",
    "\n",
    "test_dataset = sqr_concat_dataset_test #--- (2000, 28, 140)\n",
    "test_labels = concat_data_labels_test #--- 2000\n",
    "\n",
    "print('train:',train_dataset.shape, train_labels.shape)\n",
    "print('valid:',valid_dataset.shape, valid_labels.shape)\n",
    "print('test:',test_dataset.shape, test_labels.shape)\n",
    "\n",
    "print('test_labels[0]---',test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (10000, 70, 70, 1) (10000, 6)\n",
      "Validation set (2000, 70, 70, 1) (2000, 6)\n",
      "Test set (2000, 70, 70, 1) (2000, 6)\n"
     ]
    }
   ],
   "source": [
    "image_size = 70\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "\n",
    "def reformat(dataset): #, labels\n",
    "  dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  #labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) #one hot coding\n",
    "  return dataset #, labels\n",
    "\n",
    "train_dataset = reformat(train_dataset) #, train_labels   \n",
    "valid_dataset = reformat(valid_dataset) #, valid_labels\n",
    "test_dataset = reformat(test_dataset) #, test_labels\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LecunLCN(X, image_shape, threshold=1e-4, radius=7, use_divisor=True):\n",
    "    \"\"\"Local Contrast Normalization\"\"\"\n",
    "    \"\"\"[http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf]\"\"\"\n",
    "\n",
    "    # Get Gaussian filter\n",
    "    filter_shape = (radius, radius, image_shape[3], 1)\n",
    "\n",
    "    #self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)\n",
    "    filters = gaussian_filter(filter_shape)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    # Compute the Guassian weighted average by means of convolution\n",
    "    convout = tf.nn.conv2d(X, filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "    # Subtractive step\n",
    "    mid = int(np.floor(filter_shape[1] / 2.))\n",
    "\n",
    "    # Make filter dimension broadcastable and subtract\n",
    "    centered_X = tf.sub(X, convout)\n",
    "\n",
    "    # Boolean marks whether or not to perform divisive step\n",
    "    if use_divisor:\n",
    "        # Note that the local variances can be computed by using the centered_X\n",
    "        # tensor. If we convolve this with the mean filter, that should give us\n",
    "        # the variance at each point. We simply take the square root to get our\n",
    "        # denominator\n",
    "\n",
    "        # Compute variances\n",
    "        sum_sqr_XX = tf.nn.conv2d(tf.square(centered_X), filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "        # Take square root to get local standard deviation\n",
    "        denom = tf.sqrt(sum_sqr_XX)\n",
    "\n",
    "        per_img_mean = tf.reduce_mean(denom)\n",
    "        divisor = tf.maximum(per_img_mean, denom)\n",
    "        # Divisise step\n",
    "        new_X = tf.truediv(centered_X, tf.maximum(divisor, threshold))\n",
    "    else:\n",
    "        new_X = centered_X\n",
    "\n",
    "    return new_X\n",
    "\n",
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype = float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    \n",
    "    for kernel_idx in xrange(0, kernel_shape[2]):\n",
    "        for i in xrange(0, kernel_shape[0]):\n",
    "            for j in xrange(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gauss(i - mid, j - mid)\n",
    "    \n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "def gauss(x, y, sigma=3.0):\n",
    "    Z = 2 * np.pi * sigma ** 2\n",
    "    return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183e01990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : [0 1 4 9 7 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztnXuMbNlV3r/VVf2q6q5+3tt3ZuzAkEmIIyODB2wmwSbJ\nIBwjAUaKHJIoExI5lg2WJg+JwYoRZCYiyAgyIcGRpSgiQEg0/yQGR2Ewdh7GE9vCA0YE8zCeYcZz\n7+1n9au6+nG7d/6oXvuus2ufenRXd9Wu+n7SUVdXVXfts+uc/e219lpri3MOhBBCSCqM9bsBhBBC\nSDdQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgI\nIYQkxZUJl4j8kIi8JCJ1EfmsiHzLVX0WIYSQ0eFKhEtE/iaAnwbwYwC+CcAXATwvIstX8XmEEEJG\nB7mKIrsi8lkAn3POPXn+uwB4FcDPOuc+HLx3CcA7ALwM4LDnjSGEEJIKUwC+FsDzzrnNvDcVe/2p\nIjIO4FEAP6HPOeeciPwGgMcif/IOAP+p1+0ghBCSLH8HwC/nvXgVrsJlAAUAq8HzqwBuRd7/8hW0\ngRBCSLq83OrFQYgqpHuQEEKIpaUuXIVwbQA4BbASPL8C4O4VfB4hhJARoufC5Zw7AfAFAI/rc+fB\nGY8DeKHXn0cIIWS06Hlwxjk/A+DnReQLAD4P4B8DKAH4+Sv6PEIIISPClQiXc+6585ytp9FwEf4O\ngHc459av4vMIIYSMDleSx9VVA0TejIZrkRBCCAGAR51zL+a9OAhRhYQQQkjHULgIIYQkBYWLEEJI\nUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAgh\nhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWL\nEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQ\nuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQk\nBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkRdfCJSJvE5FfEZHXRORMRL4n\n8p6nReS2iByIyCdE5JHeNJcQQsiocxGLqwzgdwD8IAAXvigiTwH4AID3AngLgBqA50Vk4hLtJIQQ\nQgAAxW7/wDn3awB+DQBERCJveRLAM865j5+/5wkAqwDeBeC5izeVEEII6fEal4g8DOAWgE/qc865\nXQCfA/BYLz+LEELIaNLr4IxbaLgPV4PnV89fI4QQQi4FowoJIYQkRa+F6y4AAbASPL9y/hohhBBy\nKXoqXM65l9AQqMf1ORGpAHgrgBd6+VmEEEJGk66jCkWkDOARNCwrAPg6EXkTgC3n3KsAngXwIRH5\nMoCXATwD4KsAPtaTFhNCCBlpuhYuAN8M4H+iEYThAPz0+fP/EcA/cM59WERKAD4KYB7ApwG80zl3\n3IP2EkIIGXHEuaYc4uttgMibAXyhr40ghBAySDzqnHsx70VGFRJCCEkKChchhJCkoHARQghJCgoX\nIYSQpKBwEUIISQoKFyGEkKSgcBFCCEkKChchhJCkoHARQghJCgoXIYSQpKBwEUIISQoKFyGEkKSg\ncBFCCEkKChchhJCkoHARQghJCgoXIYSQpKBwEUIISQoKFyGEkKSgcBFCCEkKChchhJCkoHARQghJ\nCgoXIYSQpKBwEUIISQoKFyGEkKSgcBFCCEkKChchhJCkoHARQghJCgoXIYSQpKBwEUIISQoKFyGE\nkKSgcBFCCEkKChchhJCkoHARQghJCgoXIYSQpKBwEUIISQoKFyGEkKSgcBFCCEkKChchhJCkoHAR\nQghJCgoXIYSQpKBwEUIISQoKFyGEkKSgcBFCCEkKChchhJCkoHARQghJiq6ES0Q+KCKfF5FdEVkV\nkf8qIn8+8r6nReS2iByIyCdE5JHeNZkQQsgo063F9TYA/wbAWwF8B4BxAL8uItP6BhF5CsAHALwX\nwFsA1AA8LyITPWkxIYSQkUaccxf/Y5FlAGsA3u6c+83z524D+Cnn3L86/70CYBXA33POPRf5H28G\n8IULN4IQQsiw8ahz7sW8Fy+7xjUPwAHYAgAReRjALQCf1Dc453YBfA7AY5f8LEIIIeTiwiUiAuBZ\nAL/pnPv986dvoSFkq8HbV89fI4QQQi5F8RJ/+xEAfxHAX+5RWwghhJC2XMjiEpF/C+C7APwV59wd\n89JdAAJgJfiTlfPXCCGEkEvRtXCdi9b3AvirzrlX7GvOuZfQEKjHzfsraEQhvnC5phJCCCFdugpF\n5CMA/haA7wFQExG1rHacc4fnj58F8CER+TKAlwE8A+CrAD7WkxYTQggZabpd43ofGsEX/yt4/u8D\n+AUAcM59WERKAD6KRtThpwG80zl3fLmmEkIIIZfM4+pJA5jHRQghJMuV5nERQggh1wqFixBCSFJQ\nuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQk\nBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBC\nSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgI\nIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWF\nixBCSFIU+92AfiEi0eedc9fcEkIIId0wUsKlYiUi/lChcs7BOZd5jlwMOykI+5gQcv2EE/XU78WR\nEi4AGBsb86I1NjYG5xzOzs7866l/of3GTg7sT4X9S0hvyPMa9eJv7X06iN6pkRIuK1h6qGjpTw6s\nl8datGpx2X5lH5OQbgbSUacbwerkvbG+7eTv9D39+G5GRrjsQGqFy3J6etqn1g0XoXApHHxIjE4H\nSV4/zX11URFrZVF1ImTh31/3dzMywgXAi5b+LBQKmVnDZUxv0iC2jhi+xgGIXASK1326sYg6eS0m\nZK3GxPBevu7vpivhEpH3AXg/gK89f+r/AXjaOfdr5j1PA3gPgHkAnwHwfufcl3vS2vbtAwCMj4/7\no1gsZh7rob/fu3cvc5ycnDQ91p+xtbBRvpGsMIkIxsfHMTk5iampKUxNTWFychKTk5M4Pj5uOrQ/\nz87OcHp66h8PO7G1v7zn8tyrnTweJS4y4Uy9r6xgtHtP7H2tzj92TcU+p5/XXrcW16sAngLwxwAE\nwA8A+JiIfKNz7ksi8hSADwB4AsDLAP4FgOdF5A3OueOetTognOVPTk5ienoapVIJpVIJ09PTmJqa\nwvj4OCYmJjAxMeEfn5yc4OTkBMfHx/7n0dER6vV603F6eppZr0n94r8M1nrVx5OTk6hUKpljdnYW\ntVoNBwcHqNVq/vHh4WHLicEwE7pSYweQnRx18lhJ7bq8qLejXSBQ7HPs56XWT0BzX7U6h07f10k/\nXPbve01XwuWc++/BUx8SkfcD+FYAXwLwJIBnnHMfBwAReQLAKoB3AXju8s1tJpz1j42NYWJiAjMz\nM6hUKpibm0OlUkG5XPYWgD1UqI6Ojvzjg4MD7O3tYXd3F7u7uwCAk5MTAI0gDo1ETPHC7xWx9cKp\nqSlUKhXcuHEDy8vLWF5extLSEnZ2drC9vZ35WSwWfb8DGAnRsn0W+2lfB+4LkxUove5ir1kG5drs\nVCA6bW9sjSccA1p9xjBMOjuxtuz7LvM5nVp0182F17hEZAzAuwGUALwgIg8DuAXgk/oe59yuiHwO\nwGO4IuEy7clYXOVyGQsLC1haWsLS0hIqlYq3vOzPw8PDpmNvbw9bW1soFhvdc3JygoODA/9Fnp6e\njvx6mBWtQqGAQqHghWt5eRkPPvggHnroITzwwAPY2Njwx/j4eObvgcZgfO/evehnWFIebJSY4Oth\nnxeRJpE6OzvzkbD2ufDxoGDF5Cra1cpaDRkGwQrp5lzaufp68RnXSdfCJSJvBPB/AUwB2APwfc65\nPxSRxwA4NCwsyyoagnYlhBetuqxmZmYwPz+PGzdu4IEHHsDCwkLGfaiHugEPDg78z93dXT/AqmgV\ni8VMyHzK7oZeoP2tolUsFr2rcHl5GQ899BAefvhhfM3XfI2fNNiJgEZwqmgdHR217c9h6e9Q9GOP\nrXDZn2dnZ74fwlSOQYrgjFlGvW5T7N5vJVzad/r7KDFs53sRi+sPALwJwByAvwHgF0Tk7T1tVYfE\nZq/FYhETExOYnp5GuVz27sK5ubmmoIHx8fHMTa9BG2NjY976Cte57HrYKLi3WhH2//j4OKanpzE7\nO4uFhQU/aTg9PcXJyUnGDatrhuqeDVMTUrdoY25BvT7DI891GK5l6eB7enrqA1r08b179zKP81yI\n/cSeU7gubR+HfdfKvRpaq3nXje0j21fhpGBQ+iwM1NGfrSzMcJ0z9j/C917UzdxqnfU6+rBr4XLO\n3QPwlfNff1tE3oLG2taH0QjYWEHW6loB8NuXbGeUcMavwqOBGHZQuHfvng8M0L/VGyn2JUxMTKBc\nLmN+fh5nZ2coFArY39/HwcGBDzTQgYNkB+rw+9CAGBtxODU1haOjI4yPj3tLI+//AunNGAuFQia6\n1fZDeMSCMfLciToJsBOok5MTP9E6OjryPwfx2rQDbszajAm7jQK2j2P9o9VwQnQ91fbT0dFRU2DW\nycnJwFxrsXOz453+DN3H1rpstXaat17fToCs0NuI4Fgbrope5HGNAZh0zr0kIncBPA7gdwFARCoA\n3grg53rwOU3ol6FWlg4Ok5OTmJiY8F8q0JhxaQCGzvKPj4+jAwnQCKkvlUo4PT316zc7OzvY2dlB\noVDw/2+UiS2M25vLDtg2IEatXv2ObD7dsFAoFPy5Tk9P+3VVu8aqB4CmQcA5F03pULeqFanDw0Ps\n7++jVqthf38fZ2dnGXescp2uxFYzfhWtmEDZ6yS8bsJDrzMrfLFzdM5lolq1r2q1WmapwDnnrdVB\nwE4C9VzDqOiJiQkUCoWMeFiXcmwd1V5r1urMm8SHqHs/PGKff1V92W0e108A+B8AXgEwC+DvAPh2\nAN95/pZn0Yg0/DIa4fDPAPgqgI/1qL1hezIDpB0UYxaXrl/t7e35o1wuY3Z2FjMzM5idncXs7Kwf\nKEqlkhet2dlZTE1N+Yvk6OgItVrtKk4rSWIuW51Q2MMO2HoDtrK4Yp8zKANLK3QQLpfLmJmZwczM\nDMrlMsrlMkqlkv9ZKpX8gGlzCM/Ozvy1bH/qdRweOpCfnZ3h+PjYexaU61hzCom5BfVnbMI5MTHh\n01dsKotdk7a/62BuD2u92kF4b2/PR7TaY29vD4VCwX8Hh4eHV9onnRJ6L3SMs/ePToKKxaJ3fdrD\nTiKtdRtzm3YarQrAW/1qBJycnPigNZ0sXfW11a3FdRPAfwTwAIAdNCyr73TOfQoAnHMfFpESgI+i\nkYD8aQDvdFeUwxXeAPpF6mzNDojqKqxWq9jY2MD6+jo2NjawuLjoIw/lPCJR/5+K1szMDM7Ozvya\n2NHREfb391EoFK7itJIgzwcfu9k6sbjaCVcqgqXo7LhUKmXWWXWCZH+q2FiPwNnZWWbAVqE7OTnB\n/v4+9vf3sbe353+qu0hFy/Zn+F1ZQbnuPo1dI1aYY/2T99O6y/Sndf/bY3t7G5ubm9jc3MTGxoZf\nTlDROjk5Qb1eHyjLX8c2ew/Z60GviYmJiYwQ6RG67XUiHyuyYHNUrYjFUKvfektCC/uqo1y7zeN6\nTwfv+XEAP37B9nSFdTnoF6vCZS0u4L5wbW1t4c6dO3jttdfw2muvYWVlxbv8JicnMTs7i3K57C24\n0BWholWtVn2U3CgTLhyHfnhrbYWuHmtx5QlhqqirsFQq+WCVxcVFn1eoQlapVHB6euoDgdT1d3p6\n6j0A9jg+Psbu7i52dnb8z1KplBGtiYmJ6ETA9ms/BCtcbwnvWyvy8/Pzvo/yfo+5GkPh0sFzY2MD\nq6urmJmZ8aKl62Fqae3v719rn7TCrgPqWGSvJ3tMTk56EdJDhSusHFQoFJrW9PT9rda9LCcnJ02i\nFVYVuup7N+mR10ZZ6RdRKBRQq9WaLsx79+5hc3MTOzs72N/fR71ezyQc7+/vY3d3F9PT0wDQFH1o\nrYRQFEcROzDodxBGa4U+dCDrUsyLjkqJ0B2jN/TCwgIWFhb8gKtJ8OrWOTg48D/DdaujoyPff5pQ\nXyqVmhbYbbSmnTn3KzoulpOmlmcYnKJiFeZVWovKHtPT016QnXM4Pj7215z9DmI5ceEk4ujoyLtj\nrWjlCf5V0moSEZ6DipD2nS5zTE9PR6NNwyApHbNCkdP10DC60qYPaHsA+D4LA4JC93WtVsPx8XF0\n/eyyJC9cevPahVl14emAoIOECtfBwYGvhKHvqdVq2NnZwfj4OJxz/oZRd2E4a7EDb9imUSAULaA5\n5DjmPweay0WljIhkZsQ6wZmbm8sIlw4wAPyNDsBPrMJajgB8orsNFNK+PT4+9laa5iDaAblffRFa\nQCq6dm2vXC43rdXYiNMwkEX7Vt2hh4eHmcjLMFIxjOYcHx+HiGRct3rt1ut17O/v+3s/FlJ/Vfd0\nOwvYWqcqXnYtX/tzZmYm2tZYAIwKV3i0Ey7bxxqYZqsNHR8fY3t7O1MhJ5Zq0KtJ1VAI17179zIm\nqw4GNm/o7OzMu1cODg78TMAKl53RaQi8Rn3pjNEGE9iLfFQES1EhspFr4Y1gDzuYxqIRU8VaRKVS\nCTMzMyiVShm3llpcExMTTfmBh4eHvuhw6OZR0ZqenvYuRY0YtMJVq9X8/7Gz5370hY180wFWxdu6\n+0qlUjRS0FoH4URRAwvUwgSaQ7cLhUImilOfs3mGGrgAwIvW9PR0k3D1+57Os7h0PV+vt0ql0pRW\nEEYk2sjEvPszPHRyEB56DYbuxvX1daytrfnJv9YjtdZgr/o0eeHSTrRuQytaehPp7Epnp9biOjw8\nRK1Wy+RD2MAMHUBsLon6d+3CpP05CoR+7Zi1ZcNkrcWVuotQseIyOzvrB+bZ2VlfZFgPXSM9OjrC\nzs4OqtUqqtVqxu2lN7i1Vubm5nxelrW49Bq3Flcr4brq69OuW6l4lMtlLC4uYnl5OVPDUmuHxlJR\nwgmNvbc1aT020J6dnaFYLGJmZsav2aiQ2gmGPhYRH7RhXZHXcS+3i/IM1wNtioldE1TXamhh2nEq\njCzMy8GKHbEEb7v8Yg+NDVBX7u7uLg4PD305t17mdiUvXNbEjfm79bFGDtmK5Pr3R0dHmfecnZ35\nslHa6TFX4SDNzq4bPV/rBgwFyw7EYZRSbCaXIjoI2oohS0tLmRB4feycw87Ojheu1dVV3L59G0dH\nR03XsnVrLS0t+YANu57bqcWVt1je62tWXYXWIpidncXS0hJu3bqFBx54wB8zMzOZtS+9t2KVQGwh\nbF3TU7do6J7WXDfgfmSnurxCASsUCtjc3PSpLoNoccWsrpjFZd3U+jgULCtcsUCMVsIVJnjH1tQm\nJibgnPMBbBsbGz7gJbZmdhmSFy7b6UDcDWUjjcL3q+DZWnkTExOZ9QK9IWMLwKNKeFPHAjLCCzuW\nXR8u3KaGDiwawGPXczSwR9dcdZKk66mbm5tYW1vza1q2D6ampnyCrF6LdoKmwqWL4Op2bOcqvMp+\nVovLBg9olKAK+o0bN7CysuLXj8MKEHaNT68X60GxNUVje+jp4GnTEWwyrn4f4+PjODk5aconHKR7\n2rpBw0jJ8F5TbCRhOF6F4mP/d0y8nHMtIzdDkdNoa5tuEAvE6gVJC5diXVD2CwlnTrH8BBtSbxfZ\nYzlGYZ5CbMBNdQDuFbGbTGfDdr8zXdC1C8MpYs9RrQENHtDw9L29PT9Qrq2toVqtYn9/30cP2lm1\nHupqs9eiXt9qhaiLe29vz1tdecIVWl1X0d8qDCpc6saamprKFFiu1+t+sLXt0BzJsHRVbAcHdUGF\nazVTU1OZgBbrpgrznOzEoNWecFfhMmz3fWh79ZpSsbHR0sfHxz43NQxwUYsrT7TyRDE8YrmEmuge\nildsImEDtHoZ7Zq8cLXzRYcuk1BwYjklNg8sVkom/N8pWwy9IhaJpYd1cVlXjx0s+hUJ1wv0prXC\nfO/ePZ/QqgPG8fExtra2moQLaI4AK5fLmZxEmzOjFpdaHpreoZOBQVjjUjeWhrIXi0Xfdl1jDjcS\nVVELg1fUDRpGscXWasrlcmZt0AZxhBUf8tYGY/fzVYpX3nel6/d6b6nrTp/XSkB5ew3G0gPCyXzs\np23P/Py8PzRgbXJyMmr15UUrxiKLL0vywgVkL4A8q0t/j12QYcROmK/VyrylaDUTujRGyeJSSwhA\n5uZWa0I3KLXCpYOLWita4Nnu3K3CpZ8VWly2/E6renvX7SrMs7gAZIRJH4e5QFZYwiNmIczOznrR\nqtfr3uIKXaz6WWrVqXC1sgquYu2r1fek15Rtv3739Xrd552quzM89JqxrjrrJmzXBhHBzZs3cXh4\n6ANfpqenM7UR7aQhZm3R4mpDNzeqfU4HjdBV2MriylubSXXw7SV5/ni9iO2MOUyaTRG77hRaXGFu\n1tHRUcaiUOGKRYx1YnGpcO3v70dzcq6bmKtQK1WocGkU78nJiS9ZZUtYafFbWxRXIypD12DsHOv1\nOubm5rC8vJyJHg5dbxrUErNUw6UE5br71Abr2GvLup/zDhsh2U0QlI0NGBsb86KlaQZzc3M+4CUU\nrphoqUcl1reXYWiES+m2Y1pZXO1chRf9zGGg3Q0QugpjFleYtZ8q1qq0azLhBqW6LhOmCgD3I+Ba\nBXjkuQo1T1EHuX55ATSIKVzj0kkgcN/iqtfr2N7eRrVa9Ymr29vbGSFTMVO3oD0/XRsMA7GOj4+x\ntLSE3d3djMVlhctadjYaU7+P0FNjz++q3a2KtlcnRfr5sXPWCUOsSkb4d/p77NzCQy0rtbTm5+dx\ndHSEqampaDh9nqvQvqdXDJ1wdUuYkW6TF8Oah622Thg1wlBdtVrDPrIWibU6dNCILYqHrt9BJgyW\nsDUtw01I1cKykZbO3Y+A05D6SqWC+fl5HzIucn93A+23WPh73vrMdXF2duYFdX9/H9vb2/66CI/D\nw8NMvUX9qZaW/ZnnFoxVhw9zmfT7UMHX70VLvKnVFa4NDso12IlHR6+PMNk4DCprZXHFKo7oREpd\nj/rdAfATABtEs7a2hq2tLb9RrKYWWRchLa4OiQUNhK+HxT6tcIW1CRkK30BnZGFyZEy8YiHcYdWI\ndv7vXl/4vSKcyetgF1vLC9MBFFtLr1KpYHFxEQsLC76AqloS6hY8ODhoWpfRtijXYRWEqBtwd3fX\nD3Aq5mH1huPjY29V2T2yrBtVB76YaAH3XZO2DqLWhAzdrAD8BEOtVF1rtMIVY9CuuRjWClUBjllZ\neeOW3s96Haq7emFhwe9qYJO0T05OUKvVsLu764/bt29jY2MD29vbPl0h/P56xVALV/hlxcz8cH1L\nM9LDUGSbE5FywmyvsNGYti6dLYmlfZRXXy8UrhiDPmhY1x2QjTAMq3XbNQt7I6vFpXlPmvOkrkIA\nme1MbN+1crNedyLt6Wmjyv3e3l6mzbFSRGEEoS3YqmuC7QY+nSzZEHDd3UHvX713gWztUhUua3GF\nQS151tagXpM6iXLORcepVmOWTsx1DznNv9MdDbRP1QOgwrW1teW3iVpbW8P6+nqmHmzsu+vFpGpo\nhSvPPA7Fy7oKtahnqVTKuApj9QkpXPG9t0KLK3QVxtYXYsIVGzQGccCwUX72PGMVIPIWqHXAsBbX\n0tKS71u1uE5PT7G3t9dkccW8CEA8v/Eq0VB3FfN6ve4txjAk20Zh2p+xhf3QjWz7TSecui6owmXv\n3zABXIVLd4poNQkYxGsuhrW47GOl3Xil4fNaMUgnTwsLC6hUKhmLSwMx9vf3M9tEaQmz7e1t1Go1\nbzHbNtr2XKZvh1a4gGbRis1AwxyumKswr2rGKItXKFzWpaqzt1auQi103Crx04rXoA4gOkjrOVrB\nDq0rK1rtXIVLS0uZHBh1RVqLywpXq2CC68JGDNbr9cx1EE4kY3lAYdWGdiHUdsJp6/aFrkJd44q5\nCkOLK0XCdTmg+8LVmpah9Vnn5+exvLzsLS4VrsnJSf8dq8V19+5dvPLKKz4qVK/RVmkZ2saLXp9D\nLVxKq8HPurzCyhl2bWsURSrvwrKiFe5wbK0uK1xhZYkwwi5VVyGAJqugG+waqw6+Wkk9lohrq2TY\ndZlBiITT4IxOiIlZ+LtOivS58O+tWNkNJlW8YhGZYQScjerU+1+DGvICXjoJmLhuulkDjkUQFotF\nPwHQydONGzcwPz/v+1KjDFX8d3d3Ua1Wsba2htu3bzcFa4TC1csxdCSEqx2xBUy76J6XezQoF+1V\nEFsftNgb3S7mqjtBBd9aHjZs/KoSEwed2KBhAwxssVQVJi1aure3h+3tbe8uDAMKYpFwgxLUEgpT\nWKfQWuqxI+Zu1A0ntfr+zMwMFhcXsbi4iEqlglKphPHxcQDwId1aCFl3Pbd7Wun/UusrrwKE/Zki\nNvVHrz0VKq3gv7S0hMXFRV9C6/DwENVq1Vurd+/e9YEYsSLP4USu1xN/Ctc54UAdVnzoJsM+dTq5\nyPKSTe2iuJ25hjf8VZWCGQQ6sfBtKoFaXHYfq8nJSdRqNZydnXkXoa4fdBIJl/f5/SAUanvtqHfD\n7scVillY3Vx/6qaUdoPKubk5LC0t+Y07dRNJtSh0B+TT08YOwaFoVSoVH9Voj3CbexvxOCjk3beh\n1aPb8NhDt52xW88sLi76e1MDqpxr7HBghUurwISVcNqlEnCNq0fYWaFaXLo2UywWo4mKef9H/0fK\nWPdS+DhKjdYNAAAgAElEQVTMfdMtPNTi0rUFIFsCylpcw2ZtxdxZscAJ7T/rbg1druqyshaXCpcN\n0OgkhLvf/WsFS0Xbuqb0sNdOuN2JFTH9aQdeDazSwIKYxaXCdXra2Fla13PUYqtUKn6jWc0hC0tP\nafttYvAgkhdJbYXL7hUX7pe2tLSEpaWlpor8BwcHfl3LCpctetzOdd6L63EkhOsiHaUDrboKC4VC\npjBsnqswL5Kn34NHp3Qy+ALxiK4wDDnPVWgj7IbF4gq/9zzBt+uDNv8t3DW4lXDpQBKzuNrNcvuF\nFS67DYxGAqqrL1Ys1gqYjV6NvTe0/q3FNT097aPtNP1gZmYGe3t7qFQq2Nvbazo0J03bDdwPNhrE\nfgbyr0X93QrX/Pw8FhYWcPPmTdy8eTNqcWkwULVaxebmpg9/D4UrrG4S+/xe3edDL1ztItNiaw5A\n1lV4fHzsq3u3chW2upCvO8LrsrS7KcMUgtDiirkKQ4tLX7M/UydcJ429bvOZ7GBsLS5N9LSuQi2L\nZN1XMYvruoIxOiV0j9prR3OG5ufnMTc312RB2XxKK/DWMgv7UHO6NDjDWnhjY2NetA4PDzE7O9tU\nZkr7WQMSbF/aCNlBJG9t2q59al+odXrjxg0vWnqoxVWv17G1tYV6vY5qtYrbt2/7rXlsmS7dDDWv\nTb2+FodeuDohnAnrgKvBGeo2tNUeWkXCDYurMA91ddkyRXNzc6hUKpkkRQ2PjhXUHSRX1nVhIwit\nlaAVzXWgBLIDpE0jaLcIrgxKn1q3sj000VqDKbRaiE0mVuszz+LKe87mXmq6QiwQyH4fWn9PRDJr\n23adS70vgxhpnNeWsOxTmOi+vLyMlZUVLC8v+0otwP3kcS3Ftb297ZONNzc3fYFkLe2kE6iYx+kq\nrsWhFq6Y26TVmoP1nwP3qzPrABImzcZCPVuZ6SnQ6WxdXTW6PbvOmEulkt97qdWeR5ZhF3rFuqzs\n2o5WJlDBV/dguP+UrSiRUkV9vVbUipqamvJ5QktLS5k1ldj2HGGtUP0ZCpUeecnvtj9tn+qgG04q\nwse2LSoIg0BMtJxzGdes9pkmGFvhunnzJubn5zE9PQ0RwdHRkXdJazWMjY0NbG1tYXt7268DWtHK\na0Or3y/DUAuX0q7D7Jerhw3O0DWYVoVhgdYlplIalNsJvg7AVrh0QVz3jwLgkxTbCZf9v+FnDRO6\nvmBdZOomU+Gy61qxzRO1D1OJbLXXiq6Dati6jWRT91QYjGGDfOz/BNDkIgz3z9N7WHOP1HLVXCM7\nCbBehNjamSbe2hzFfltcrcRCXwvLsqlbf25uLmNxlctl/3eaM+icw+rqKtbX17G5uemFa29vz78n\nrI6RB12FHdJpTkvoJtQL05bs0aOTGnH9vph7RbsLzVZ7UIurUqn4SYBaXJpK0OnWEUCalmonWOGy\ng4e1uFS4YpaW9mFYYaLfgt8qCEkFwVYG0QRXne3fvHnTu6tilhWApnM+OztrEi0NaNGJogqXcy7j\n9rMuf9tea5XErK2wpFk/aTfO6HVhK9uESe76HaysrGBiYqJpU896ve4tLitcu7u7mTqcKvyx6++q\nrsmhFa5OiS2WW+GyA4huahfW18uLxBvmAdhWe1Dh0lBj23dasy5mcdlBNxz8hrHvdPZrhWtpaSnX\nVWg3oLTiBeTv5n3dfdZJFGrMOlcX4Y0bN7CysoJbt27h5s2b/n/Y6yLc60knQNYq04F5bGysaU0r\nXCvUQVmjhUN3Wp67cJCES8mbNOg9FCtpF7oKV1ZWMDY2hmq16sVd90pTi8u6Cvf395tKdGlbrmvt\neiSEq1UHxta4NGfL1ogLC8O2q5wRWnvdtGmQ0f6yeTgqXKVSKeNC0DWubvptmAldhTrrtaHbALxV\nlVcx3f6/QccGZ9jdnW3Cr1Yir1QqGWsptJqsS8pam3bSaesS6v2r97DNRdIisGGQhxXMMIQ/Vnex\nX33a6vPD4BNdX9R7Vft7fn7eh8OfnZ15QTo4OEC1WsWdO3dy17fateOq7/GhFq484bAzA7vIa2dX\nOrNVy0HNZ63BZS2u8HPsDWVvAH3N5jbZmfMghobb9utPm2tko8AmJiZ8UqyGcO/u7jZVNE8lqOAi\ntHOB6uxXB3BdF9Q6cCr6Jycn2NnZ8Va+Rrfa/2Uf9+uaaZc3psFNtiq7XSdSi3xvbw8bGxuZ+8L+\njFXb10FY88BUeGxBZxUsey1qfpauWYXVOmwR3vDQ8kattuK5Ltq5iYvFog/+0eApLe1UqVQwPj7u\nher4+Bg7OzuoVquZrUqq1WrTxpCt2nNdDLVwAa3FC0BmtmaFq16vA4gLV16Qgf09FvCh6xdhVexw\nhhn+r35irVE9HytWoXCFSbO6mGtv+EE5t17SahAJc2hCl02xWMyEYGvqRThQdrKm2g8Ra3XuoXCF\n4f4qWtVqFZVKpckFZQNQwvtjaWnJr1PpIF0oFDI7HddqNdRqNT8oa+6R1t0Ld1AeG2vsXh3uwqz/\nxwZoDcJ1nHdNOOcwPj6OUqmE+fn5pnJOlUrFb1Gi56Uh75ubm1hfX8f6+npm4qleKP2MvDZcR78M\nvXABzZE29nFY5VwtCZ0RWuFSv3hMuGLBIDaiR/NK7IxR/fF2Zmnbe520GvBsH1nXgz2mp6czfWaT\nZtW9MOwWVztrS/tRLS4VLgD+2lILwW670Yng93tNsNVnq5DUajUA9/Mjtdp9tVrFzMwMSqVSU01L\nvVZiBXdVtHSADtNX1NLSz9AcJK3+oJtchkdsW/p2lc+vg25ckzpJssL14IMP4tatW75YwMTEBE5P\nT32ldxV07Z+1tTU/7qnF1YmFfR2MhHCF2I634qIWl+ZrANntvruxuPR/W/9+sVjMLC5boWsVmXOV\ntHM52QVejeKKuQm1inSsTJHO2IbZ4lLy3Hn6u7W4dJ1LXYPqKtRBpJWrMBWsxQVkPRh7e3tNFntY\ngFnvCzsB1McAMlaF3pP2M3R7eXV/ra2tYW1tDaurq9jd3fWucOsOV2ENg0HCn9f5fXSyjhS+R/tm\nbm4ON27cwEMPPYSHHnooE4SmrsKYcK2vrzft5J23b951M5LCBeRbXOoqtC4Na3GFFeJjhOtnVhC1\nfJR1rdiboN8RdeFn25BaW9E7tLamp6f9IKMVM+w2HHYAHlaLKyScnYZrXGpx6QBrhWtra6sri2tQ\n0ev78PAwIyixbU100mO9Ero5Z6x+oeYlzc3NoV6v+3vSugr39/e9m3BjYwOrq6u4ffs27ty5g2q1\nCiA+eQvXoFutTV81nVpa4fVmLa6bN2/ida97HV7/+tc3CbC6UkNX4draWvT8w8/sByMnXDZgIpzJ\nhRZX6I+3bgsdgPR3G1ZrC9Da4qm2VIzdz6efImUfx9qh56pWgkYmqWtHZ8o6A45FX40isYg0W8rI\n5gYdHR35CDg7+7VBLakKF3A/ERhARozC7V3U2gkDMbQS/NTUVKbAdV7tUFtjNBZNaI9UiU0uw8nA\nwsKCjxycm5vz961GVKr7tlar+cjBnZ2dzHVnP2OQGBnhig0keWVjdEBRMbKuBBUsnd1ofoctAmrz\nPeyh/8f64WML0YM0QGlei90CQm+EcrnsK2WE/WmFTgeQcGdkSzvfeWro5MVWLbA1CW1FiDDtQiPg\nbBX4POEKLfRBunYUa6nY587OznwhYb037NpWWBkkVpjYTgattaTuPh2g2xXIVlpF6fWLWNSyRUS8\nFW+9ILdu3fJ5gnrd2UhO3cF4e3vbJxnv7u769axBZmSEC2hONg6Lf+ZlyOsAowOzXkhaGirclG16\nejqToKgXmt4wKlp6o/ZbtFpFpmkfhaViKpVKRrhsPpwKl+aOHB4eZiYDsdQB+7v9mSrWUlWLW/O1\nrNgD2WAFG4qt4dyx/LdYJN+g9lkoXPq7Xv/WA5IXVWiDW6xo2WhAJdzZoV2RZyB+D4Tn0E9aLSGo\nK1W3iNEUgVu3bmF5ebmpMosK187ODjY3N7G2tuYDV3Z2dlCv1wd6rzFgRIXL+tVDKyvcvM4OtDow\n2wF6bGwss4OqPla3h936W2fOR0dHme0S7I6h/bS4YjeGWlxWuHRr9JmZGUxNTWUKj4YRmqVSCQcH\nB7nCZen34NAtrWbnth90MqPuVbW4bDHnmHCpxZBncdlo2UHvOzt5s+21LmV9Pra2FN676p6OWVxW\nuEKLK7YJrP38WJvta4MYuakWV7lczlTcv3nzpheuUqmU2bVBLa6NjQ3cuXPHi5Za+rS4BgTr7gu3\nBu/EVQgg81hvqGKxmPEj62PnXNP23xqVqOtddoY5qLNmdXnpwKuZ9jFXYWhxqatQ13XytoNo5woZ\nRNqtD1qLSzfaDIVLqzxoANDh4SFqtZoXrjAsPK9fBr2/wsAjJfY4vA9aBQrpPdrO4rqoqzAmVoM4\nSVCLS+9PLaOlVfdjtTAPDg6ws7PjhWtrayuz2zEtrgGilXiFobaxNSoVG/t/JiYmMD8/72c5S0tL\nWFxc9GGmNolR18R0AAfuFw/tZ5+0QoUr3PQvXK+xrsKwNpqKW7s1rlSItT8c0Gy/qehrn1krFUDG\nMtA1rr29vWs7n+vgMhOzPIsrb41LhStc47IVb2KuwrzJyCCKVegFCjeGfPDBB71Lf3Z2NlOdRa8z\nFa67d++iWq16cddjkBkp4QovagB+RmYjj3QmPDk5iUqlgps3b2b+Rv8X0HAJ6ZbjIo29bPb29vyW\nHvbQhFzd6noQcnM6DYqw6xStDpv70un6gjKIA0Q78tqsica2oK66bGxpIlupvN22L6NKGBilk4Ew\nAtgKl16Dttq5Rmfm9XHevRCuLfYTGwGtVufMzAweeOABrKys+OoYi4uLKJVKEBEfbKFW/Kuvvoq7\nd+9ia2vLb1FirdFOJ9L97IuRES476NowdFvTTK0jDcCYmJhApVLB6ekpisWiF5qwKrJeQEBDCPVi\nCLcE15m03Tl0EHKa2kWm2bWGVqJlw5hjs91WwhWuc6RAK8GPBbXYfbdsvluY2D4I18R10k4sQheh\nrZUZWl1WuOyktNvJQZ6w9RvNzbJHpVLBrVu3vHAtLS1hYWEBhULBp1jo/Viv13H79m3cvXvX72Rs\nrz3bN62u7373xcgIF5DNJ9EL3O7Ro4mgGu6uFpfOamygRZjhr4f9X1rQ0x6xEir97I92i86haLUT\nMCtcocXVytqypCReecSCWubm5jA9PZ3ZvsQOqq2sAWA4+iUkFhgRTqRCi0td0LGcSyCex9VpH4cM\n2tqzToa0wrtG+eq+ZhqQsbi46As1ayCGJhlvbGz4vC2dZIdbwQz6eurICFc4+OoMLWZxTU9Pe4ur\nWCxiZmYm436wmef25tCcG1uN2h77+/t+INefg3IhKO0ErFuLq5Nde1PP39JBNkwjiAmXWuetLK52\nFikwOAPIZbDnFAZFxCwu7TsrXHkWl157nVpcsUncoIkWAF8tpFKp+D3N7KEW1+Liog/yqdfr2Nzc\nxJ07d7C2tuajB3d2drC/v58pKaYHcL37a3XLyAgXcD8UF4AvTxSucdVqNZTL5UzlC/Upayi73dRP\n8yG0XI+uce3s7DQd4QZsepP1k4tYXaGVGVvjCnNohsUNlhc6HRITrvn5eQD3+1n7qV0psdSFvRVh\nODzQfE3mrXFp1RabZhFGFOqE1FpcKVgUeYTCpS5CDQxT0VpcXPTepXq9jo2NDbz66qt49dVXM14f\nFXV9b8igWvmXEi4R+REAPwHgWefcPzHPPw3gPQDmAXwGwPudc1++zGf1inAGoQV0ta7exsYGRCRz\nc6gFZv3mKmBW8NQdqEKl61kaVahbpQwarS5MW/vNnqcNgVcX7NHRkU9krFar3srU8jHDtJFkq9Bu\nAJlBVrdLL5fLTYVb1erqNjhjUAeUqyC09q1lH7qlrfvf7selz3W7xtXPPlbBtpHMei1pPqVaWRqx\navd10wjVsA6hnVDqPWkn9frZg8yFhUtEvgXAewF8MXj+KQAfAPAEgJcB/AsAz4vIG5xzxxdv6tVw\nfHyMWq2GarXqaw/WajW/+GsXgjWJ2B5aFFXNchUvFSyN2EnV0lBB0rU/zbwPxV6rnKtwqR9dq8O3\nKrIbisAgD8gxawDIFm0GstFfthCxzm5t7lZocQ3y+feaTtxSdtKobnjtUxukUC6XcXZ2ltk7SyeM\nrda4rHtyUCYEItJUhLhQKGR2L9Zk4/n5eRSLRZ+Co5tCrq6uYnV11Zdy0tc6CcRoF7DVby4kXCIy\nA+CX0LCqfjR4+UkAzzjnPn7+3icArAJ4F4DnLt7Uq0HD1re3tyEiPmzUFsfVn3Z2Z2d5eoPYvC2d\n7Q2DcGmEpAYU2L221JLQ4JWYi9QOHnk7xw7SoNEtdkYcCyTQNRmtoK+WQbe7DgwbsUmAXWOxP3UC\nZQOowui6ToQrr49j11+/rS11N9sjT7jUHa8FdE9OTrC+vu7D3vU+1B3K7Xpz3n03yPfiRS2unwPw\nq865T4mIFy4ReRjALQCf1Oecc7si8jkAj2GAhUtF6+DgAFtbW9EyUDap0R7WdWgft9vHJgV0wAj3\n2bLFPPXx6elpZsDQx2H1glTXF5TQSrCCZXcHsKHb2k9HR0d+LUbDk61wpVwF/qLERMtiw9vr9Tom\nJiZQKBQyglUul7G/vw/nXJNo2b3gWq1xXffkqZWXwQqXvX7yhGt3d9d7RtTzs76+HrW4YrVR89a5\nB/Va7Fq4ROT7AXwjgG+OvHwLgEPDwrKsnr82cBwfN7yX6v7a3t7OFPG0FTV0sAnXKOwWDDZc3vrj\nUxUuXTOwLkPbJ7avdM0w5kO3fZNqX4RY0bLCZas7hK5CraCiFldsg9Jh6Z9OyBOLPFfh4eGhD31X\nwVKLX4VL3fR2bdleh4MQnBG65mKRlHp/6TqpVq6JCZdujVOr1bC5uYnV1VUf8q51CFW4OnX/Dapo\nAV0Kl4i8DsCzAL7DOTfYNUE6RC2ig4MD/5wdfGy5p3BROM/tNUzoOdu9eYBm95idsYXHKKDCZa+X\nmMWlofDap2oZDFvwSjd0MoBqgJAtlVYulzMWV7lc9hZXaHWF4d6D3sehxWUrv4fCNTs7i2q16oVr\nfX0dr7zyit+MVA8VLv3/YURnO8t3kOjW4noUwA0AL8r9KUMBwNtF5AMA/gIAAbCCrNW1AuC3L9nW\nKyX0t2sUk50ZdVLwdJTIc7ekcOH3Gnu9iIgP2llfX0epVPL7cW1ubmJzc9PPhHX9QcuAqSsn/N+x\nxfNRQde4bBknANjb2/O7lTvnfEk2rXauybVhtfkUsKkn1trU3Yo3NjYwPT2NYrGIcrmM1157DXfv\n3sX6+no0ojcvKArIWnup9E+3wvUbAL4heO7nAXwJwE86574iIncBPA7gdwFARCoA3orGuthAE4qX\nftH6ZYY5WKl8yVdB2Ff2+djjYcYOinrNHB4eYmdnB+vr695aL5fL2N7ezgSuaO3KWq3mLYNYTk1e\nf48C2q86gOtzu7u7fiNKjQ4G4DdF1HJG4b2aSv/ZuqrHx8coFos+KEoLNTvX2A/wzp07uHPnDtbX\n1zOTIV0/bRUUFT5OoX+6Ei7nXA3A79vnRKQGYNM596Xzp54F8CER+TIa4fDPAPgqgI9durXXgP3y\nrHDZgaldIcpUo+MuwqBFY103MdEC4BPTNa+mXq9jcnLSF1vWQwVLw+JjFpf9rFFELS61qFTErGgd\nHBxgd3cXzjk/IdCAhUH0kIRWdGyNz1brOT4+RqFQ8BZXsdgYuk9OTjA5OYm1tTWsra150bZRhLre\nbK+rTiaeg0wvKmdkztQ592ERKQH4KBoJyJ8G8E43gDlceVi3j37B+qW3czsMY2mePIb53LolnOio\nq1BFS4N+bO08u8OxjUDNq2IwqthBXNe71D2oorWzs4OpqSkAyKSmqKtQ/09KhK5CEckUAdeo1GKx\n6F3Q1v2sllbeenzMVZgK0u8Gi8ibAXyhr41oQV4GeSvRavc+MlzEAlVim5PKeX1CG5WqM+GwDBiv\nmyyxPo7tlwcgM1jrMaj9mScaWqRZ80j1sQ1G0ceFQgHb29veDa2P7W4Wqa3xAXjUOfdi3osjVavw\nIiT0RZM+EbpaNCfQRsLpYGvrPI5a6PtliA26JycnHUW2DjKt2meDUqwnR9NytKyTiEQr9gyz5U7h\n6iGx6C8yOsTCiVWY9Lpot20E6Z7Y+lDq2HVTLQgO3LfQbOCGiGQKCY9CSgWFq8fEghXI8GO/9/D7\nt2kVibptkiDFIINWqDgB99f5wqANzQu066X37t3rc8uvHgrXFTAMNw3pnlC09PdYDhavkd7QaqKY\neh+HAT926yBbIAFAU3BP6ufeDgoXIT0k5rbKSx4e9sHluhjWfrRWlroLtayYfQwgU86KwkUIuRDD\n5rYi10unVvkopN3EoHARQkiijJpgKWP9bgAhhBDSDbS4CCEkMUa92AEtLkIISYi8XNFRyiGlxUUI\nIYlgxSkWrToqOaS0uAghhCQFhYsQQhIhL81i1HIDKVyEEJIQoWCNilhZuMZFCCGJYffSsr+PChQu\nQghJlFETLIWuQkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKE\nEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQu\nQgghSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB\n4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSdGVcInIj4nIWXD8fvCep0XktogciMgnROSR3jaZEELI\nKHMRi+v3AKwAuHV+fJu+ICJPAfgAgPcCeAuAGoDnRWTi8k0lhBBCgOIF/uaec24957UnATzjnPs4\nAIjIEwBWAbwLwHMXayIhhBByn4tYXH9ORF4TkT8RkV8SkdcDgIg8jIYF9kl9o3NuF8DnADzWk9YS\nQggZeboVrs8C+AEA7wDwPgAPA/g/IlJGQ7QcGhaWZfX8NUIIIeTSdOUqdM49b379PRH5PIA/BfBu\nAH/Qy4YRQgghMS4VDu+c2wHwRwAeAXAXgKARuGFZOX+NEEIIuTSXEi4RmUFDtG47515CQ6AeN69X\nALwVwAuX+RxCCCFE6cpVKCI/BeBX0XAPPgTgnwM4AfBfzt/yLIAPiciXAbwM4BkAXwXwsR61lxBC\nyIjTbTj86wD8MoAlAOsAfhPAtzrnNgHAOfdhESkB+CiAeQCfBvBO59xx75pMCCFklBHnXH8bIPJm\nAF/oayMIIYQMEo86517Me5G1CgkhhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQ\nuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQk\nBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBC\nSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgI\nIYQkBYWLEEJIUlC4CCGEJAWFixBCSFJQuAghhCQFhYsQQkhSULgIIYQkBYWLEEJIUlC4CCGEJAWF\nixBCSFJQuAghhCQFhYsQQkhSdC1cIvKgiPyiiGyIyIGIfFFE3hy852kRuX3++idE5JHeNZkQQsgo\n05Vwicg8gM8AOALwDgBvAPBPAVTNe54C8AEA7wXwFgA1AM+LyESP2kwIIWSEKXb5/h8B8Ipz7j3m\nuT8N3vMkgGeccx8HABF5AsAqgHcBeO6iDSWEEEKA7l2F3w3gt0TkORFZFZEXRcSLmIg8DOAWgE/q\nc865XQCfA/BYLxpMCCFktOlWuL4OwPsB/CGA7wTw7wD8rIj83fPXbwFwaFhYltXz1wghhJBL0a2r\ncAzA551zP3r++xdF5I0A3gfgF3vaMkIIISRCtxbXHQBfCp77EoA/c/74LgABsBK8Z+X8NUIIIeRS\ndCtcnwHw9cFzX4/zAA3n3EtoCNTj+qKIVAC8FcALF28mIYQQ0qBbV+G/AvAZEfkgGhGCbwXwHgD/\n0LznWQAfEpEvA3gZwDMAvgrgY5duLSGEkJGnK+Fyzv2WiHwfgJ8E8KMAXgLwpHPuv5j3fFhESgA+\nCmAewKcBvNM5d9y7ZhNCCBlVxDnX3wY0qm58oa+NIIQQMkg86px7Me9F1iokhBCSFBQuQgghSUHh\nIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKEEJIU\nFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQggh\nSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKE\nEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQu\nQgghSUHhIoQQkhQULkIIIUlB4SKEEJIUFC5CCCFJQeEihBCSFBQuQgghSUHhIoQQkhQULkIIIUkx\nCMI11e8GEEIIGSha6sIgCNfX9rsBhBBCBoqvbfWiOOeuqR05DRBZAvAOAC8DOOxrYwghhPSTKTRE\n63nn3Gbem/ouXIQQQkg3DIKrkBBCCOkYChchhJCkoHARQghJCgoXIYSQpBgo4RKRHxKRl0SkLiKf\nFZFv6XebukVE3iYivyIir4nImYh8T+Q9T4vIbRE5EJFPiMgj/Whrt4jIB0Xk8yKyKyKrIvJfReTP\nR96X3PmJyPtE5IsisnN+vCAifz14T3LnFUNEfuT82vyZ4Pkkz09Efuz8fOzx+8F7kjw3ABCRB0Xk\nF1v/BZgAAASJSURBVEVk47z9XxSRNwfvSfb8LsLACJeI/E0APw3gxwB8E4AvAnheRJb72rDuKQP4\nHQA/CKApZFNEngLwAQDvBfAWADU0znPiOht5Qd4G4N8AeCuA7wAwDuDXRWRa35Dw+b0K4CkAbwbw\nKIBPAfiYiLwBSPq8MpxPBt+Lxv1ln0/9/H4PwAqAW+fHt+kLKZ+biMwD+AyAIzTSht4A4J8CqJr3\nJHt+F8Y5NxAHgM8C+NfmdwHwVQA/3O+2XeKczgB8T/DcbQD/2PxeAVAH8O5+t/cC57d8fo7fNqTn\ntwng7w/LeQGYAfCHAP4agP8J4GeG4XtDY7L7YovXUz63nwTwv9u8J9nzu+gxEBaXiIyjMcv9pD7n\nGt/AbwB4rF/t6jUi8jAas0F7nrsAPoc0z3MeDatyCxie8xORMRH5fgAlAC8My3kB+DkAv+qc+5R9\nckjO78+du+f/RER+SUReDwzFuX03gN8SkefO3fMvish79MUhOL8LMRDChcbMvQBgNXh+FY0vZVi4\nhcZAn/x5iogAeBbAbzrndD0h6fMTkTeKyB4abpmPAPg+59wfIvHzAoBzIf5GAB+MvJz6+X0WwA+g\n4Up7H4CHAfwfESkj/XP7OgDvR8NS/k4A/w7Az4rI3z1/PfXzuxDFfjeAJMtHAPxFAH+53w3pIX8A\n4E0A5gD8DQC/ICJv72+TLo+IvA6NScZ3OOdO+t2eXuOce978+nsi8nkAfwrg3Wh8pykzBuDzzrkf\nPf/9iyLyRjQE+hf716z+MigW1waAUzQWVy0rAO5ef3OujLtorN0lfZ4i8m8BfBeAv+Kcu2NeSvr8\nnHP3nHNfcc79tnPun6ERwPAkEj8vNNzwNwC8KCInInIC4NsBPCkix2jMzlM+vwzOuR0AfwTgEaT/\n3d0B8KXguS8B+DPnj1M/vwsxEMJ1Pgv8AoDH9blzV9TjAF7oV7t6jXPuJTQuJnueFTSi9JI4z3PR\n+l4Af9U594p9bRjOL2AMwOQQnNdvAPgGNFyFbzo/fgvALwF4k3PuK0j7/DKIyAwaonV7CL67zwD4\n+uC5r0fDohzGe64z+h0dYiJh3g3gAMATAP4CgI+iEdV1o99t6/I8ymgMDN+IRsTdPzr//fXnr//w\n+Xl9NxqDyX8D8McAJvrd9g7O7SNohOG+DY0ZnR5T5j1Jnh+Anzg/r68B8EYA/xLAPQB/LeXzanG+\nYVRhsucH4KcAvP38u/tLAD6BhhW5NATn9s1orLl+EMCfBfC3AewB+P5h+O4u3C/9bkDwJf0gGtub\n1AH8XwDf3O82XeAcvv1csE6D4z+Y9/w4GiGsBwCeB/BIv9vd4bnFzusUwBPB+5I7PwD/HsBXzq+9\nuwB+XUUr5fNqcb6fssKV8vkB+M9opM7UAbwC4JcBPDwM53be9u8C8Lvnbf9/AP5B5D3Jnt9FDm5r\nQgghJCkGYo2LEEII6RQKFyGEkKSgcBFCCEkKChchhJCkoHARQghJCgoXIYSQpKBwEUIISQoKFyGE\nkKSgcBFCCEkKChchhJCkoHARQghJCgoXIYSQpPj/iIFe1WwgkvQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182581550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : [2 1 7 7 7 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGvCAYAAAADqTE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztvXuMtNld3/n99f1afXvft98Zj70M64Q4cmRig53ZYJNk\nEF4jQYwUOexG8ZLIa9nE0uQixVhrBDsTscgIMkuCI0uriABhI/+TdfBqGYyd3Rg7toUHjAiGxPEM\nZi7vre9d3dXXs39Uf8/7q1Onqqu6q7vqdH8/0qN6uqq66nmeOs/5nt/l/I6FECCEEEKUwlC/D0AI\nIYToBgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJ\nlxBCiKK4MOEys79nZi+Y2a6ZfcnMvvuivksIIcT14UKEy8z+JoCfA/CTAP4igK8BeM7MblzE9wkh\nhLg+2EUU2TWzLwH4cgjhqZO/DcCfAviFEMLHkvcuAXgngBcB1Hp+MEIIIUphAsC3AXguhLDS6k0j\nvf5WMxsF8BYAP83nQgjBzH4LwBOZf3kngH/d6+MQQghRLH8LwK+1evEiXIU3AAwDuJs8fxfA7cz7\nX7yAYxBCCFEuL7Z7cRCyCuUeFEII4WmrCxchXA8AHAFYTp5fBnDnAr5PCCHENaLnwhVCOADwVQBP\n8rmT5IwnAXyx198nhBDietHz5IwTfh7AL5nZVwF8BcA/ADAF4Jcu6PuEEEJcEy5EuEIInzyZs/U0\n6i7C3wPwzhDC/Yv4PiGEENeHC5nH1dUBmL0ZddeiEEIIAQBvCSE83+rFQcgqFEIIITpGwiWEEKIo\nJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQggh\nikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWE\nEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRc\nQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpC\nwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIouhYuM3u7mf07M3vZ\nzI7N7Icy73nazF4xsx0z+4yZvb43hyuEEOK6cxaLaxrA7wH4MQAhfdHMPgzgQwDeD+CtAKoAnjOz\nsXMcpxBCCAEAGOn2H0IIvwHgNwDAzCzzlqcAPBNC+PTJe94L4C6AdwP45NkPVQghhOhxjMvMHgdw\nG8Bn+VwIYRPAlwE80cvvEkIIcT3pdXLGbdTdh3eT5++evCaEEEKcC2UVCiGEKIpeC9cdAAZgOXl+\n+eQ1IYQQ4lz0VLhCCC+gLlBP8jkzqwB4G4Av9vK7hBBCXE+6zio0s2kAr0fdsgKAbzezNwFYDSH8\nKYBnAXzUzL4B4EUAzwB4CcCnenLEQgghrjVdCxeA7wLw71FPwggAfu7k+X8F4O+GED5mZlMAPgFg\nHsDnAbwrhLDfg+MVQghxzbEQmuYQX+4BmL0ZwFf7ehBCCCEGibeEEJ5v9aKyCoUQQhSFhEsIIURR\nSLiEEEIUhYRLCCFEUUi4hBBCFIWESwghRFFIuIQQQhSFhEsIIURRSLiEEEIUhYRLCCFEUUi4hBBC\nFIWESwghRFFIuIQQQhSFhEsIIURRSLiEEEIUhYRLCCFEUUi4hBBCFIWESwghRFFIuIQQQhSFhEsI\nIURRSLiEEEIUhYRLCCFEUUi4hBBCFIWESwghRFFIuIQQQhSFhEsIIURRSLiEEEIUhYRLCCFEUUi4\nhBBCFIWESwghRFFIuIQQQhSFhEsIIURRSLiEEEIUhYRLCCFEUUi4hBBCFIWESwghRFFIuIQQQhSF\nhEsIIURRSLiEEEIUhYRLCCFEUUi4hBBCFIWESwghRFFIuIQQQhSFhEsIIURRSLiEEEIURVfCZWYf\nMbOvmNmmmd01s39rZn82876nzewVM9sxs8+Y2et7d8hCCCGuM91aXG8H8M8AvA3A9wEYBfCbZjbJ\nN5jZhwF8CMD7AbwVQBXAc2Y21pMjFkIIca2xEMLZ/9nsBoB7AN4RQvjtk+deAfCzIYR/evJ3BcBd\nAP9TCOGTmc94M4CvnvkghBBCXDXeEkJ4vtWL541xzQMIAFYBwMweB3AbwGf5hhDCJoAvA3jinN8l\nhBBCnF24zMwAPAvgt0MIf3jy9G3Uhexu8va7J68JIYQQ52LkHP/7cQB/HsBf7tGxCCGEEKdyJovL\nzP45gB8A8FdCCK+6l+4AMADLyb8sn7wmhBBCnIuuhetEtP46gL8aQviWfy2E8ALqAvWke38F9SzE\nL57vUIUQQoguXYVm9nEA/wOAHwJQNTNaVhshhNrJ/rMAPmpm3wDwIoBnALwE4FM9OWIhhBDXmm5j\nXB9APfni/02e/zsAfhkAQggfM7MpAJ9APevw8wDeFULYP9+hCiGEEOecx9WTA9A8LiGEEI1c6Dwu\nIYQQ4lKRcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoii\nkHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGE\nKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQ\nQoiikHAJIYQoCgmXEOJKYGYws34fhrgERvp9AEIIcR5SseLfIYR+HI64BCRcQohioUh58ZJgXX3k\nKhRCFEkqWnQV5sRMXC0kXEKIYmklUhKtq42ESwhRLHQLtnoUVxPFuMSFkRv1qkMRvSKEADNralMS\nr95anIN4HSVcouf4mybtWBREF72E4sV9cT63ae4apv8/CNdZwiUujNMyvnKjZSG6RW3oIWeJ+bUa\nWOZe9+/p53WXcImekmZ4ETbynAWmjkeI85MTrVb7xFus6fO5/xuUe1XCJYQYaHKdLwdGQ0NDGBoa\nivsAcHx83LCFEBT3umJIuETHdOJGaBUwb/d+IU7DixP3R0dHmzYA2N/fx8HBAfb39+O+FzAvZFcJ\nf++l7rxu7sluX+8HEi7REenETt78p90QrV4fxJtBDCZse8PDww3bxMRE02Zm2NnZwe7uLnZ3d7Gz\ns4MQAo6OjhBCiFYYcDXbYO6e68Yd38n7BuG6SbjEqaSilfq80xuj3ShvEBr9IDEIge4SGBoawvDw\nMEZGRuI2NTWF6enpuM3MzAAAtra2sLW1haGhIRwfH+Pg4AAAGgTrKl/vNG513nMdxGsl4RIdk5bU\nIWd1QVx32k0bEA+hi5DCNTo6irGxMUxOTmJ2dhaVSgVzc3OoVCowM4yNjWFoaAghBBwcHGB3d7ch\nxnUdqmpc9bYk4RKn4gWrG+ESrbkOnWcvoatwZGQEY2NjGBsbw9TUFGZnZzE/P4/FxUUsLCzEGJgX\nrZGRkQZr6/j4WAOFwulKuMzsAwA+CODbTp76TwCeDiH8hnvP0wDeB2AewBcAfDCE8I2eHO0AMYgp\nohfB0NAQRkdHG1w0IyMjGB4ejqNgbiEEHB4e4ujoCEdHR3GfnQVdNNy/ztlfFz3yv0rtk/GsmZmZ\n6BKcnp7G7OxstLimpqYwNjYWRYnk2thVJI3/8d70CS1e1P19lrvv0kSW9B4+OjpquraXSbcW158C\n+DCA/wLAAPwogE+Z2XeGEL5uZh8G8CEA7wXwIoB/AuA5M3tDCGG/Z0d9ibTqXAahnFGnHV8nx9Vq\nBErhGh8fb9rGxsYa/g4hYG9vD3t7e9jf34/7x8fHUczY4Cls3Ph3Gn+4qh1NSq/Os9PK6CVdVwoX\nrauFhQXMz89jcnIyJmVMTk5iZGQEBwcHDXHDtJO9ilmFZhYtUbpRx8bGmgabHHD6gWOrQWV67fwj\n79WDg4N4/w60cIUQ/u/kqY+a2QcB/CUAXwfwFIBnQgifBgAzey+AuwDeDeCT5z/cy6VdJ5Bzl12m\n+6GTDqqTVNjT4iwUromJCUxNTcWNAXG/H0LAzs4OdnZ2UK1W474XKG4HBwfZlGV/A1ylziVHr8+v\n1ZIeueSZklxlQ0NDUbgWFxdx69Yt3Lp1K3bEdCEODw/j8PCwQbhyFn0p590pXrgo5pOTk1HA/Ea3\nKbfUckqvWTqw5L27v78fpyX0w/V65hiXmQ0BeA+AKQBfNLPHAdwG8Fm+J4SwaWZfBvAEChQuoLsS\nKv2yuE6bEd+qo8p1bun7hoeHo3B598zc3FzDVqlUEELA5uYmtra2Gh69SHmxqtVqcdIo3Yzcv8op\nyxdB+lu2GtCkc31KuL7e4lpcXMTy8jJe85rXZK2E/f26Y8e/1k+X1mXA+N/4+DgmJyejK9VbpBMT\nE9FL4r0fqRckfeS967e9vT0MDw8DaLxvL7M9dS1cZvZGAP8RwASALQA/HEL4YzN7AkBA3cLy3EVd\n0IritI6g09pfF0G79HR+/2milX4eH9P3e4uLwrWwsIClpaWm7fj4GGtraw3b2NhYg9uQbkSKFkds\nR0dHDZUPSutcB4F2CTSe0sQrJ1yPPfZYHASxTe3v7zec+3V0FU5NTWFmZgaVSiV6RPw2Pj7eZEH5\nuHS6+WvL/dHR0XjfHh4exljaZbans1hcfwTgTQDmAPwNAL9sZu/o6VGdk1auPb+x0/T7/rnc+3Pf\nwRvDNwbeKHydj5fpRuzld/lrQLfM+Ph4Q1bX0tISQghNc21GR0ebRGtvbw+7u7vRnciRIGMU3iXB\nuNdVIDfYSDvZdL9ThoaGGlxn7ExyLqB0f5Cub+7eoyUxPT2NSqUSswir1Sqq1WqMsbBd1Wq1aN3z\ntasqWsBDi4sxZ96XaTLLzMwMJiYmsvdYK+GiaHlvye7uLsbHxzE6OtrUzi4rGaZr4QohHAL45smf\nv2tmb0U9tvUx1BM2ltFodS0D+N1zHmfXpCNPPwckfeTmn28laP7zgbp1wBn6/tELGBtBL3/E1Ko6\n7b2dfmYKJ3Du7e1hZ2cndo6Tk5MxvrWzs4NarRazmcbHxzE7OxtjEzl3Q61WQ7Vaxfb2duyAfFzM\nb3T/lESurXjxZ4YXg+XnbStjY2MxtuGrSPh4ou+w2iXE9JN04DMyMoK5uTnMzMxEi2F0dDROLt7b\n28PW1hbW19exvr6O1dVVrK6uYmNjA9VqFfv7+1c6m5CwLTE5g2LPe3RmZgYzMzOYnJzM3o9sfzn3\naupC3N3dxcbGRsM2NTXVYJ1xOzo6upDz7cU8riEA4yGEF8zsDoAnAfw+AJhZBcDbAPxiD76nY3KW\n1MjISPTz8pE/rvcDT05Oxhsj3fxnc//w8DD+eOvr63F/b28vdhRAY9zGHyfp5sbKuQLP8jm5z0s/\ng+4AChevKRMyZmdnUavVUKvVMD4+HoWLI+WZmZmGERw7zL29vSha29vbcd9fR2YplkpqYbEdUvy5\nz8EBKzzk2spp3zM2NtaUIm5mqNVq2Nvbi7+R71wODg7id11UB9PqeIF8W/WWAx8rlUrsdCcmJuJI\n/+joCLVaDVtbW1hdXcX9+/exsrIS46sc9KTW7FWEbYsZwOzLfDKVFy7+/tzPWaS+X/CGwO7ubgwF\nMJY2Pj7eMNhk35i2q/P2VaTbeVw/DeD/AfAtALMA/haA7wXw/SdveRb1TMNvoJ4O/wyAlwB86sxH\neEZS0eGI1GfC0R/sRyTT09OYmJhoGhnn3IgAcHBwgPv37+P+/fuYmJiIN5T/H7oTPbmYWbc/ZCc+\n5XY3bCdWWwghWlycA3J8fIyZmRnMzs5GC5MBWwrW+Ph4/Ixc+u3+/n4ULL/dv38fIyMjUbS2t7e7\nuiaDhhct37n41OXDw8OGJJWciJx2w4+OjmJqaiq60RYXFzE0NBStWG/Rsl0DiIOJVsfe6vvOwmlt\n3ru8mMU6OTkZhctbXLRUa7Uatre3sbq6irt37+LBgwcNdQrZKffyPAYNb8mzTfmBuBeuqampJqvI\nT9BOPQW5dPparYbZ2dkG0RodHcXm5iZGRuqScnh4iFqtFgdj6Wfz77P+Jt1aXLcA/CsAjwDYQN2y\n+v4QwucAIITwMTObAvAJ1Ccgfx7Au8Ilz+FKYzK5zLhKpRJLxaTb1NRUw//6yXzptre3h0ql0iBa\nLDHjRcvHyHJupG4Dm928/zRha9cpeouLGURHR0eoVCrRSmJcgQ3Yd8ys2J2e8/7+Pra2tqJgcd9X\n+N7e3o7ZSyWSWv6+8gPFfWJiIiYVsK3kbvD077TDp8U1NzeHGzduYHl5GcPDw9H6GBsba2jDwEPR\nyrli09+r1zFTIN+Gea8ypsVEA1oL3lWYWlwUrtSiSK3XqyhgOeHyopUK197eXkNhgePj46bQCL0n\n6ba3txc/y1vAqWj1us/zdDuP630dvOenAPxU10fSY/woN70Z5ubm4iTGxcVFLC0tNTzOzs42zURv\nJVy1Wq1JtDY2NmLsgDGGtPPhMfr9s4hX+jm51zv9nBx0Y/Fc2CFQaLzFdXh4GBu3L36ac7vyM/xG\n64qitbq6Gm+GUslZXOmImBYmr2+aCNTqc/3vT4trbm4ON2/exKOPPhpjkePj4w1BdFq97OBPE8pe\nXYf0s3Nt3VtcMzMzcTDpLS6O/Oly9sJ1//79a+Ea9KRuaB/jSi2u6enpKFp7e3vxf0II2cF6LjNx\nb2+vYRDB95tZ/E2q1Wq2HfeqbRXdK6SjA25+Eh4fmZHEOUd85P7s7CwmJiZiR8nRr7eYvBB6VyAn\n/lUqlZjtNDIygs3NzYa6aT5ek/6A57nRLvLmZCfgR61m9aUjNjc3sbKygrGxMZgZqtVqUxbTzMxM\ndtTmJ0wy/X14eLjBLeRvDu9/H6RkglYw85ITP/1+ulHI9/f3o8B4crEG77oeGRmJnZRv83yNgsU2\nyMHHRSQNdUPue3MZq6lHo1arYWNjo8Hi9+fk6aSjHPS2dBrsX3Z3d7G9vY2NjQ2Mjo7i8PAwZu9u\nbm5ifX0dk5OT2UxB9mNpO6UFxzY4MjLSNHWF8718duJFz5crXrhyWUgUJO8SZCeadqr+ZvcjX9+Y\nue+/i/ucqc9EhIWFheg35uexUfk4hmfQb5xUuIaGhrC7u4vNzc0G197GxkZTvJCxMD6m1ixduPxc\n/h+Fi4k0fi5OPzvbTkndXYydtqrxyNFvTrg8bPN+8xUT/ERTjqLTBBsmafgFFj29SPhJ8d6EVufH\nc2slXENDQzg8PES1WkUI9cnu1Wq14Vz8Z/nHVsfk3zPobaoVjBlzMEk3arVabap2Mz4+HkXGZ5py\nmZh0Y0x2YmIi3nfsD/gZnIbAQRGnIFzk9SxCuFq5z9jQfX2u8fHx7ATZSqXSMBrllmZ4UWhyJVHS\nwDotAY5WZmZmYrzAW1oMIPM5kloOg3rjpFYXR3Kbm5sIoV6toFqtNs0d4cZkgePj4ziZlB04ryFd\nHV640tn+TOPuNuuuH9AKmpubw/z8PObn5zE1NQWgec4WrYh2wuU7WT+1g1lkbM++jiTjZb4dsqP3\nFkqrxJ2Lot1n+4m0Xrh4XQ4ODuI5ULiYBOCvUWqhpt/bSWJSKfD3ZdYv3XUbGxtNmdRMBvIZvkdH\nRzF7kwN+Xk/2qb6tsM3Ste2zVilc7dpVL9yFAy9cvgGmF6JVzGBhYQG3bt3CI488gtu3b+P27duY\nm5vLumnSbDe6VHLzXejGODw8bKpETYsLQHSdsYPf2tqKs81JCYLl8dUsQggxAYUjvfX19aybcGZm\nBrVaDUdHR1G0KpVKjLsAjYV8W1lcZ00V7xeMOVUqFSwtLeHmzZuYmZlpWfuNgyGf8ZeDbkJeL1pb\nOYuL3oDU4vIj7cu0XnOdVi6rMLW4OKXCn8fx8XGTcOWmm/jM3rQP8UJ3nkSBfsP7MI0x5eaoMhsz\n3aamprC4uIjd3d0oWrxfmULficXlJ323Gijw7/Mw0MLVafqsH3XSXcd6Zq973evw2te+FgsLC9mE\ni7QcES++9//y0Y9WeJMwhkCLa3R0NBacpWitra3FTCgvlEAZopUeKychUrRoATCgnroJaWkxDsgb\nwycskFbCxe+nFTzonQwtLgoXB09003GUyn1WDmFH2w6fnejTxlPhAh5a/l640uoIl3kdT7N00hjX\nwsICJicnY+YqrxstfgqXr1CeS6Ly7eUqWVvAQ1ehF610Kk86PSe1+mdnZxtEi1OI6O5uZXFRuPwc\nwcsYEA20cJ0Gg4X+AvuF5W7evInl5WU8+uijmJubA9BcgomuJ+9KYafs63Pt7e3FG2pqair+kD6W\n4GMLjPn4+Mbo6GhDx3uZkz7PShoH4HNsnAcHB/GG8JM9+X+Mh3k3Qjr48J+dZjXl5tINGrlOklMv\nKpVKdF3Pzc3F+VR+dNwNabV+b9n6rDs/SPIdzN7eXtPI+bI771yb4t+5+3liYqKhc6ZobW9vN2S0\npueR89akgpVaW4M+IGoFByFAc3zPt0sA2TmpIyMjTTEq4OF0GF+qjY+cK8d5gp24CoHeeJuKEq6c\nq9DPz2InMTs7GxeWY/A7LcjJRz8xk/t0Pfi5IAcHB03BdnYWaW0+xmp8YgL3/ecB6EvH0Q25xs/0\n67RsFl1jTIphFflHHnkEN2/ejHPk/HyPtM4jy/Uw1d7fDJcR9O0WXos0QYixGZ8hOTY2Fpd5Yee7\ntraGjY0NbG5uNk2YzUHLlm2d61MxM5aDI1/FgG3dl/Xxg4t+dNa5duVjyIzL8D7e3d1tyI70CSZp\nR5nzEPDvdOBKSvKA5EjvT58B7be0zB23SqWCGzduNG28Xw8ODuJKDzs7O7hz5w7u3buHBw8eYG1t\nLVq/bG/X2uI67cbKCdf8/DxmZ2djR8E4CoO6frIry8L4Wnne9ZBufvLy7OxsHMGyY2Knlc6b8KNi\n/g/wcDTTaqTor0E/SdOv/RIKftqBn3fjN94Ec3NzTfOWvIu2VqvFjpzCxc4pTd++zOvS7vegq9NP\nKB4fH28QLlYYSGOfm5ubWF1djTc9Lf12rlDvcl1aWsKtW7dw48aNBhchXbm0SDgQ88Kfa3P9EK+0\nLJufh+RLsPn6hF64UkH2tBOmnFANwr12VrxYpXMGfVEAPznZ378cCC0sLGBxcTEOinjPcxDPCe13\n7tyJJbYoXGlmYe736JWLdqCFi7Q6SV+DkKN9ChfdJt7iqlarWFtbw+rqKlZWVrC6utokWhSudJ2a\no6OjaEGkri8fw2Fsw89W95YXhZSfm1YtOC2u1w/SURyveSrKPoPOb37NLl4j4OEMe2/xrq+vRzeQ\nt7hokV22xdVJnNW3QQ5aeL5si62Ea2VlBdvb21G4fZJB7ibPxc5u325cNYjxVT8C5jVMra3cuV7W\n9fUDobQclhcuPu8trjQZIGdxcb9VVqGn3/fYeeH96d3rfjDFgQ3zAFJvkJ/T6rMLmS3o44vr6+vR\n4lpdXY33rM8HaLUqcq+ucxHC1YpOXIXM0PLCde/ePbz66qu4c+dOU4VydiLe/8/HSqXS5L5i9s3k\n5CRCCNFlVqvVGjp1bj7LK61akO7305XjjyN1PdDCYAdNcUpHbNxnx82One4sWlycILmxsREtLlrD\n7Jz8tITLyirs5PfwGX7eIs9ZXEzpToWLqwl4d2g7i4uuQgrXY4891pDowc2X4/IWV6vzvOwBQc6l\nlVY353XzcTtaXOnSJa2EqZNYS8n4a+ldgowVpnO5coNL1h5MwyBra2sxRsr7c2VlJdZn9RbXZSb9\nXCnhYmJG6ipkY2fa9t27d/HSSy/hW9/6VkOFci9cxF/8+fn5JkuLlTq4AjBHxDk3ITPsKFq1Wi2b\nbNBuCkA/8CNjn0FI4WK5LG7+78XFxaYlYxgI9hYXl6TgTeDdXD7pox8xrna/R2pxUbQ4cvUxLr7f\nT5NYXV1FrVZraQGlCQNsX7Ozsw3CxVUJaG15V2HqUkvPqd33XSS5AVEuxsU4YrsY12muwqtO6hFJ\nk1y8ZbW0tBTd99xnn+ndiGNjY9E9uL+/j83NTdy9exf37t3DyspK9FrR4rrMOGExwpXLNPOuKT/S\np3vh6OgIOzs7MVj94MGDeLF5wb3LgdYTl+fw3zk0NNRQ23BpaSn6gTlamZiYiGKZjn58pz1oGXLt\nRtx+ygH95Vxvi9f9xo0buHnzJpaWlmKnTauXk43Tmo8AYlZiurBkWsJn0BIyPD5teGZmBvPz89l4\nHoUjt7SIX/qcj7mYxdDQUMMkb3Y0uXmDbOdcl4oDBZJm0vE5/3jRtEooSJ8H0DTgo+eDrlUOIIB8\nX+Hx55laCWkMcNDaXa5SkI+x+gLO3sryoQvfVy4uLmJubi5W1eBnpItE0jvCTMJc+nsKf7+LuIYD\nJVytOlCfJus3VmRIN/6PLwi7tbUVTdv19fWYoMHRmp+PxRGer9U1OjoaO+rUJeZ/fP4/jyG9Mf3N\nOAi0cocRH+T1cQf6wynkt27dwsLCQkMpLY6S004IeDgnK021becCSmNLF92pdJIm7a1+vxq0F20O\noDjvjR1uq+kQ/D6Wc/LBdU7ITT0KtF63t7exvr6OBw8exMHZ9vZ2TDhq9X39JG0fvh/w1VooWhzk\n+AEnf4dcP8E5bWlGIRei9FnGaSzQ/98g0KouJV2C/rX0Pd4r4JPM0uVifGk6v46ez7T281nbxUu5\n3+trODDC5dU5J2A+k40/kndHcVtYWGjy9e/t7WF9fb3JJ8syTL7mm6+fl8u8ySUesAYfhctbU61G\nz7nO/LJp5abMJR/4ya50OdACvXHjBm7duoW5ubmmBTlbnacXLm91eeE6zdq6jNhMTrTS68O6hN7i\n8p3A4eFhtCT9fJc0DuDbPoAGdw83trdc1mxOuOj+zglX+n3puV00rayt9Hhy7YQVQHgN+TvQdZ92\n4vwNvUXF38UvfsjBhI9vX/Z1aYdPzvHik6vDSmvcl8Pzlhg335boEQIaJ/x78TpNuDrpV859HXr2\nST2iXRxql/w4AAAgAElEQVSBAXDGi1JLi5bQxsZGHB1sbm7GIHiaBVOtVmOHzEf+sLlisekaXmw0\nvrPOWVytRGtQaCcAFPbThOvmzZuoVCoNbtGcxeW/K1fvzNfRaxW7SI/3oq2vdp+dcxUuLS0BQMMN\nz0UxT7O4coM1P9GYFpefpwgga3Gtra01ZIPlLK5+d8it2gfQaBmlljkL7fJ9vLdGR0ebsuZmZ2fj\n+9JlXVgomi4xn+XbLkmmX3CQxJUoGLJIMwJZVq2VWzGtBE+PE68jgIZ6rV60/PSUdslSFzmwHBjh\nSi2uXFCaNzE7zXQdLW4HBwfY3t6OixUyA4YxLm9xzczMxA6A7p7cIpM+RdRv09PTcaTCx9RMzrkK\nB028gHxw/jSLi1Uhbt26FbMm+X+tXEAkjVv4sj6pxXUZo7izwM6SCRO0uPb392OSBDvbra2tthaX\nxw/W2OYZ081ZXBQulhijcKUTvAeFtF2kG0WrnXDlJtj6pCHvFaEw+W1/fz+KFqemjIyMNHTEl5ms\n0gm56RA3b96Mg3YfzqAF6gfMaeyP+0D+/vTC1anFlXKau/1M16Enn9JDWnWeviahH4F6EaGw+CWk\nmU2YK8ZJS4J10VrNQ+LmrS8+TkxMtOyYrwJ+FOarg/iYl58n4ue+8TF3o5yWEZaKf27/ssl1sF7Q\n2YYWFhawu7sL4KGgMBElbXuMgwGNMRifncrPpFeBwsUlZdip04W2tbUVswzTzx00TrO4cgVhufm0\nb7bJqampplj0wsJCk3CFEGJWrxfHvb09DA8PNyTNDNJ1o4WfTgHKxfr92oKtHttZTLwmPhPVDxJy\n3qPL6gsHTrhI6vv3/tY0SJh2fn41WFaSZjUDzvzmY269KFpSFCd2EjSheQxMY/aZh97UzgU305HK\nIN0UuQ6OLhUWaOX5ra6uNoz6zSxWkfbnenh4mC0xw6QZ1nucn58HgAYLgindnFfHY+vXNePgyQv4\n4uJiHNX71Hd2lH6Uy3lIaefDMk9+hDs8PNzkTeDyPMxgBRCvDedsMXswF5sZJMshh7e0/LXwQu4t\nDe/e9/Ebfy+ng0vfjsbHx+Mg2XsVeC0Zl2Rh4kGAliKtzo2NjbiEjb9PGb9PB5FpyS9ea7Ztvw0P\nD8cB2MTEBBYXF+P9urKyEivN7+3txbad4yIGTgMrXCmpr9WnE6cmqxcurtY5NTXVVBRyd3e3IZDr\ng5X+ZvCVu4GHwsWGkfqQ/SjOB5ZT4bqsibSt6MR8T0eixBd0ZSc9NjbWsLoqN18KiftmFjtZChfn\nOvl4Db8jFyzvdSd8WqJCuoQOb2bGGDjgmZycjG3Fz1FiG/KixcFVutyJmWXnxfnSYryGnGycTow/\nLdtrUEXMixYFw89f4yrjufX1/G/jPQE5N/3h4WFD1ix/E06Gp8VCF+UgQJcm7w9a7KlobW5uAkDT\nIJKDmnTLTVZmVX4Acbkoer28aG1vb2ctrItsXwMnXO1UOxcoTC0YZhdNTU1F0ZqYmMDc3FzDEia+\n4rvPuEnTQlNrygd2aVFxMUTgYUfFjsNbXH7Ow2nVky9rdNwqi5PwXL1oc9Tly1cdHBxgZGSkoUQM\n41W5G4KN32fOUaB8kgGFy0+ezWXjnZfUxZH7DdJkienp6QaLywsXRSqdWOsnzHM6gU/F5mAMQMOk\nbk4W5bXg5otFc/5bpxUyBskC88eWWlwUF29xHRwcNFWE8IOpNO6c84pw0EnRYux2dXW1IdRQrVb7\neWka4KCZ3gjG6Bj7o2hNT09H6yxtX35QwPuKfWQa0+fAgd4Vlm2jaG1tbcWBQasMzCudnHEaaVps\nu/kEFC6K1szMTLTMvHDQJZNaTENDQ00lhuj2ARBf8yMY4KEPmJNOvcWVjnwGyVXYicUFPLxp9vf3\nGyYn0m1qZg1WLZMTcqWvvDuHI2lWz9/e3sbGxkaTVUdLotfZSjm/fC7WSuFilRYKT668Ewc/vmOk\nVcC4Fa9TWguO7r9chYMQQvQYsCRWmvTh21d6bheZ6dULvAuL9yhdeRQuVlLJtSkuK5S6wnJiBqDB\n0mJ75fpvFC2+dxDwrkK/MCQtLW9pcgmYdCCZW5VhZmamob3duHEjDsqZfMT96enpaGmtrq7GTOrc\nQOii2tng/CKnkM4nSCsPeOGiNeCTBbzwkFZZTenERG7+huJ3cXTMER0D5ukxd+IqbJU9189OJoQQ\nRZ8C5d2Dfh0zAE3ls6rVahzB+ZEcU8aZ1j0/P4/l5eVYxPPBgwdxXpwPMrMjuig3YTuLK83y8wkT\nqcXFmz51n/KmT6vi+5jK7u4ujo+Ps8tM7O/vx2xBih6FizGuVtZ8J+fYD3IuWm9xcUDJAU4I9Zqg\naVJWpVKJ8VPfPzBbMI3hmNXXTUvvcy4wy1Jk/p7uN95VSI8HF7lNCyZwwrvffF1MP5CuVCp45JFH\nsLW1FZOJzCxeU9bHZCyX5cpmZmaixZXLS7goBka4cq6fNEnAp04PDQ1Fs9h3dN6f3Wqyr3/0mTPc\nfC00P1pJs2p4Q+V8xrkZ56mV2O6H7XdnQngcXvxDqNfDo3XKa5haA+xMmUHHDp2/DTsTX5GDbkTe\nfGmJrH5nFaZVzH0hZ9+p8NGn+acJRWncgan1HNDkqiAAiNecHVO6LEpuSYlBxltHQHPRWA4Y2H74\nHs6/BB5mDw8NDTWtLr23txfbWRq35iCI7Wx8fBzVajVWPvFJWYNACA+XA+JxMQHKe40oXOmAqNWS\nTQCyq2jzmvAaTUxM4PDwMLpm/URvn2+QVh/pNQMjXCR3oj6utLe3FzvPjY0NrKysYGJiosGdlI4+\nfGmcNDvNT6jzI7TcEutpFQP6ff3ntROttLbXRVgOlwHPkTeGzwbzHbWfZOunHjCW4NepYsfhBxyD\nNmk7tcy9YPm5WhMTEzHewI0jWW8xendxmnziU47T8/cDuGq1mnUV+nbezsLqd/tL3XpAo8udgpPL\nxuX9TsseQPSWpPduOjhi55y6D/l9bJO+tuYg4DOrffvz58F99pfpxP60LiM/13tPmK3IPo+WPN22\nfsDJGKNfeJefeeWF67QT9BUI6K7jRfWLzHFhx3TzlpF3RfiO1vuDc2WjGFCfm5sDgAbfLs8hl0CS\nBt79TZi7DpeZ+dXpTZl2evwtvMsQQFakgcaSXX7KAS0sb1mlEyZbFV/tR6frRYvHRBHnHKrR0VFs\nbW1hfX29YeOkWf8ZOSs+J1p8ZNv3KdFeuHwZJM9lt6vTyA0i0wQnP18wN9DznXKre5Zbul5aai2w\nH0ktskEQrvR3o3ABDwUnLQjOGpl+MO7DKr4v9IaBT/oYHh6O8xOZ1NFOuHJ1Di8qc3pghAs4PUnA\nZ/Lxx/LxFqaD+uoXzIzhXAdvyh4dHcVYjI/LsCBqWoBzfn4eBwcHMLNYIT1tVF64UvFKLa5+Jmec\n5Wb0x8qUbW8J83m/5SwuL1x09/hZ/jmry3fe/SZ1PXvh4nmweoXftre3mxKBWO0h3dIMOC/W3iVJ\n4UoX3WzVYfRbsFJS8eJ5+sSW8fHxBtHie3ifMZmHg4OccOUWUGQ/wZgZ3WJpm+y3e9rD3x94GD7x\nk4H9I1/3HiD2O164gMa5qZwDxhgX678yw5P3Mgei9KKkg6vcdevV4GmghKsdvOC8wD7ry/v7t7a2\n4oRNjhLYIHOdKisMcO4Gl4FILYf9/X3cvHkTQN3SmpmZiT+kd5X5GJd3FfIGOs3iAi4+aJ5mz7U6\nhnbPc+RG0fKp8X4jPs7VqcXVyl3Yr46knauQFhCD1A8ePMCdO3cats3NzablJ9iWuFYb22qavu2F\ny1fKOM1V6BmUZAxymsXlR/ZetHhNqtUqjo6OsL29jQcPHuDu3btYX19v6S3xg9lKpRK9N8yUYxYy\np8RwYDEIMS7v1fFZzWmb9PH7nBs655oFGrMV/bSfhYWF2B+mrkJfNWZ6ejr+D9tnq2Q4//dZ22IR\nwuV/AA//5oRVrqTrKwiEEGKD9FUduL+2tha39fV1rK2tYWtrK1tU0sziejZ+OYV0lBFCaJgv5rd0\nEuBpAtEvco3KdzReeHNJL76DSescUrhyk5gHaZpASnrT+1gf132jC/XevXu4c+cOXn75Zbzyyit4\n+eWXsbm52bTMRBo/YOpxroNJOwYvXKnFlcaxfMfnn+83ncS4mLCSGxTt7+/HtOxXX30VKysrDfcd\n92dnZxtqYIYQoivMiz1Fky7CQbDwPb4f7OXv540CJn4MDQ1l5wZ6V64XMJ/BedGWahHC1Qo/+mSm\nIYCYPs2gOavA+3kL3Fj+idvW1lacLc7P8nNI/FwdJhb4zoQ3FYucMuPLz5+4SN9vt6SxDz6Xw9/I\naXHTtKPxayM99thjeM1rXoNHHnkkLrI4PT0dJ25z5Lu3t4e1tTVsbGw0VFI/rVp8L69Du46dolSt\nVqMbianI6UR2ugc3NjbiEhzpaJSdkLeq2IFQyNLK/D593s+ZS+folEzOqmX8hevobW9vNxXMZidL\nUaLldHR01LDYrF9NolKpxHt4Z2cnruzL342Din5z0dYyr5cfWHKeIufHDQ0Nxfbl26DPXPQJIBc5\nMLoywsUGnlphrMDABpyLcTF1m/sHBwcNS3Nw368+y3TZsbGxOKrzAeL79+/Hm4o3QDqHq18B826/\nN2dR+dGWD6KnWZdTU1NYXl7G8vIybt26hRs3bmB+fh5TU1MAEBNkGCOjcOWsiIsSfd8ppJaJvzZ+\nIEQLkRNBU9He3NyM5+LXjqLlzs+mQHnRokh70Zqenm6YmpF2Gj5rLCfu3poZBEurFakbm8frhYvJ\nLlwUloNDJqd4i4CDAq7U7be5ubnYfo+Pj6NLLF05ehCEC7hYa9lPrqdgcZudncX4+DjMLHqffDv0\nRcxTqzY9/l5ZYUULl3fVAGi46SlaGxsbsQRPrhpGLjMphNBQE46dsK9H5xdgY4fBGBt97t7i4vyJ\ndu6wy+xUfEfWyeve2uJcprQ6POMIacX+tGr13NwcxsfHGzKeuM9Og8Llb4SLnELQqcXF+XyM71Wr\n1WxGICsZ0HL0FhfQ2Ha969GXBksrz6dzC73F1ekcwUEVrdRV6F3NABrur9XV1YYlivwE7IODgzj9\nJV0tnQuecqtUKg1eEMbMaHGdJlw59+tFXJdOPSLngbHV6enpuDTPrVu3osVF4aLXoZ3V36nFdS2S\nM3LQ4uJIlT5tipa3Bphlk/rU/YRQP9Jl2aZ0xVFvcTGxYHt7O95YnFvmhct3wDxu/6P1cxTczff6\njC/OpvfpxVx+w1eU4IjNC1qlUmmKBTKr07sKfcJBGmDux7XgTetTh9MF+Lif1sX0N3M6F8cn83A+\n2P7+foNo+dJQuU4jTToaVIHqFO8uTC2utbU13Lt3r6XFxVRuv9ov1427efMmbt++jdu3b2NmZqZh\nrh2zildWVhpc/DnhSi3Dixavi8ZbXBSu5eXlWBGGCUMcvKUVX3wf5yci586lF1ZX8cLl4wHeIvCp\nof61dESd2zjSBRpXHPXi5V2FNKFZIoYLV+ZiXED7pa0H1ZWTxh3StdFYtZuj2Zs3b8ZHP+mTj35O\nHssWra6uNllcFK5Wrq/LhBazz75qZ7Gm1j2fY3vh//opE6yVt7+/H6/t3Nxc7JhzosXEkPS7SiHn\nefBtzQvX5uYmVldXce/evZhQ5SuHHB0dYXJyMuv6ohVx+/ZtPProo5iensb9+/dxcHAQB5i05LzF\nxfvWH99Vw8e4KFy3b99uqGbTzuKqVqsNnpGLrgxUvHD1+gZl/CZN3+bqolyfi9lwwMPYBy2u1dXV\nbHwrd4P6c/GPvToXPuYy/tIOIn2eVkQuCcO7AylcjCPcvHkTN27ciFaXn293eHgYsz55rWihekuV\nSTJ0sQ0CrbIpu4k95Nps6gL1LjI/5YAB8LQSAjNc0+y8Qccnp3irk/FiultZvojv5XxA3zboCTg+\nPm6ojcl9Wv6cI8cwga+ovra2htXV1SiGdMH2o/21SnFPLT3i+w//++fmQaZTLYaGhuIq5rxvWTia\n2ZV0X+/s7MTrRM9IOn/wtIFTL9pn0cLVK9KGkYrW/Pw8lpaWGpZNZ0ecmwyaui/4o5JcMsBFnUtu\nQm9OiNJH7ueSVMbGxuJCm3QVsmK6X+LDdxK1Wq3BImAGF7PvuE/R5wKSgxIYz9GNFdguHuJdr1y8\ntFKpxIovAGLKvS+mS9d2TrQG1XIn/nj5G/tpLbyPWNKNKdrT09O4ceMGhoeHMTc3F8tpcQshZBeH\n5YATQGx7ZoZ79+7h3r17uH//foO15bMU0zZ4WUlVaSWMNEXf7+eqAjEWndYw9KXwuL+wsIDl5WXc\nvn0bS0tLmJubi/UH/QCJRZ7v3LmD1dXVWMrssqeySLhO8JaGrwLuLYmFhYW4oiorRnNEzBEibzjf\n+NtZDb22sFKXadrw05VO05qOfp8bU719une6gJ9fc4tCxhqSubqPW1tbDYLFRz8tgVWqB41uBhzp\n+3ICRterXxNqbm4OU1NTsZ3RFe2Fy8/ZyrWjy0geOA/sZAndUCwczOojMzMzsbzY1NQUhoeHY3yK\n14Tb8fFxw4CKjxyIAWhI275//34ULqbCM6GmlXDx2C8SDja94Pj5UX5QCqDBRceN/Vg6XSO9Z/06\nZ0yg4rpbtICZdEaX/r179xqEK60jmUv+6uU1u1LCdZbsm9Qk9xYXhYsWFzsT1jU7zeLyVcFPG7V1\nerzdnIuvQOAtKS8+6aPfWq0omy6X7qtBpJt3y/iRMUfTqXillawHUbiAztLL/ci43f954WJmphcu\nxni8xeXrEuYGRf7zB9H68pmU3DezKFy+jbBCBC0ullpL135jrC+NqXrLge5Bfg8tLW6rq6tNJd/6\n5ar2Za/8/ZR6UYDmUmusGJ9bYSBnjeb2JyYmoueDA01eI27e4rrMpLMrI1ytEh6AzuYp+RGMz0ii\ncN24cSMum+KrPfgaX6zcwRJS6dyadvRCtHLnk5sYzGy11FLKFSHNbX5ROe9CpCvDjwqBh3O1fAKG\nFyu/T1+6T+0eZDoVrXYjT19dxAsXrzOAuD5U6irMBcH9dw6aYHm8YPH+8MK1trYWSwmlFlRuXhur\n73vPAPe3trawsbERhYtVctj+/EYXtd8uG29xsW2kNSx9Oap0lQumt/vVuumG5mTs+fn5uPm+zZe9\nYnxrc3MT9+/fx8svv4yVlZWGieCnTRm4iAHUlRGudpw2Kk5jQD5jjsK1uLgYs5XYaJiOnwaTKVx+\njshluBvS80jXjeKWNmY26FYbX/dxLV/BnPuMV6QVStKpAnfv3o2uGS9eKysrA93RnpWcu9CfJ923\nk5OTDcLlV+rl4KedcJWWrZomJPnKJFx+fmJiIg6EmMlLD0haBJvWURqT5URkLtBZrVaxsrIS3V1M\nNuB+mkV4kbT6XXLCxZhnGqOmpcprMDo6GgsLp3MqvUtwaWkpbrzOfuNvU6vVGoTrwYMHTa7/VuXf\n2p3jebgWwgW0tr7Smlujo6Mxo4adtF9UjR0yTWNmJDGe5TuUiw5W+sbRKh6VG32mlpWfl5auWeSX\njPFV4P15+Y4nXddsf3+/QZj8xFFfVucyO4vLJhWWnMXFDopVzLnSr09gYMeerjBwlfBp75xqwqSN\ntFKDt8x9SbCcy5yTlh88eBDdg37yMq2GQbmeZhaToHxhYPZRXpiHhoaa7jvOZ+P9Tc8KB58syMDF\nN+nS99vR0RFeeuklvPzyy7GAsZ+v1a4Emw+FXETG9JURrrNm6Hk3DTtrptAyE4kdOF0avoGw1iED\nuq1KO/WaNLuIlpQXI6btp7EsH5SliHFOWpqkwRuD1mVuBdU0jTmt8MBkFS49QYuUwfVBzx48C6mV\nw/3cTeytfC9caZFnuqTTxUivCj6Wt7m5GQeKbFdetFjlgh1s7tF3wEz28BvnavVr8NTOpevT/r2V\n5OPLHIgODw83tBEKOUMe6fspeBQuWvPpgJPFojkvlcvGDEKVlisjXEDrMkbt/Kw+9d37gP1EY/7Y\nvBE4UXRnZyd2xOnS6b5KQq99vLk5Hhxd0c3EydK5xIrUsqLA5Rajo5uG5YiYaJHbcsFyuk/Tdc84\nVSCt5H+VyI06c6SZrExAYLFntqe0EkdpE407gRYXRSs30ZVtCWiev8SYc9oJ+7R5Zq0yPuMLFPfr\neqZ9gxeuubm5OM8q9YwwczeXnMF25RO06D3x1Vv8nDY/qd33b9x84WHvVWpnNFzENb1SwkVyF7KV\naPj4AlORvauQjYM1CTkCZAZhWp3aW1y50XUv/b1evPzojP5rBvhzlSvSbWxsrKkT8PscDVOMOVJl\nZ5ITKG4+iM4ttzLrVaST39rXfvQLHnorl8KVWlxXSbi8xcXzZZtKRWtra6spEYjJUr4AcVqM2A+s\nmJnpy75d1vU8zTuUWlyLi4tYXl6OywH5jYvkpotG5vpBFob2G0vk+VUy/EoZ6Uaxa+dRuujreCWF\ny5MLEnrx8BYXy+tQuFKLy1scTDZg6ntOuNod03l+2JwPP3UrLC8vY3FxscElyMc0e4hZkjn3X1rH\nkfu+cacNPt3SkWCu4V8ll1e3tCqh5Yv65iyuq+YqBBDPy68Jxcn8FCy2q1xx46Ojo4Y5R3zMrY3n\nK44MmvXqY1xzc3OxBBP7Jb9xhejc6hdeyGhhra+vx+tMq4oJKmmiSu7zTovbX8Z1PJdwmdmPA/hp\nAM+GEP6he/5pAO8DMA/gCwA+GEL4xnm+66Lw2Xd+ch7Filk7fqIxs5586RNfnTodCZ8l9tYt/vv8\nzcjGdnBwgJGRkVhJPw36p37ydPMLazI9tpV4eXfM5uZmU9B30DqJfsF24act+Iw4H1vkYCkXHL9K\nsH14WIvUuwL39vaaqkLkhIv7uWLaJVw77/FI+xQ/cOWA3MdS2WelMVJO2WknWCx9dRqX0bflOLNw\nmdl3A3g/gK8lz38YwIcAvBfAiwD+CYDnzOwNIYT9sx/q2egkzpCbrOvjOmm6uxctXyKGroeL7pRT\nqzENajORpFarZTML08wkdpI56yq3+SoOrTZei7QUzHUXLB+Q58b2Rtjp5NqeT4dPO1/fLi4yo+uy\n4bXY29uLcZoQQnYx09SN7UV+kCzU1I2X/j68p1lLcX19HdPT07H0F12n29vbGB8fbznATDcKV6vt\nvItnXlY7O5NwmdkMgF9F3ar6ieTlpwA8E0L49Ml73wvgLoB3A/jk2Q+1O9oFDNORS66GH9/XSrh8\n5Yf19fUoXJflJ08TPvyyD3QtcaXedF5LbqTqU/1Tf3nO7eAnfaaTQP26PJddw6wEckVPffmeToQr\nXU495Spda38fsr0fHh42TcRl4oHPaE2np6TJUv47+nFercIGfukcxtLHx8dRq9UwPT0dRWt6erph\nMVs/ry2XQEWPkY9NpwvpnjXD8jKv4Vktrl8E8OshhM+ZWRQuM3scwG0An+VzIYRNM/sygCdwicLV\nKekEZG9x0TXhSzpxgrGfdU93xGVZXB5+Fxu5j4dwUmFusnCaPcj/S/3ZOTdfmjrb7pEjt5yr4yp1\nrp2Sm/Dui6d6N89pFleuIktuwFb6dfbuUm995SqfcxCXFocdVKu/XSo5XXtbW1sxe3B3dxfVarVh\nLqavB5pLhPIDy1YZwT67spPFM087/ouma+Eysx8B8J0Avivz8m0AAXULy3P35LW+cNrF9a5CPxs9\n13kwbrO+vh5dhT5T6bKEK/0OChdFq1qtxqoL6Zwv33nyEUBWoFIfOzvVXAC4lei1OubrKF6pcHlr\ngZwmXD6z8KJLiQ0CFC5veaVlxbzwp3O6/ODJ026aTL/x0262t7fjvbyzs9OQbMWVKpg16R9zWZXM\nUE09K2lCVk64+hXPytGVcJnZYwCeBfB9IYSDizmky6Wdq9CP7jjSYc0zb3F5M/0yXIWtYlyc9+JH\nol58TkuKSN/X6/PINfxB6zAukpxo+bly3lXo5yOx06Fw+SkEOeG6ateTA6DzTpnwIuefG8Tr5ePW\nvrxcOq2FZehyceZ0ThaF6ywMkmgB3VtcbwFwE8Dz9vBMhgG8w8w+BODPATAAy2i0upYB/O45j7Vr\nOnGb+DWA1tfXY2aSr/TOtNOVlRW8+uqrcRY5q0j7UUonwd9e3ihexLzwAGgSrk7SWC/yJr6Kbqxu\nSH8jXgvOV1pdXY2FVDc2NvDqq6/GlX45WdavMqvszO4oKXnFD15qtVr0AjGjkolXDAe0mrvWqwzU\nQbt3uxWu3wLwF5LnfgnA1wH8TAjhm2Z2B8CTAH4fAMysAuBtqMfFLp3TsgopXNvb2zFbqVarNVVP\nn5ycxMbGRqx35sufeNHKfUe7RJFenZd3k6RB51S0Wh3PeWIAp2VutjqH60Y6sAAQl41YWVmJSTLT\n09MNK0K3Eq7rSrv7qZN2NehtzwsX1yKj+7BWqzWUZTOzbMyKCRqtVhEgnVqdp/Wll0lXwhVCqAL4\nQ/+cmVUBrIQQvn7y1LMAPmpm30A9Hf4ZAC8B+NS5j/aMtLvIdK8xlZap3j6FnGnknHTMeRC0uNK4\nTivxuojzyjUkio+3xPzz/v05kevmeNMO5LSG3e8GPwjwOrMzocVFa79Wq2FiYiK2Nc6Ju6w6mINM\nJwPAdm1w0CyHVrB9cN4lXaW5rGBaYu3mYLYa6KTu09OuxaBcq15Uzmg4kxDCx8xsCsAnUJ+A/HkA\n7wp9mMPVCbS4QggxntAqbZyv+1IpXEqhXynfqWD5IHWr97T6/3bPpZwmWIMaO+g3vCa0jClcW1tb\nsVwR5+L5oHo6d6uTmOVVI41NtSIdlHXS5gcNxrSA/Dw2/wjkV0A+bUDtRau0+9b6fZBm9mYAX+3X\n9+eyu9IUW+6f1iD6IVw5WrkBL+o72o1gB+F6DBppNpyvmsHNTwj3G0fNg9LWLpNcckUrrkIbTLN/\ncxvflyZVtdr8Z/tHYODifW8JITzf6sUrX6vwNHqVsTRIDEjDEy3IuXW9W4idUjeJNeLq4RN5RCND\np/Pl8eUAAAyzSURBVL9FiPZ06pYUD8mJlx/xqrNqTSlxmMviLIlfqXVV2jW79haXOBtXIY7Qb9JE\ngVbxRllbdbrNaruK16yVSJ0l4cQncHXzf4OAhEuci9METLQn13mkr4uH5LJhW73nqtEqIYrkYlid\nWKcl3rsSLtETSmr0g0a7a6frmue6XZdWyVDpXE7u+9evomtVwiXEAFFiJyIultNEy++XakF1i5Iz\nhBCiENolQpUYqzorsriEEGKAaZXEk3MDXpckKQmXEEIMOLmklOscG5VwCSFEIeQyB3OvXXUkXEII\nUSDXSahSlJwhhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQggh\nikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWE\nEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRc\nQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYqi\nK+Eys580s+Nk+8PkPU+b2StmtmNmnzGz1/f2kIUQQlxnzmJx/QGAZQC3T7bv4Qtm9mEAHwLwfgBv\nBVAF8JyZjZ3/UIUQQghg5Az/cxhCuN/itacAPBNC+DQAmNl7AdwF8G4AnzzbIQohhBAPOYvF9WfM\n7GUz+69m9qtm9loAMLPHUbfAPss3hhA2AXwZwBM9OVohhBDXnm6F60sAfhTAOwF8AMDjAP6DmU2j\nLloBdQvLc/fkNSGEEOLcdOUqDCE85/78AzP7CoA/AfAeAH/UywMTQgghcpwrHT6EsAHgPwN4PYA7\nAAz1xA3P8slrQgghxLk5l3CZ2QzqovVKCOEF1AXqSfd6BcDbAHzxPN8jhBBCkK5chWb2swB+HXX3\n4GsA/K8ADgD8m5O3PAvgo2b2DQAvAngGwEsAPtWj4xVCCHHN6TYd/jEAvwZgCcB9AL8N4C+FEFYA\nIITwMTObAvAJAPMAPg/gXSGE/d4dshBCiOuMhRD6ewBmbwbw1b4ehBBCiEHiLSGE51u9qFqFQggh\nikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWE\nEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRc\nQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpC\nwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCi\nKCRcQgghikLCJYQQoigkXEIIIYpCwiWEEKIoJFxCCCGKQsIlhBCiKCRcQgghikLCJYQQoii6Fi4z\ne9TMfsXMHpjZjpl9zczenLznaTN75eT1z5jZ63t3yEIIIa4zXQmXmc0D+AKAPQDvBPAGAP8IwJp7\nz4cBfAjA+wG8FUAVwHNmNtajYxZCCHGNGeny/T8O4FshhPe55/4kec9TAJ4JIXwaAMzsvQDuAng3\ngE+e9UCFEEIIoHtX4Q8C+B0z+6SZ3TWz580sipiZPQ7gNoDP8rkQwiaALwN4ohcHLIQQ4nrTrXB9\nO4APAvhjAN8P4F8A+AUz+9snr98GEFC3sDx3T14TQgghzkW3rsIhAF8JIfzEyd9fM7M3AvgAgF/p\n6ZEJIYQQGbq1uF4F8PXkua8DeN3J/h0ABmA5ec/yyWtCCCHEuehWuL4A4DuS574DJwkaIYQXUBeo\nJ/mimVUAvA3AF89+mEIIIUSdbl2F/xTAF8zsI6hnCL4NwPsA/M/uPc8C+KiZfQPAiwCeAfASgE+d\n+2iFEEJce7oSrhDC75jZDwP4GQA/AeAFAE+FEP6Ne8/HzGwKwCcAzAP4PIB3hRD2e3fYQgghrisW\nQujvAdSrbny1rwchhBBikHhLCOH5Vi+qVqEQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQ\ncAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQo\nCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBC\niKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJ\nIYQoCgmXEEKIopBwCSGEKAoJlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKAoJ\nlxBCiKKQcAkhhCgKCZcQQoiikHAJIYQoCgmXEEKIopBwCSGEKIpBEK6Jfh+AEEKIgaKtLgyCcH1b\nvw9ACCHEQPFt7V60EMIlHUeLAzBbAvBOAC8CqPX1YIQQQvSTCdRF67kQwkqrN/VduIQQQohuGARX\noRBCCNExEi4hhBBFIeESQghRFBIuIYQQRTFQwmVmf8/MXjCzXTP7kpl9d7+PqVvM7O1m9u/M7GUz\nOzazH8q852kze8XMdszsM2b2+n4ca7eY2UfM7Ctmtmlmd83s35rZn828r7jzM7MPmNnXzGzjZPui\nmf33yXuKO68cZvbjJ23z55Pnizw/M/vJk/Px2x8m7yny3ADAzB41s18xswcnx/81M3tz8p5iz+8s\nDIxwmdnfBPBzAH4SwF8E8DUAz5nZjb4eWPdMA/g9AD8GoCll08w+DOBDAN4P4K0Aqqif59hlHuQZ\neTuAfwbgbQC+D8AogN80s0m+oeDz+1MAHwbwZgBvAfA5AJ8yszcARZ9XAyeDwfejfn/550s/vz8A\nsAzg9sn2PXyh5HMzs3kAXwCwh/q0oTcA+EcA1tx7ij2/MxNCGIgNwJcA/O/ubwPwEoB/3O9jO8c5\nHQP4oeS5VwD8A/d3BcAugPf0+3jPcH43Ts7xe67o+a0A+DtX5bwAzAD4YwB/DcC/B/DzV+F3Q32w\n+3yb10s+t58B8P+d8p5iz++s20BYXGY2ivoo97N8LtR/gd8C8ES/jqvXmNnjqI8G/XluAvgyyjzP\nedStylXg6pyfmQ2Z2Y8AmALwxatyXgB+EcCvhxA+55+8Iuf3Z07c8//VzH7VzF4LXIlz+0EAv2Nm\nnzxxzz9vZu/ji1fg/M7EQAgX6iP3YQB3k+fvov6jXBVuo97RF3+eZmYAngXw2yEExhOKPj8ze6OZ\nbaHulvk4gB8OIfwxCj8vADgR4u8E8JHMy6Wf35cA/CjqrrQPAHgcwH8ws2mUf27fDuCDqFvK3w/g\nXwD4BTP72yevl35+Z2Kk3wcgiuXjAP48gL/c7wPpIX8E4E0A5gD8DQC/bGbv6O8hnR8zewz1Qcb3\nhRAO+n08vSaE8Jz78w/M7CsA/gTAe1D/TUtmCMBXQgg/cfL318zsjagL9K/077D6y6BYXA8AHKEe\nXPUsA7hz+YdzYdxBPXZX9Hma2T8H8AMA/koI4VX3UtHnF0I4DCF8M4TwuyGE/wX1BIanUPh5oe6G\nvwngeTM7MLMDAN8L4Ckz20d9dF7y+TUQQtgA8J8BvB7l/3avAvh68tzXAbzuZL/08zsTAyFcJ6PA\nrwJ4ks+duKKeBPDFfh1XrwkhvIB6Y/LnWUE9S6+I8zwRrb8O4K+GEL7lX7sK55cwBGD8CpzXbwH4\nC6i7Ct90sv0OgF8F8KYQwjdR9vk1YGYzqIvWK1fgt/sCgO9InvsO1C3Kq3jPdUa/s0NcJsx7AOwA\neC+APwfgE6hndd3s97F1eR7TqHcM34l6xt3fP/n7tSev/+OT8/pB1DuT/wvAfwEw1u9j7+DcPo56\nGu7bUR/RcZtw7yny/AD89Ml5/TcA3gjgfwNwCOCvlXxebc43zSos9vwA/CyAd5z8dv8dgM+gbkUu\nXYFz+y7UY64fAfDfAvgfAWwB+JGr8Nud+br0+wCSH+nHUF/eZBfAfwTwXf0+pjOcw/eeCNZRsv1L\n956fQj2FdQfAcwBe3+/j7vDccud1BOC9yfuKOz8A/weAb560vTsAfpOiVfJ5tTnfz3nhKvn8APyf\nqE+d2QXwLQC/BuDxq3BuJ8f+AwB+/+TY/xOAv5t5T7Hnd5ZNy5oIIYQoioGIcQkhhBCdIuESQghR\nFBIuIYQQRSHhEkIIURQSLiGEEEUh4RJCCFEUEi4hhBBFIeESQghRFBIuIYQQRSHhEkIIURQSLiGE\nEEUh4RJCCFEU/z/Vr11QsL5bOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18b3ce090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : [9 3 4 6 3 5]\n"
     ]
    }
   ],
   "source": [
    "n=random.randint(0,1000)\n",
    "\n",
    "cropped_train_dataset=np.resize(train_dataset,[10000,70,70])\n",
    "cropped_valid_dataset=np.resize(valid_dataset,[2000,70,70])\n",
    "cropped_test_dataset=np.resize(test_dataset,[2000,70,70])\n",
    "\n",
    "displaySequence(n, cropped_train_dataset, train_labels)\n",
    "displaySequence(n, cropped_valid_dataset, valid_labels)\n",
    "displaySequence(n, cropped_test_dataset, test_labels)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_3:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_4:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_17:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_18:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_26:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_27:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_35:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_36:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_44:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_45:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_53:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_54:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (64, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (64, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (64, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (64, 4, 4, 32)\n",
      "hidden--- (64, 1, 1, 64)\n",
      "reshape--- (64, 64)\n",
      "logits1--- (64, 10)\n",
      "logits1== Tensor(\"add_62:0\", shape=(64, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_63:0\", shape=(64, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_71:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_72:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_80:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_81:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_89:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_90:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_98:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_99:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_107:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_108:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_116:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_117:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_125:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_126:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_134:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_135:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_143:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_144:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_152:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_153:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_161:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_162:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "layer1_weights--- (4, 4, 1, 16)\n",
      "layer2_weights--- (4, 4, 16, 32)\n",
      "layer3_weights--- (4, 4, 32, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "===================\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights--- (2000, 34, 34, 16)\n",
      "conv2d(sub, layer2_weights--- (2000, 7, 7, 32)\n",
      "max_pool(lrn, [1,2,2,1], [1,2,2,1]--- (2000, 4, 4, 32)\n",
      "hidden--- (2000, 1, 1, 64)\n",
      "reshape--- (2000, 64)\n",
      "logits1--- (2000, 10)\n",
      "logits1== Tensor(\"add_170:0\", shape=(2000, 10), dtype=float32)\n",
      "logits2== Tensor(\"add_171:0\", shape=(2000, 10), dtype=float32)\n",
      "s1_w== <tensorflow.python.ops.variables.Variable object at 0x189485ed0>\n",
      "s2_w== <tensorflow.python.ops.variables.Variable object at 0x1894b7750>\n",
      "Initialized\n",
      "Minibatch loss at step 0: 22.425350\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parksoy/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 1: 22.412785\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 2: 22.707245\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 3: 22.604176\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 4: 22.255470\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 5: 22.651310\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 6: 23.172886\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 7: 22.792862\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 8: 22.671812\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 9: 22.220455\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 10: 21.787907\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 11: 21.900715\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 12: 21.669224\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 13: 22.300400\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 14: 22.987175\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 15: 22.228031\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 16: 22.257103\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 17: 22.184738\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 18: 23.219719\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 19: 22.473070\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 20: 23.013826\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 21: 22.265068\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 22: 22.405048\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 23: 22.752254\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 24: 22.540791\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 25: 22.183525\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 26: 21.870855\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 27: 22.789476\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 28: 22.592463\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 29: 22.028593\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 30: 22.377665\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 31: 22.354836\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 32: 21.888838\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 33: 22.049183\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 34: 22.915443\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 35: 22.411516\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 36: 21.673491\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 37: 22.417671\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 38: 22.473383\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 39: 22.763901\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 40: 22.010702\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 41: 23.386341\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 42: 21.535389\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 43: 23.058250\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 44: 22.026934\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 45: 21.693096\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 46: 21.946102\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 47: 22.087767\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 48: 22.234282\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 49: 22.858791\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 50: 22.672733\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 51: 21.771648\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 52: 23.017475\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 53: 22.822220\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 54: 22.249367\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 55: 22.008965\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 56: 21.424414\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 57: 21.912563\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 58: 21.764057\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 59: 21.583651\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 60: 21.991180\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 61: 22.153290\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 62: 22.126293\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 63: 22.299129\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 64: 21.932602\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 65: 22.278137\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 66: 21.888723\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 67: 21.506584\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 68: 22.222794\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 69: 22.403833\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 70: 22.468102\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 71: 22.017778\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 72: 22.409657\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 73: 22.669859\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 74: 21.877831\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 75: 22.612270\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 76: 21.459469\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 77: 22.421547\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 78: 22.122919\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 79: 22.925541\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 80: 21.684107\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 81: 21.808115\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 82: 22.726364\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 83: 22.362936\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 84: 21.670097\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 85: 22.267115\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 86: 22.512100\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 87: 21.912081\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 88: 22.168541\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 89: 22.444176\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 90: 23.056427\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 91: 21.582483\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 92: 22.186283\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 93: 22.149860\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 94: 22.424503\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 95: 22.233015\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 96: 21.616980\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 97: 21.828382\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 98: 22.395681\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 99: 21.937864\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 100: 22.053465\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 101: 22.378345\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 102: 22.847923\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 103: 22.355616\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 104: 22.198757\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 105: 23.086071\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 106: 22.590395\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 107: 22.444538\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 108: 22.028492\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 109: 22.090939\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 110: 21.467445\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 111: 22.261759\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 112: 21.988756\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 113: 22.069136\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 114: 21.716108\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 115: 22.664640\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 116: 22.218700\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 117: 21.717150\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 118: 22.103609\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 119: 22.131454\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 120: 21.657688\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 121: 22.586185\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 122: 21.943565\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 123: 22.081110\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 124: 22.030043\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 125: 22.380253\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 126: 22.171745\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 127: 21.901447\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 128: 21.793152\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 129: 22.080555\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 130: 21.712917\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 131: 22.187960\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 132: 21.798615\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 133: 22.145737\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 134: 21.813290\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 135: 22.186008\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 136: 22.496521\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 137: 22.400026\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 138: 21.659420\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 139: 22.658482\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 140: 21.662384\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 141: 22.565544\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 142: 22.320000\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 143: 21.976847\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 144: 21.877853\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 145: 22.133419\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 146: 22.594421\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 147: 22.087357\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 148: 22.366940\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 149: 22.606342\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 150: 21.884205\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 151: 21.821205\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 152: 22.321564\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 153: 21.374462\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 154: 21.699852\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 155: 22.057570\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 156: 21.859617\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 157: 22.014671\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 158: 22.293074\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 159: 21.691479\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 160: 22.050888\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 161: 22.253576\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 162: 22.233044\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 163: 22.331491\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 164: 21.931881\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 165: 21.219540\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 166: 21.412430\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 167: 21.442482\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 168: 21.636009\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 169: 22.224781\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 170: 22.145100\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 171: 21.692923\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 172: 21.477180\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 173: 22.865833\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 174: 21.534100\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 175: 22.680920\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 176: 21.983898\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 177: 21.747742\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 178: 22.240417\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 179: 22.164820\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 180: 22.020599\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 181: 21.163448\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 182: 22.247860\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 183: 22.151024\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 184: 21.864971\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 185: 21.518957\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 186: 22.051624\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 187: 21.520792\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 188: 21.981604\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 189: 21.896936\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 190: 21.965714\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 191: 21.536663\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 192: 21.679163\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 193: 22.237007\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 194: 22.228661\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 195: 21.751041\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 196: 22.397491\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 197: 21.881594\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 198: 22.215595\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 199: 21.636177\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 200: 21.707981\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 201: 21.221727\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 202: 21.509712\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 203: 21.839993\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 204: 22.495316\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 205: 22.378792\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 206: 21.333124\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 207: 22.376278\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 208: 22.552036\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 209: 21.570969\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 210: 21.891174\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 211: 20.996582\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 212: 21.882200\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 213: 21.380684\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 214: 21.132191\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 215: 21.289062\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 216: 21.776348\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 217: 21.629372\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 218: 22.190264\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 219: 21.354065\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 220: 21.688251\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 221: 21.941624\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 222: 21.327400\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 223: 21.655668\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 224: 21.924059\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 225: 21.914145\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 226: 21.615494\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 227: 22.284737\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 228: 22.270496\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 229: 21.448936\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 230: 22.132635\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 231: 21.349173\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 232: 21.643471\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 233: 22.000629\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 234: 22.752077\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 235: 20.933102\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 236: 21.881073\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 237: 22.155649\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 238: 22.111444\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 239: 20.879642\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 240: 22.527979\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 241: 21.939825\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 242: 21.603085\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 243: 21.843441\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 244: 21.448887\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 245: 23.172474\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 246: 21.298679\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 247: 21.725521\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 248: 22.276525\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 249: 21.570000\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 250: 21.931425\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 251: 21.475170\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 252: 21.424797\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 253: 21.219906\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 254: 22.309576\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 255: 21.798418\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 256: 21.868244\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 257: 22.477051\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 258: 22.217087\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 259: 21.628811\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 260: 22.389471\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 261: 22.686523\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 262: 22.256334\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 263: 21.561563\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 264: 21.828686\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 265: 21.172396\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 266: 21.843477\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 267: 21.702656\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 268: 21.488955\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 269: 21.772203\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 270: 21.998114\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 271: 22.090483\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 272: 21.261295\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 273: 21.828295\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 274: 21.731024\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 275: 21.361773\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 276: 22.104031\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 277: 21.836500\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 278: 21.833450\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 279: 21.767504\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 280: 21.797571\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 281: 21.816280\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 282: 21.861025\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 283: 21.465977\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 284: 21.595699\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 285: 21.617521\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 286: 21.815088\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 287: 21.550177\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 288: 21.749073\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 289: 21.542601\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 290: 21.819561\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 291: 21.968033\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 292: 22.374399\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 293: 21.396446\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 294: 22.030447\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 295: 21.994976\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 296: 21.354723\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 297: 22.380768\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 298: 22.028046\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 299: 21.165638\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 300: 22.081327\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 301: 21.671303\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 302: 22.036434\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 303: 22.581961\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 304: 21.813103\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 305: 22.086044\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 306: 21.170105\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 307: 21.709406\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 308: 21.348545\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 309: 21.759947\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 310: 21.581736\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 311: 21.717659\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 312: 21.665077\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 313: 21.791925\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 314: 21.685566\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 315: 21.455830\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 316: 21.990778\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 317: 22.391815\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 318: 21.835278\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 319: 21.679268\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 320: 21.126400\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 321: 21.095663\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 322: 21.249586\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 323: 21.175564\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 324: 21.650913\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 325: 22.018040\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 326: 21.692642\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 327: 20.966137\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 328: 22.275063\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 329: 21.451851\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 330: 22.119686\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 331: 22.038033\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 332: 21.413673\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 333: 21.715939\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 334: 22.057663\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 335: 21.809578\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 336: 21.300390\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 337: 21.254789\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 338: 22.021227\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 339: 21.792215\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 340: 21.482885\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 341: 21.498501\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 342: 21.393257\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 343: 21.574245\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 344: 21.621052\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 345: 21.620831\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 346: 21.431015\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 347: 21.301075\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 348: 21.882700\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 349: 21.820980\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 350: 21.896442\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 351: 21.711077\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 352: 22.018391\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 353: 21.395538\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 354: 22.059135\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 355: 21.309086\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 356: 20.989748\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 357: 20.880764\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 358: 21.761600\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e3f0805f30c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is Hang's code with change of \n",
    "# patch size, stride step, image size, and logits6 added \n",
    "\n",
    "num_labels = 10 # 0-9, + blank \n",
    "\n",
    "batch_size = 64\n",
    "patch_size = 4\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden1 = 64\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "# Construct a 7-layer CNN.\n",
    "#\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]))\n",
    "  \n",
    "  layer2_weights = tf.get_variable(\"W2\", shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "  \n",
    "  layer3_weights = tf.get_variable(\"W3\", shape=[patch_size, patch_size, depth2, num_hidden1],\\\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden1, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "\n",
    "  s1_w = tf.get_variable(\"WS1\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  s2_w = tf.get_variable(\"WS2\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  s3_w = tf.get_variable(\"WS3\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  s4_w = tf.get_variable(\"WS4\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  s5_w = tf.get_variable(\"WS5\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  s6_w = tf.get_variable(\"WS6\", shape=[num_hidden1, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s6_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob, shape):\n",
    "    \n",
    "    print('layer1_weights---',layer1_weights.get_shape())\n",
    "    print('layer2_weights---',layer2_weights.get_shape())\n",
    "    print('layer3_weights---',layer3_weights.get_shape())\n",
    "    print('layer4_weights---',layer4_weights.get_shape())\n",
    "    print('===================')\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1,2,2,1], 'VALID')\n",
    "    print('conv2d(data, layer1_weights---', conv.get_shape())\n",
    "    \n",
    "    \n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "    conv = tf.nn.conv2d(sub, layer2_weights, [1,2,2,1], padding='VALID')\n",
    "    print('conv2d(sub, layer2_weights---', conv.get_shape())\n",
    "    \n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "    print('max_pool(lrn, [1,2,2,1], [1,2,2,1]---', sub.get_shape())\n",
    "    conv = tf.nn.conv2d(sub, layer3_weights, [1,1,1,1], padding='VALID')\n",
    "    \n",
    "    hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print('hidden---', hidden.get_shape())\n",
    "    \n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape---', reshape.get_shape())\n",
    "    \n",
    "    \n",
    "    logits1 = tf.matmul(reshape, s1_w) + s1_b\n",
    "    print('logits1---', logits1.get_shape())\n",
    "    \n",
    "    logits2 = tf.matmul(reshape, s2_w) + s2_b\n",
    "    logits3 = tf.matmul(reshape, s3_w) + s3_b\n",
    "    logits4 = tf.matmul(reshape, s4_w) + s4_b\n",
    "    logits5 = tf.matmul(reshape, s5_w) + s5_b\n",
    "    logits6 = tf.matmul(reshape, s6_w) + s6_b\n",
    "    print('logits1==',logits1)\n",
    "    print('logits2==',logits2)\n",
    "    print('s1_w==',s1_w)\n",
    "    print('s2_w==',s2_w)\n",
    "    \n",
    "    return [logits1, logits2, logits3, logits4, logits5, logits6]\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits1, logits2, logits3, logits4, logits5, logits6] = model(tf_train_dataset, 1, shape)\n",
    "  \n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_labels[:,0])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_labels[:,1])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_labels[:,2])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_labels[:,3])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_labels[:,4])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits6, tf_train_labels[:,5]))\n",
    "    \n",
    "  # Optimizer.\n",
    "  \n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-5, global_step, 10000, 0.95)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[0]),\\\n",
    "                              tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[1]),\\\n",
    "                              tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[2]),\\\n",
    "                              tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[3]),\\\n",
    "                              tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[4]),\\\n",
    "                              tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[5])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[0]),\\\n",
    "                              tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[1]),\\\n",
    "                              tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[2]),\\\n",
    "                              tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[3]),\\\n",
    "                              tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[4]),\\\n",
    "                              tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[5])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[0]),\\\n",
    "                             tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[1]),\\\n",
    "                             tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[2]),\\\n",
    "                             tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[3]),\\\n",
    "                             tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[4]),\\\n",
    "                             tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[5])])\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "num_steps = 5000 #100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()  \n",
    "  #reader = tf.train.NewCheckpointReader(\"CNN_1.ckpt\")\n",
    "  #reader.get_variable_to_shape_map()\n",
    "  #saver.restore(session, \"CNN_1.ckpt\")\n",
    "  #print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "    print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))\n",
    "  save_path = saver.save(session, \"CNN_multi_withoutCNNsingle.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e635cbbb9cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "print(valid_prediction.get_shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "logits_0-- (16, 10)\n",
      "tf_train_labels--- (16, 6)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (16, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (16, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (16, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (16, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (16, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (16, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (16, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "layer1_weights--- (3, 3, 1, 16)\n",
      "layer2_weights--- (3, 3, 16, 16)\n",
      "layer3_weights--- (3, 3, 16, 64)\n",
      "layer4_weights--- (64, 10)\n",
      "data--- (2000, 70, 70, 1)\n",
      "conv2d(data, layer1_weights-- (2000, 34, 34, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 17, 17, 16)\n",
      "conv2d(hidden, layer2_weights-- (2000, 8, 8, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 4, 4, 16)\n",
      "conv2d(lrn, layer3_weights-- (2000, 2, 2, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 1, 1, 64)\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 64)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 11.711293\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 50: 11.592678\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 100: 11.677632\n",
      "Minibatch accuracy: 16.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 150: 11.616104\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 200: 11.499729\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 250: 11.489838\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 300: 11.863457\n",
      "Minibatch accuracy: 11.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 350: 12.083042\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: 12.305185\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 450: 11.602713\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 11.898415\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 550: 11.589253\n",
      "Minibatch accuracy: 5.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: 11.600997\n",
      "Minibatch accuracy: 16.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 650: 11.564640\n",
      "Minibatch accuracy: 17.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 700: 11.972988\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 750: 11.843906\n",
      "Minibatch accuracy: 11.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: 12.044451\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 850: 11.892457\n",
      "Minibatch accuracy: 11.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 900: 11.579664\n",
      "Minibatch accuracy: 23.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 950: 11.824469\n",
      "Minibatch accuracy: 7.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: 11.733122\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 9.9%\n"
     ]
    }
   ],
   "source": [
    "#improving accuracy-working but only 10%. \n",
    "\n",
    "#this is Udacity code:\n",
    "\n",
    "#adding learning rate\n",
    "#changed the stride of conv\n",
    "#patch size to 4\n",
    "#padding to valid\n",
    "#hardcoded w3 dimension to 256\n",
    "#sparse\n",
    "#changed label to int32\n",
    "#made to 5 logits. \n",
    "#fixed the label for zero accuracy \n",
    "#adding lrn = tf.nn.local_response_normalization(hidden)\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 3\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, num_hidden], stddev=0.1)) #image_size // 7 * image_size // 7 * depth\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) #num_labels\n",
    "\n",
    "  #sharing weight: POR num_hidden,\n",
    "  layer_final_weights_0 = tf.get_variable(\"W0\", shape=[num_hidden, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_1= tf.get_variable(\"W1\", shape=[num_hidden, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_2 = tf.get_variable(\"W2\", shape=[num_hidden, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_3 = tf.get_variable(\"W3\", shape=[num_hidden, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_4 = tf.get_variable(\"W4\", shape=[num_hidden, num_labels],\\\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \n",
    "    print('layer1_weights---',layer1_weights.get_shape())\n",
    "    print('layer2_weights---',layer2_weights.get_shape())\n",
    "    print('layer3_weights---',layer3_weights.get_shape())\n",
    "    print('layer4_weights---',layer4_weights.get_shape())\n",
    "    \n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    #Layer#1\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='VALID')\n",
    "    print('conv2d(data, layer1_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'VALID')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer1_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#2\n",
    "    conv = tf.nn.conv2d(lrn, layer2_weights, [1, 2, 2, 1], padding='VALID')\n",
    "    print('conv2d(hidden, layer2_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'VALID')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer2_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#3:::: newrly added onto Udacity model \n",
    "    conv = tf.nn.conv2d(lrn, layer3_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    print('conv2d(lrn, layer3_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'VALID')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer3_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#4:Fully connected \n",
    "    shape = lrn.get_shape().as_list()\n",
    "    reshape = tf.reshape(lrn, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])---',reshape.get_shape())\n",
    "    #hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    #print('relu(tf.matmul(reshape, layer4_weights) + layer4_biases)---',hidden.get_shape())\n",
    "    \n",
    "    logit_0=tf.matmul(reshape, layer_final_weights_0) + layer_final_biases_0\n",
    "    logit_1=tf.matmul(reshape, layer_final_weights_1) + layer_final_biases_1\n",
    "    logit_2=tf.matmul(reshape, layer_final_weights_2) + layer_final_biases_2\n",
    "    logit_3=tf.matmul(reshape, layer_final_weights_3) + layer_final_biases_3\n",
    "    logit_4=tf.matmul(reshape, layer_final_weights_4) + layer_final_biases_4\n",
    "    \n",
    "    return [logit_0,logit_1,logit_2,logit_3,logit_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = 0#tf.train.exponential_decay(1e-8, global_step, 10000, 0.95)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "logits_0-- (16, 10)\n",
      "tf_train_labels--- (16, 6)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2547.249756\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 50: 12.019458\n",
      "Minibatch accuracy: 11.2%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 100: 12.193613\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.0%\n",
      "Minibatch loss at step 150: 11.472589\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 200: 11.646766\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 250: 11.489622\n",
      "Minibatch accuracy: 3.8%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 300: 11.537664\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.2%\n",
      "Minibatch loss at step 350: 11.515615\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 400: 11.458696\n",
      "Minibatch accuracy: 16.2%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 450: 11.551954\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 500: 11.532597\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 550: 11.493242\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 600: 11.490803\n",
      "Minibatch accuracy: 11.2%\n",
      "Validation accuracy: 11.9%\n",
      "Minibatch loss at step 650: 11.536770\n",
      "Minibatch accuracy: 7.5%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 700: 11.510558\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 750: 11.488017\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 800: 11.508016\n",
      "Minibatch accuracy: 17.5%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 850: 11.486820\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 900: 11.530793\n",
      "Minibatch accuracy: 8.8%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 950: 11.568964\n",
      "Minibatch accuracy: 7.5%\n",
      "Validation accuracy: 11.2%\n",
      "Minibatch loss at step 1000: 11.482586\n",
      "Minibatch accuracy: 17.5%\n",
      "Validation accuracy: 11.2%\n",
      "Test accuracy: 11.1%\n"
     ]
    }
   ],
   "source": [
    "#improve further from 10%. Udacity model based.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#improving accuracy-working but only 10%. \n",
    "\n",
    "#this is Udacity code:\n",
    "\n",
    "#adding learning rate\n",
    "#changed the stride of conv\n",
    "#patch size to 4\n",
    "#padding to valid\n",
    "#hardcoded w3 dimension to 256\n",
    "#sparse\n",
    "#changed label to int32\n",
    "#made to 5 logits. \n",
    "#fixed the label for zero accuracy \n",
    "#adding lrn = tf.nn.local_response_normalization(hidden)\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth1], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth1]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1)) \n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth3])) \n",
    "\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth3, depth4], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth4])) \n",
    "\n",
    "\n",
    "  #sharing weight: POR num_hidden,\n",
    "  layer_final_weights_0 = tf.get_variable(\"W0\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_1= tf.get_variable(\"W1\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_2 = tf.get_variable(\"W2\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_3 = tf.get_variable(\"W3\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_4 = tf.get_variable(\"W4\", shape=[depth4, num_labels],\\\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \n",
    "    print('layer1_weights---',layer1_weights.get_shape())\n",
    "    print('layer2_weights---',layer2_weights.get_shape())\n",
    "    print('layer3_weights---',layer3_weights.get_shape())\n",
    "    print('layer4_weights---',layer4_weights.get_shape())\n",
    "    \n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    #Layer#1\n",
    "    print('*****Stage#1*****')\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(data, layer1_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer1_biases)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#2\n",
    "    print('*****Stage#2*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer2_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer2_biases)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#3\n",
    "    print('*****Stage#3*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer3_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer3_biases)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#4\n",
    "    print('*****Stage#4*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer4_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME') \n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape()) \n",
    "    hidden = tf.nn.relu(sub + layer4_biases)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "   \n",
    "\n",
    "         \n",
    "    #Layer#5\n",
    "    print('*****Stage#5*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer5_weights, [1, 1, 1, 1], padding='VALID') \n",
    "    print('conv2d(lrn, layer5_weights--',conv.get_shape())\n",
    "    #sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    #print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(conv + layer5_biases)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    \n",
    "    #Layer#6:Fully connected \n",
    "    print('*****Stage#6*****')\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])---',reshape.get_shape())\n",
    "    hidden = tf.nn.relu(reshape + layer5_biases)\n",
    "    \n",
    "   \n",
    "    logit_0=tf.matmul(hidden, layer_final_weights_0) + layer_final_biases_0\n",
    "    logit_1=tf.matmul(hidden, layer_final_weights_1) + layer_final_biases_1\n",
    "    logit_2=tf.matmul(hidden, layer_final_weights_2) + layer_final_biases_2\n",
    "    logit_3=tf.matmul(hidden, layer_final_weights_3) + layer_final_biases_3\n",
    "    logit_4=tf.matmul(hidden, layer_final_weights_4) + layer_final_biases_4\n",
    "    \n",
    "    return [logit_0,logit_1,logit_2,logit_3,logit_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "logits_0-- (16, 10)\n",
      "tf_train_labels--- (16, 6)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 4079.699951\n",
      "Minibatch accuracy: 10.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9f0289cb8bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3496\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3498\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#improve further from 10%. Udacity model based.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#improving accuracy-working but only 10%. \n",
    "\n",
    "#this is Udacity code:\n",
    "\n",
    "#adding learning rate\n",
    "#changed the stride of conv\n",
    "#patch size to 4\n",
    "#padding to valid\n",
    "#hardcoded w3 dimension to 256\n",
    "#sparse\n",
    "#changed label to int32\n",
    "#made to 5 logits. \n",
    "#fixed the label for zero accuracy \n",
    "#adding lrn = tf.nn.local_response_normalization(hidden)\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "num_hidden = 64\n",
    "keep_prob=0.9\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth1], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth1]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1)) \n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth3])) \n",
    "\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth3, depth4], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth4])) \n",
    "\n",
    "\n",
    "  #sharing weight: POR num_hidden,\n",
    "    \n",
    "  layer_final_weights_0 = tf.get_variable(\"W0\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_1= tf.get_variable(\"W1\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_2 = tf.get_variable(\"W2\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_3 = tf.get_variable(\"W3\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_4 = tf.get_variable(\"W4\", shape=[depth4, num_labels],\\\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \n",
    "    print('layer1_weights---',layer1_weights.get_shape())\n",
    "    print('layer2_weights---',layer2_weights.get_shape())\n",
    "    print('layer3_weights---',layer3_weights.get_shape())\n",
    "    print('layer4_weights---',layer4_weights.get_shape())\n",
    "    \n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    #Layer#1\n",
    "    print('*****Stage#1*****')\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(data, layer1_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer1_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#2\n",
    "    print('*****Stage#2*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer2_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer2_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#3\n",
    "    print('*****Stage#3*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer3_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#4\n",
    "    print('*****Stage#4*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer4_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME') \n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape()) \n",
    "    hidden = tf.nn.relu(sub + layer4_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "         \n",
    "    #Layer#5\n",
    "    print('*****Stage#5*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer5_weights, [1, 1, 1, 1], padding='VALID') \n",
    "    print('conv2d(lrn, layer5_weights--',conv.get_shape())\n",
    "  \n",
    "    hidden = tf.nn.relu(conv + layer5_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "    \n",
    "    #Layer#6:Fully connected \n",
    "    print('*****Stage#6*****')\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])---',reshape.get_shape())\n",
    "    hidden = tf.nn.relu(reshape + layer5_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    \n",
    "   \n",
    "    logit_0=tf.matmul(hidden, layer_final_weights_0) + layer_final_biases_0\n",
    "    logit_1=tf.matmul(hidden, layer_final_weights_1) + layer_final_biases_1\n",
    "    logit_2=tf.matmul(hidden, layer_final_weights_2) + layer_final_biases_2\n",
    "    logit_3=tf.matmul(hidden, layer_final_weights_3) + layer_final_biases_3\n",
    "    logit_4=tf.matmul(hidden, layer_final_weights_4) + layer_final_biases_4\n",
    "    \n",
    "    return [logit_0,logit_1,logit_2,logit_3,logit_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "logits_0-- (16, 10)\n",
      "tf_train_labels--- (16, 6)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (16, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (16, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (16, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (16, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (16, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (16, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (16, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (16, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (16, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (16, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (16, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "layer1_weights--- (5, 5, 1, 16)\n",
      "layer2_weights--- (5, 5, 16, 16)\n",
      "layer3_weights--- (5, 5, 16, 32)\n",
      "layer4_weights--- (5, 5, 32, 64)\n",
      "data--- (2000, 70, 70, 1)\n",
      "*****Stage#1*****\n",
      "conv2d(data, layer1_weights-- (2000, 70, 70, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 35, 35, 16)\n",
      "*****Stage#2*****\n",
      "conv2d(hidden, layer2_weights-- (2000, 35, 35, 16)\n",
      "max_pool([1,2,2,1]--- (2000, 18, 18, 16)\n",
      "*****Stage#3*****\n",
      "conv2d(hidden, layer3_weights-- (2000, 18, 18, 32)\n",
      "max_pool([1,2,2,1]--- (2000, 9, 9, 32)\n",
      "*****Stage#4*****\n",
      "conv2d(hidden, layer4_weights-- (2000, 9, 9, 64)\n",
      "max_pool([1,2,2,1]--- (2000, 5, 5, 64)\n",
      "*****Stage#5*****\n",
      "conv2d(lrn, layer5_weights-- (2000, 1, 1, 128)\n",
      "*****Stage#6*****\n",
      "reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])--- (2000, 128)\n",
      "Initialized\n",
      "predictions== (5, 16, 10)\n",
      "batch_labels== [[9 5 0 7 0]\n",
      " [2 8 0 7 9]\n",
      " [8 2 1 3 8]\n",
      " [6 9 6 5 5]\n",
      " [3 6 6 2 3]\n",
      " [4 4 6 1 7]\n",
      " [1 9 5 4 7]\n",
      " [7 5 1 1 5]\n",
      " [9 8 1 3 1]\n",
      " [0 8 7 4 1]\n",
      " [9 5 3 4 8]\n",
      " [0 1 8 2 6]\n",
      " [6 3 7 1 1]\n",
      " [7 1 7 4 4]\n",
      " [3 3 5 3 8]\n",
      " [8 8 7 6 1]]\n",
      "predictions[00:]== [[[  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   8.58982209e-29   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   5.05011201e-17   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.64575729e-04   9.99835372e-01   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  9.88064826e-01   0.00000000e+00   0.00000000e+00   1.18108783e-02\n",
      "     6.48899988e-13   0.00000000e+00   1.24296348e-04   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     7.64108098e-37   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   1.57886661e-22\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   3.96040117e-10   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   6.25339808e-17   0.00000000e+00\n",
      "     1.00000000e+00   1.11789544e-27   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  7.44648851e-25   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     8.62856009e-10   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   8.87950599e-01   0.00000000e+00\n",
      "     0.00000000e+00   1.12049341e-01   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.72673980e-28   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  1.23989556e-37   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   9.99999762e-01\n",
      "     0.00000000e+00   2.85724582e-07   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   3.87319276e-32   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   1.65896732e-37   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.48238279e-05\n",
      "     0.00000000e+00   0.00000000e+00   9.99955177e-01   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   1.79088237e-11   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     5.03176822e-24   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.48251325e-01\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     8.51748705e-01   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   6.83555885e-15   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   1.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.92185631e-19   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     3.04777379e-28   0.00000000e+00]\n",
      "  [  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.14829643e-31\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     1.00000000e+00   0.00000000e+00]]]\n",
      "batch_labels[0]== [9 5 0 7 0 5]\n",
      "Minibatch loss at step 0: 3067.503662\n",
      "Minibatch accuracy: 10.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1054ce04fb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3496\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3498\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/parksoy/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#now\n",
    "\n",
    "#improve further from 10%. Udacity model based.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#improving accuracy-working but only 10%. \n",
    "\n",
    "#this is Udacity code:\n",
    "\n",
    "#adding learning rate\n",
    "#changed the stride of conv\n",
    "#patch size to 4\n",
    "#padding to valid\n",
    "#hardcoded w3 dimension to 256\n",
    "#sparse\n",
    "#changed label to int32\n",
    "#made to 5 logits. \n",
    "#fixed the label for zero accuracy \n",
    "#adding lrn = tf.nn.local_response_normalization(hidden)\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "num_hidden = 64\n",
    "keep_prob=0.9\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth1], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth1]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1)) \n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth3])) \n",
    "\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth3, depth4], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth4])) \n",
    "\n",
    "\n",
    "  #sharing weight: POR num_hidden,\n",
    "    \n",
    "  layer_final_weights_0 = tf.get_variable(\"W0\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_1= tf.get_variable(\"W1\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_2 = tf.get_variable(\"W2\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_3 = tf.get_variable(\"W3\", shape=[depth4, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_4 = tf.get_variable(\"W4\", shape=[depth4, num_labels],\\\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \n",
    "    print('layer1_weights---',layer1_weights.get_shape())\n",
    "    print('layer2_weights---',layer2_weights.get_shape())\n",
    "    print('layer3_weights---',layer3_weights.get_shape())\n",
    "    print('layer4_weights---',layer4_weights.get_shape())\n",
    "    \n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    #Layer#1\n",
    "    print('*****Stage#1*****')\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(data, layer1_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer1_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#2\n",
    "    print('*****Stage#2*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer2_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer2_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#3\n",
    "    print('*****Stage#3*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer3_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME')\n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape())\n",
    "    hidden = tf.nn.relu(sub + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    #lrn = tf.nn.local_response_normalization(hidden)\n",
    "    \n",
    "    #Layer#4\n",
    "    print('*****Stage#4*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print('conv2d(hidden, layer4_weights--',conv.get_shape())\n",
    "    sub = tf.nn.max_pool(conv, [1,2,2,1],[1,2,2,1], 'SAME') \n",
    "    print('max_pool([1,2,2,1]---', sub.get_shape()) \n",
    "    hidden = tf.nn.relu(sub + layer4_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "         \n",
    "    #Layer#5\n",
    "    print('*****Stage#5*****')\n",
    "    conv = tf.nn.conv2d(hidden, layer5_weights, [1, 1, 1, 1], padding='VALID') \n",
    "    print('conv2d(lrn, layer5_weights--',conv.get_shape())\n",
    "  \n",
    "    hidden = tf.nn.relu(conv + layer5_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "    \n",
    "    #Layer#6:Fully connected \n",
    "    print('*****Stage#6*****')\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])---',reshape.get_shape())\n",
    "    hidden = tf.nn.relu(reshape + layer5_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    \n",
    "   \n",
    "    logit_0=tf.matmul(hidden, layer_final_weights_0) + layer_final_biases_0\n",
    "    logit_1=tf.matmul(hidden, layer_final_weights_1) + layer_final_biases_1\n",
    "    logit_2=tf.matmul(hidden, layer_final_weights_2) + layer_final_biases_2\n",
    "    logit_3=tf.matmul(hidden, layer_final_weights_3) + layer_final_biases_3\n",
    "    logit_4=tf.matmul(hidden, layer_final_weights_4) + layer_final_biases_4\n",
    "    \n",
    "    return [logit_0,logit_1,logit_2,logit_3,logit_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print('predictions==',predictions.shape)\n",
    "    print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    print('predictions[00:]==',predictions)\n",
    "    print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-657e63ed15ca>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-657e63ed15ca>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    p=[  9.88064826e-01   0.00000000e+00   0.00000000e+00   1.18108783e-02     6.48899988e-13   0.00000000e+00   1.24296348e-04   0.00000000e+00     0.00000000e+00   0.00000000e+00]\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "p=[  9.88064826e-01   0.00000000e+00   0.00000000e+00   1.18108783e-02\\\n",
    "     6.48899988e-13   0.00000000e+00   1.24296348e-04   0.00000000e+00\\\n",
    "     0.00000000e+00   0.00000000e+00]\n",
    "print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "logits_0-- (16, 11)\n",
      "tf_train_labels--- (16, 6)\n",
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "data--- (16, 70, 70, 1)\n",
      "data--- (16, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "data--- (2000, 70, 70, 1)\n",
      "data--- (2000, 4900)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 445.320770\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 11.0%\n",
      "Minibatch loss at step 50: 138.294312\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 53.5%\n",
      "Minibatch loss at step 100: 69.898750\n",
      "Minibatch accuracy: 61.2%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 150: 51.698578\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 200: 41.360313\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 250: 40.609756\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 300: 34.479794\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 350: 54.784756\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 400: 33.299656\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 450: 24.265505\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 500: 40.915665\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 550: 46.924820\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 600: 33.400948\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 650: 8.253138\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 700: 10.430573\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 750: 25.111679\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 800: 18.927851\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 850: 18.077690\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 900: 20.669819\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 950: 15.861673\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1000: 15.066165\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 83.0%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "\n",
    "image_size=70\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "num_hidden = 64\n",
    "keep_prob=0.9\n",
    "num_labels=11\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  \n",
    "  #sharing weight: POR num_hidden,\n",
    "    \n",
    "  layer_final_weights_0 = tf.get_variable(\"W0\", shape=[image_size * image_size, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_1= tf.get_variable(\"W1\", shape=[image_size * image_size, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_2 = tf.get_variable(\"W2\", shape=[image_size * image_size, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_3 = tf.get_variable(\"W3\", shape=[image_size * image_size, num_labels],\\\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  layer_final_weights_4 = tf.get_variable(\"W4\", shape=[image_size * image_size, num_labels],\\\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "  layer_final_biases_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    " \n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "\n",
    "    logits_0=tf.matmul(data, layer_final_weights_0) + layer_final_biases_0\n",
    "    logits_1=tf.matmul(data, layer_final_weights_1) + layer_final_biases_1\n",
    "    logits_2=tf.matmul(data, layer_final_weights_2) + layer_final_biases_2\n",
    "    logits_3=tf.matmul(data, layer_final_weights_3) + layer_final_biases_3\n",
    "    logits_4=tf.matmul(data, layer_final_weights_4) + layer_final_biases_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "Initialized\n",
      "Minibatch loss at step 0: 12.098768\n",
      "Minibatch accuracy: 9.1%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 50: 11.549642\n",
      "Minibatch accuracy: 7.2%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 100: 10.751108\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 14.2%\n",
      "Minibatch loss at step 150: 8.018034\n",
      "Minibatch accuracy: 11.6%\n",
      "Validation accuracy: 21.2%\n",
      "Minibatch loss at step 200: 6.507009\n",
      "Minibatch accuracy: 14.7%\n",
      "Validation accuracy: 25.4%\n",
      "Minibatch loss at step 250: 4.985286\n",
      "Minibatch accuracy: 12.2%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 300: 4.824570\n",
      "Minibatch accuracy: 14.7%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 350: 3.841651\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 29.0%\n",
      "Minibatch loss at step 400: 3.551758\n",
      "Minibatch accuracy: 16.9%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 450: 3.425624\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 29.6%\n",
      "Minibatch loss at step 500: 3.142303\n",
      "Minibatch accuracy: 15.9%\n",
      "Validation accuracy: 29.9%\n",
      "Minibatch loss at step 550: 3.120581\n",
      "Minibatch accuracy: 16.9%\n",
      "Validation accuracy: 30.1%\n",
      "Minibatch loss at step 600: 3.069486\n",
      "Minibatch accuracy: 17.8%\n",
      "Validation accuracy: 30.1%\n",
      "Minibatch loss at step 650: 3.425691\n",
      "Minibatch accuracy: 15.9%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 700: 2.223134\n",
      "Minibatch accuracy: 19.4%\n",
      "Validation accuracy: 30.3%\n",
      "Minibatch loss at step 750: 2.819638\n",
      "Minibatch accuracy: 17.8%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 800: 2.134078\n",
      "Minibatch accuracy: 18.4%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 850: 2.509819\n",
      "Minibatch accuracy: 16.9%\n",
      "Validation accuracy: 30.6%\n",
      "Minibatch loss at step 900: 2.394254\n",
      "Minibatch accuracy: 17.5%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 950: 1.883190\n",
      "Minibatch accuracy: 17.8%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 1000: 2.266509\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 30.8%\n",
      "Test accuracy: 46.7%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. \n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 1\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=0.5 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth0)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "\n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "  def bias_variable(shape):\n",
    "    initial= tf.Variable(tf.zeros(shape))# tf.Variable(tf.constant(1.0, shape))\n",
    "    return initial\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    #variable\n",
    "    \n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth0], stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.zeros([depth0]))\n",
    "    \n",
    "    #operation\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_1x1(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape())\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape)\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape])\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 70, 70, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 4900\n",
      "Initialized\n",
      "Minibatch loss at step 0: 36.144005\n",
      "Minibatch accuracy: 5.6%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 50: 5.316820\n",
      "Minibatch accuracy: 67.5%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 100: 3.824831\n",
      "Minibatch accuracy: 73.1%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 150: 2.977004\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 200: 2.839193\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 250: 2.155709\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 300: 2.111673\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 350: 2.146905\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 400: 1.756672\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 450: 1.697450\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 500: 1.924713\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 550: 1.570438\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 600: 1.653786\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 650: 1.983722\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 700: 1.374733\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 750: 1.612188\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 800: 1.145682\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 850: 1.597054\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 900: 1.398907\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 950: 1.036589\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 1000: 1.272171\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 87.8%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 1\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=0.5 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth0)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth0], stddev=0.1)) #POR 0.1\n",
    "  b_conv1 = tf.Variable(tf.zeros([depth0]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_1x1(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape())\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape)\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape])\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 14.462487\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 50: 7.948806\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 46.2%\n",
      "Minibatch loss at step 100: 5.464668\n",
      "Minibatch accuracy: 62.2%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 150: 4.079159\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 200: 3.866485\n",
      "Minibatch accuracy: 75.3%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 250: 3.143194\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 300: 3.075147\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 350: 3.221571\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 400: 2.855594\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 450: 2.864711\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 500: 2.880685\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 550: 2.275808\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 600: 2.892332\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 650: 3.005105\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 700: 2.268938\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 750: 2.746812\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 800: 2.005272\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 850: 2.378618\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 900: 2.634683\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 950: 2.120980\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1000: 2.620226\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 83.0%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "#now give conv with depth0.\n",
    "\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 1\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=1 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth0)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth0], stddev=0.1)) #POR 0.1\n",
    "  b_conv1 = tf.Variable(tf.zeros([depth0]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 1)\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape== 19600\n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape]) #-1\n",
    "    print('h_pool1_flat==',h_pool1_flat.get_shape())#h_pool1_flat== (4, 19600)\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (4, 19600), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 1)\n",
      "h_pool1 == (64, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (64, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 1)\n",
      "h_pool1 == (2000, 35, 35, 1)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 1225\n",
      "h_pool1_flat== (2000, 1225)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 281.632629\n",
      "Minibatch accuracy: 5.3%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 50: 16.774441\n",
      "Minibatch accuracy: 20.0%\n",
      "Validation accuracy: 21.5%\n",
      "Minibatch loss at step 100: 11.617145\n",
      "Minibatch accuracy: 26.9%\n",
      "Validation accuracy: 26.3%\n",
      "Minibatch loss at step 150: 10.699842\n",
      "Minibatch accuracy: 28.8%\n",
      "Validation accuracy: 30.3%\n",
      "Minibatch loss at step 200: 8.849193\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 35.5%\n",
      "Minibatch loss at step 250: 8.613932\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 39.0%\n",
      "Minibatch loss at step 300: 8.833560\n",
      "Minibatch accuracy: 40.3%\n",
      "Validation accuracy: 42.3%\n",
      "Minibatch loss at step 350: 7.471454\n",
      "Minibatch accuracy: 50.9%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 400: 7.279792\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 47.2%\n",
      "Minibatch loss at step 450: 7.189803\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 500: 6.668615\n",
      "Minibatch accuracy: 56.9%\n",
      "Validation accuracy: 51.3%\n",
      "Minibatch loss at step 550: 6.446750\n",
      "Minibatch accuracy: 55.3%\n",
      "Validation accuracy: 53.1%\n",
      "Minibatch loss at step 600: 7.033371\n",
      "Minibatch accuracy: 49.7%\n",
      "Validation accuracy: 54.9%\n",
      "Minibatch loss at step 650: 6.309502\n",
      "Minibatch accuracy: 58.1%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 700: 6.102709\n",
      "Minibatch accuracy: 62.2%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 750: 6.399445\n",
      "Minibatch accuracy: 59.1%\n",
      "Validation accuracy: 58.9%\n",
      "Minibatch loss at step 800: 5.506807\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 60.3%\n",
      "Minibatch loss at step 850: 5.735612\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 61.3%\n",
      "Minibatch loss at step 900: 5.960600\n",
      "Minibatch accuracy: 61.2%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 950: 5.061719\n",
      "Minibatch accuracy: 69.1%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 1000: 5.125159\n",
      "Minibatch accuracy: 66.6%\n",
      "Validation accuracy: 64.0%\n",
      "Test accuracy: 64.2%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "#now give conv with pathsize 5, depth0: 64.2%\n",
    "\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 5\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=1 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth0)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth0], stddev=0.1)) #POR 0.1\n",
    "  b_conv1 = tf.Variable(tf.zeros([depth0]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 1)\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape== 19600\n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape]) #-1\n",
    "    print('h_pool1_flat==',h_pool1_flat.get_shape())#h_pool1_flat== (4, 19600)\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (4, 19600), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (64, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 19600\n",
      "h_pool1_flat== (2000, 19600)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 154.637451\n",
      "Minibatch accuracy: 8.4%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 50: 8.161394\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 100: 3.092715\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 150: 3.866882\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 200: 1.976316\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 250: 1.580194\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 300: 1.229388\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 350: 0.800789\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 400: 0.503040\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 450: 0.743776\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 500: 0.390475\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 550: 0.243753\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 600: 0.203209\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 650: 0.253406\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 700: 0.148028\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 750: 0.194210\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 800: 0.099497\n",
      "Minibatch accuracy: 99.7%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 850: 0.091398\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 900: 0.107782\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 950: 0.072649\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 1000: 0.065534\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Test accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "#now give conv with pathsize 5, depth0: 64.2% depth1:91.7%\n",
    "\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 5\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=1 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth1)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1)) #POR 0.1\n",
    "  b_conv1 = tf.Variable(tf.zeros([depth1]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 1)\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape== 19600\n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape]) \n",
    "    print('h_pool1_flat==',h_pool1_flat.get_shape())\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (-1, W_fc_shape), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (64, 70, 70, 32)\n",
      "h_pool1 == (64, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (64, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Layer1**********\n",
      "conv== (2000, 70, 70, 32)\n",
      "h_pool1 == (2000, 35, 35, 32)\n",
      "********Stage2: fully connected**********\n",
      "W_fc_shape== 39200\n",
      "h_pool1_flat== (2000, 39200)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 181.907562\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 15.1%\n",
      "Minibatch loss at step 50: 15.976991\n",
      "Minibatch accuracy: 69.7%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 100: 7.563267\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 150: 6.695582\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 200: 2.430868\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 250: 2.213112\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 300: 2.364947\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 350: 1.107750\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 400: 0.650698\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 450: 0.816147\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 500: 0.476582\n",
      "Minibatch accuracy: 97.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 550: 0.192709\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 600: 0.221130\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 650: 0.060835\n",
      "Minibatch accuracy: 99.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 700: 0.114113\n",
      "Minibatch accuracy: 99.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 750: 0.142666\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 800: 0.058824\n",
      "Minibatch accuracy: 99.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 850: 0.055163\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 900: 0.055889\n",
      "Minibatch accuracy: 99.7%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 950: 0.016429\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 1000: 0.029749\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.2%\n",
      "Test accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "#now give conv with pathsize 5, depth0: 64.2% depth1:91.7% depth2: 91.5%. \n",
    "#increasing depth 16 to 32 didnt help. need to make it deeper ny increasing layers\n",
    "\n",
    "\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 5\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=1 #no pooling=0.5\n",
    "W_fc_shape=int((image_size/(2*pooling))*(image_size/(2*pooling))*depth2)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth2], stddev=0.1)) #POR 0.1\n",
    "  b_conv1 = tf.Variable(tf.zeros([depth2]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Layer1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape())\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #2x2 vs. 1x1. \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 1)\n",
    "    \n",
    "    ######Stage2=fully connected and dropout ########\n",
    "    print(\"********Stage2: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape== 19600\n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool1_flat = tf.reshape(h_pool1, [-1, W_fc_shape]) \n",
    "    print('h_pool1_flat==',h_pool1_flat.get_shape())\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool1_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (-1, W_fc_shape), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool1_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool1_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool1_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool1_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 16)\n",
      "h_pool2 == (64, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (64, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 16)\n",
      "h_pool2 == (2000, 18, 18, 16)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 5184\n",
      "h_pool2_flat== (2000, 5184)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 242.897736\n",
      "Minibatch accuracy: 5.6%\n",
      "Validation accuracy: 11.2%\n",
      "Minibatch loss at step 50: 9.996594\n",
      "Minibatch accuracy: 30.9%\n",
      "Validation accuracy: 33.2%\n",
      "Minibatch loss at step 100: 6.672040\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 52.6%\n",
      "Minibatch loss at step 150: 5.031000\n",
      "Minibatch accuracy: 65.3%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 200: 3.675408\n",
      "Minibatch accuracy: 74.4%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 250: 3.060570\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 300: 3.336959\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 350: 2.447450\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 400: 2.259763\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 450: 1.563451\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 500: 1.604200\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 550: 1.309798\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 600: 1.758563\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 650: 1.281363\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 700: 1.016501\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 750: 1.196805\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 800: 0.858526\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 850: 1.190499\n",
      "Minibatch accuracy: 93.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 900: 1.107278\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 950: 0.816477\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 1000: 0.883194\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "#now give conv with pathsize 5, depth0: 64.2% depth1:91.7% depth2: 91.5%. \n",
    "#increasing depth 16 to 32 didnt help. need to make it deeper ny increasing layers\n",
    "#Add second conv. depth1=91.0%\n",
    "\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 5\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=2 #no pooling=0.5\n",
    "W_fc_shape=int(math.ceil((image_size/(2.0*pooling))))*int(math.ceil(image_size/(2.0*pooling)))*depth1\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1)) \n",
    "  b_conv1 = tf.Variable(tf.zeros([depth1]))\n",
    "  W_conv2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth1], stddev=0.1)) \n",
    "  b_conv2 = tf.Variable(tf.zeros([depth1]))\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Stage1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape()) #conv== (64, 70, 70, 16)\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 16)\n",
    "    \n",
    "    \n",
    "    ######Stage2########\n",
    "    print(\"********Stage2**********\")\n",
    "    conv=conv2d(h_pool1, W_conv2) + b_conv2 \n",
    "    #h_pool1=[batch, image/2,image/2, depth], W_conv2=[patch_size, patch_size, depth1, depth1], b_conv2=[depth]\n",
    "    print('conv==',conv.get_shape()) #conv== (64, 35, 35, 16)\n",
    "    h_conv2 = tf.nn.relu(conv)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) \n",
    "    print('h_pool2 ==',h_pool2.get_shape()) #h_pool2 == (64, 18, 18, 16)\n",
    "    shape= h_pool2.get_shape().as_list()\n",
    "    \n",
    "    ######Stage3=fully connected and dropout ########\n",
    "    print(\"********Stage3: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape== 4624 \n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [shape[0],shape[1]*shape[2]*shape[3]]) #tf.reshape(h_pool2, [-1, W_fc_shape]) \n",
    "    print('h_pool2_flat==',h_pool2_flat.get_shape())\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool2_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (-1, W_fc_shape), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool2_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool2_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool2_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool2_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "logits_0-- (64, 11)\n",
      "tf_train_labels--- (64, 6)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "data--- (64, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (64, 70, 70, 16)\n",
      "h_pool1 == (64, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (64, 35, 35, 32)\n",
      "h_pool2 == (64, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (64, 18, 18, 128)\n",
      "h_pool3 == (64, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (64, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "data--- (2000, 70, 70, 1)\n",
      "********Stage1**********\n",
      "conv== (2000, 70, 70, 16)\n",
      "h_pool1 == (2000, 35, 35, 16)\n",
      "********Stage2**********\n",
      "conv== (2000, 35, 35, 32)\n",
      "h_pool2 == (2000, 18, 18, 32)\n",
      "********Stage3**********\n",
      "conv== (2000, 18, 18, 128)\n",
      "h_pool3 == (2000, 9, 9, 128)\n",
      "********Stage3: fully connected**********\n",
      "W_fc_shape== 10368\n",
      "h_pool3_flat== (2000, 10368)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 702.297241\n",
      "Minibatch accuracy: 7.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 50: 9.575317\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 35.3%\n",
      "Minibatch loss at step 100: 6.456579\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 150: 4.741922\n",
      "Minibatch accuracy: 66.6%\n",
      "Validation accuracy: 65.9%\n",
      "Minibatch loss at step 200: 3.912915\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 250: 3.281106\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 300: 3.411923\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 350: 3.128342\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 400: 2.425878\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 450: 2.305349\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 500: 2.086356\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 550: 1.783410\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 600: 1.954654\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 650: 2.300119\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 700: 1.404956\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 750: 1.408744\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 800: 1.342255\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 850: 1.319723\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 900: 1.379423\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 950: 0.981116\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 1000: 0.912301\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 88.3%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "#fully connected only=83% accuracy!!!!!\n",
    "#adding fc, conv1x1,pool1x1: 46.7%. after taking out variable, initialize with std=0.1, 87.8%!!\n",
    "#now give maxpool 2x2. with depth0, 68.6%. 82.9%\n",
    "\n",
    "#now give first conv with pathsize 5, depth0: 64.2% depth1:91.7% depth2: 91.5%. \n",
    "###increasing depth 16 to 32 didnt help. need to make it deeper ny increasing layers\n",
    "\n",
    "#Add second conv. depth1=91.0% depth2=88.6% depth3=91.1% depth4 89.8%\n",
    "#Add third conv. depth2 at final=89.0% after fix 2**pooling, 88.2% depth4 88.7%\n",
    "\n",
    "image_size=70\n",
    "num_labels=11\n",
    "num_channels=1\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#Hyperparameters\n",
    "patch_size = 5\n",
    "keep_prob=1\n",
    "\n",
    "depth0 = 1 #no pooling, no conv\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "depth4 = 128\n",
    "depth5 = 1024\n",
    "\n",
    "pooling=3 #no pooling=0.5\n",
    "W_fc_shape=int(math.ceil((image_size/(1.0*2**pooling))))*int(math.ceil(image_size/(1.0*2**pooling)))*depth4 #HYPERPARAMETER=depth\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  #variable\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1)) \n",
    "  b_conv1 = tf.Variable(tf.zeros([depth1]))\n",
    "  W_conv2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1)) #HYPERPARAMETER=depth\n",
    "  b_conv2 = tf.Variable(tf.zeros([depth2])) #HYPERPARAMETER=depth\n",
    "  W_conv3 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth4], stddev=0.1)) #HYPERPARAMETER=depth\n",
    "  b_conv3 = tf.Variable(tf.zeros([depth4])) #HYPERPARAMETER=depth\n",
    "\n",
    "\n",
    "    \n",
    "  W_fc_0 = tf.get_variable(\"W0\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_1 = tf.get_variable(\"W1\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_2 = tf.get_variable(\"W2\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_3 = tf.get_variable(\"W3\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "  W_fc_4 = tf.get_variable(\"W4\", shape=[W_fc_shape, num_labels],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "  b_fc_0 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_1 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_2 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_3 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  b_fc_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  # network architecture.\n",
    "  def model(data):\n",
    " \n",
    "    #data = tf.reshape(data,(-1, image_size*image_size))\n",
    "    print('data---',data.get_shape())\n",
    "    \n",
    "    ######Stage1########\n",
    "    print(\"********Stage1**********\")\n",
    "    conv=conv2d(data, W_conv1) + b_conv1\n",
    "    print('conv==',conv.get_shape()) #conv== (64, 70, 70, 16)\n",
    "    h_conv1 = tf.nn.relu(conv)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) \n",
    "    print('h_pool1 ==',h_pool1.get_shape()) #h_pool1 == (64, 35, 35, 16)\n",
    "    \n",
    "    \n",
    "    ######Stage2########\n",
    "    print(\"********Stage2**********\")\n",
    "    conv=conv2d(h_pool1, W_conv2) + b_conv2 \n",
    "    #h_pool1=[batch, image/2,image/2, depth], W_conv2=[patch_size, patch_size, depth1, depth1], b_conv2=[depth]\n",
    "    print('conv==',conv.get_shape()) #conv== (64, 35, 35, 16)\n",
    "    h_conv2 = tf.nn.relu(conv)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) \n",
    "    print('h_pool2 ==',h_pool2.get_shape()) #h_pool2 == (64, 18, 18, 16)\n",
    "    \n",
    "    \n",
    "    ######Stage2########\n",
    "    print(\"********Stage3**********\")\n",
    "    conv=conv2d(h_pool2, W_conv3) + b_conv3 \n",
    "    #h_pool1=[batch, image/2,image/2, depth], W_conv2=[patch_size, patch_size, depth1, depth1], b_conv2=[depth]\n",
    "    print('conv==',conv.get_shape()) \n",
    "    h_conv3 = tf.nn.relu(conv)\n",
    "    h_pool3 = max_pool_2x2(h_conv3) \n",
    "    print('h_pool3 ==',h_pool3.get_shape()) #h_pool3 == (64, 9, 9, 64)\n",
    "    shape= h_pool3.get_shape().as_list()\n",
    "    \n",
    "    ######Stage4=fully connected and dropout ########\n",
    "    print(\"********Stage3: fully connected**********\")\n",
    "    \n",
    "    print('W_fc_shape==',W_fc_shape) #W_fc_shape=(image_size/(2.0*pooling))))*(image_size/(2.0*pooling)))*depth3 #HYPERPARAMETER=depth\n",
    "\n",
    "\n",
    "    \n",
    "    #variable\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [shape[0],shape[1]*shape[2]*shape[3]]) #tf.reshape(h_pool2, [-1, W_fc_shape]) h_pool3_flat== (64, 5184)\n",
    "    print('h_pool3_flat==',h_pool3_flat.get_shape())\n",
    "  \n",
    "    logits_0=tf.matmul(h_pool3_flat, W_fc_0) + b_fc_0 #h_pool1_flat== (-1, W_fc_shape), W_fc_0=[W_fc_shape, num_labels]\n",
    "    logits_1=tf.matmul(h_pool3_flat, W_fc_1) + b_fc_1\n",
    "    logits_2=tf.matmul(h_pool3_flat, W_fc_2) + b_fc_2\n",
    "    logits_3=tf.matmul(h_pool3_flat, W_fc_3) + b_fc_3\n",
    "    logits_4=tf.matmul(h_pool3_flat, W_fc_4) + b_fc_4\n",
    "    \n",
    "    return [logits_0,logits_1,logits_2,logits_3,logits_4]\n",
    "\n",
    "\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits_0,logits_1,logits_2,logits_3,logits_4] = model(tf_train_dataset)\n",
    "  \n",
    "  print('logits_0--',logits_0.get_shape())\n",
    "  print('tf_train_labels---',tf_train_labels.get_shape())\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,0], logits=logits_0))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,1], logits=logits_1))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,2], logits=logits_2))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,3], logits=logits_3))+\\\n",
    "         tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_train_labels[:,4], logits=logits_4))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1e-2, global_step, 10000, 0.95)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  \n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_train_dataset)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print('predictions==',predictions.shape)\n",
    "    #print('batch_labels==',batch_labels[offset:(offset + batch_size),0:5])\n",
    "    #print('predictions[00:]==',predictions)\n",
    "    #print('batch_labels[0]==',batch_labels[0,:])\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,0:5]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,0:5]))\n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,0:5]))\n",
    "  save_path = saver.save(session, \"CNN_multi_TwoStages.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
